{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.1. Mining text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mining text data involves extracting meaningful patterns, insights, and knowledge from unstructured textual information. This is a crucial aspect of data mining, as text data is abundant in various forms such as articles, reviews, social media posts, and more. Techniques like natural language processing (NLP) are employed to analyze and derive valuable information from textual content.\n",
    "\n",
    "#### Key points about mining text data:\n",
    "\n",
    "- Tokenization and Preprocessing:\n",
    "        Text data is typically processed through tokenization, breaking it into individual words or phrases. Preprocessing steps may include removing stop words, stemming, and lemmatization to reduce noise and enhance analysis.\n",
    "\n",
    "- Feature Extraction:\n",
    "        Techniques like Term Frequency-Inverse Document Frequency (TF-IDF) or word embeddings (e.g., Word2Vec, GloVe) are used to convert text into numerical features that can be utilized in data mining algorithms.\n",
    "\n",
    "- Sentiment Analysis:\n",
    "        Analyzing sentiment in text is a common application, determining the emotional tone of a piece of writing. This is valuable for understanding customer opinions, reviews, and social media sentiment.\n",
    "\n",
    "- Topic Modeling:\n",
    "        Identifying topics within a large collection of text documents is achieved through topic modeling algorithms like Latent Dirichlet Allocation (LDA). This aids in organizing and categorizing textual content.\n",
    "\n",
    "- Applications:\n",
    "        Text mining finds applications in diverse fields such as social media analysis, customer feedback, information retrieval, and automated document categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load the 20 Newsgroups dataset as an example of text data\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "df = pd.DataFrame({'text': newsgroups.data, 'category': newsgroups.target_names[newsgroups.target]})\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(df['text'], df['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text data to numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train = vectorizer.fit_transform(train_data)\n",
    "X_test = vectorizer.transform(test_data)\n",
    "\n",
    "# Label encoding for categories\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_labels)\n",
    "y_test = label_encoder.transform(test_labels)\n",
    "\n",
    "# Train a Support Vector Machine (SVM) classifier for sentiment analysis\n",
    "svm_classifier = SVC(kernel='linear', C=1.0)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "classification_rep = classification_report(y_test, predictions, target_names=newsgroups.target_names)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the 20 Newsgroups dataset, a collection of approximately 20,000 newsgroup documents across 20 different categories. We perform sentiment analysis using a Support Vector Machine (SVM) classifier on the TF-IDF features extracted from the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.2. Spatial-temporal data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mining spatial-temporal data involves extracting patterns, trends, and knowledge from datasets that include both spatial and temporal dimensions. This type of data is prevalent in various domains, such as transportation, environmental monitoring, and location-based services. Techniques applied to spatial-temporal data aim to uncover relationships, predict future occurrences, and enhance decision-making in dynamic environments.\n",
    "\n",
    "#### Key points about mining spatial-temporal data:\n",
    "\n",
    "1. Spatial and Temporal Dimensions:\n",
    "        Spatial-temporal data combines information about geographic locations (spatial) with timestamps or time intervals (temporal). This dual aspect allows for the analysis of how phenomena evolve and vary over both space and time.\n",
    "\n",
    "2. Trajectory Analysis:\n",
    "        Trajectory data, representing the movement of objects over time, is a common form of spatial-temporal data. Analyzing trajectories helps understand patterns like transportation routes, migration, and the spread of diseases.\n",
    "\n",
    "3. Event Detection:\n",
    "        Spatial-temporal data mining is useful for detecting events that unfold over specific regions and time periods. This can include natural disasters, urban developments, or anomalous patterns in environmental monitoring.\n",
    "\n",
    "4. Predictive Modeling:\n",
    "        Predictive modeling in spatial-temporal data involves forecasting future states or events based on historical patterns. This is valuable for tasks like traffic prediction, weather forecasting, and resource management.\n",
    "\n",
    "5. Applications:\n",
    "        Applications span various domains, including urban planning, epidemiology, transportation management, and environmental science, where understanding both spatial and temporal aspects is critical.\n",
    "\n",
    "####  A real-world example of practical use in Python for mining spatial-temporal data using trajectory analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString\n",
    "import folium\n",
    "\n",
    "# Generate synthetic spatial-temporal data (trajectories)\n",
    "trajectory_data = [\n",
    "    {'timestamp': '2023-01-01 12:00:00', 'latitude': 40.7128, 'longitude': -74.0060},\n",
    "    {'timestamp': '2023-01-01 12:30:00', 'latitude': 34.0522, 'longitude': -118.2437},\n",
    "    {'timestamp': '2023-01-01 13:00:00', 'latitude': 41.8781, 'longitude': -87.6298},\n",
    "    # ... additional points\n",
    "]\n",
    "\n",
    "# Create a GeoDataFrame with Point geometries\n",
    "gdf = gpd.GeoDataFrame(trajectory_data, geometry=[Point(xy) for xy in zip(trajectory_data['longitude'], trajectory_data['latitude'])])\n",
    "gdf['timestamp'] = pd.to_datetime(gdf['timestamp'])\n",
    "\n",
    "# Create a LineString from the points to represent the trajectory\n",
    "trajectory_line = LineString(gdf['geometry'])\n",
    "\n",
    "# Create a folium map with the trajectory\n",
    "map_center = [gdf['latitude'].mean(), gdf['longitude'].mean()]\n",
    "m = folium.Map(location=map_center, zoom_start=10)\n",
    "folium.GeoJson(trajectory_line).add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m.save('spatial_temporal_trajectory_map.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic spatial-temporal data representing a trajectory and visualize it on an interactive map using the Folium library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.3. Graph and networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mining graph and network data involves the analysis of interconnected entities and the relationships between them. This type of data is represented as graphs, where nodes represent entities, and edges represent connections or interactions. Techniques for mining graph data are essential for understanding complex relationships, identifying patterns, and extracting meaningful insights from interconnected systems.\n",
    "\n",
    "#### Key points about mining graph and network data:\n",
    "\n",
    "- Graph Representation:\n",
    "        Graphs consist of nodes (vertices) and edges connecting these nodes. This representation is used to model relationships in diverse domains such as social networks, transportation systems, biological interactions, and more.\n",
    "\n",
    "- Community Detection:\n",
    "        Community detection algorithms identify groups of nodes that are densely connected within themselves but sparsely connected to the rest of the graph. This aids in understanding the modular structure of complex networks.\n",
    "\n",
    "- Centrality Measures:\n",
    "        Centrality measures, like degree centrality and betweenness centrality, quantify the importance of nodes within a network. These measures help identify influential entities and key connectors.\n",
    "\n",
    "- Anomaly Detection:\n",
    "        Mining graph data is valuable for detecting anomalies or outliers in interconnected systems. Sudden changes in the network structure or unexpected connections can be indicative of abnormalities.\n",
    "\n",
    "- Recommendation Systems:\n",
    "        Graph-based recommendation systems leverage the relationships between entities to provide personalized recommendations. This is common in social media, e-commerce, and content platforms.\n",
    "\n",
    "#### Real-world example of practical use in Python for mining graph and network data using community detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "\n",
    "# Generate a synthetic social network graph\n",
    "G = nx.karate_club_graph()\n",
    "\n",
    "# Perform community detection using modularity\n",
    "communities = greedy_modularity_communities(G)\n",
    "\n",
    "# Visualize the graph with nodes colored by community\n",
    "node_colors = []\n",
    "for i, community in enumerate(communities):\n",
    "    node_colors.extend([i] * len(community))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "nx.draw(G, with_labels=True, node_color=node_colors, cmap=plt.cm.viridis, font_color='white')\n",
    "plt.title('Social Network Graph with Community Detection')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate a synthetic social network graph using the Karate Club dataset and apply community detection using modularity. Nodes are colored based on their community membership, providing insights into the structure of the social network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.1. Data mining for sentiment and opinion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data mining for sentiment and opinion involves the analysis of textual data to uncover sentiments, emotions, and opinions expressed by individuals or groups. This application is widely used in social media, customer reviews, and other platforms where understanding public sentiment is crucial. Techniques include natural language processing (NLP), sentiment analysis, and machine learning to extract valuable insights from unstructured textual data.\n",
    "\n",
    "#### Key points about data mining for sentiment and opinion:\n",
    "\n",
    "- Textual Analysis:\n",
    "        Data mining techniques are applied to analyze large volumes of textual data, such as social media posts, product reviews, and comments. The goal is to extract sentiments, emotions, and opinions expressed by users.\n",
    "\n",
    "- Sentiment Analysis Techniques:\n",
    "        Sentiment analysis involves classifying text into categories such as positive, negative, or neutral. Techniques range from rule-based approaches to machine learning models, including natural language processing and deep learning methods.\n",
    "\n",
    "- Brand Monitoring:\n",
    "        Businesses use sentiment analysis to monitor and understand how their brand is perceived by customers. Analyzing social media mentions and reviews helps in gauging public opinion and making informed decisions.\n",
    "\n",
    "- Customer Feedback:\n",
    "        Sentiment analysis is applied to customer reviews and feedback to identify areas of improvement, common pain points, and overall customer satisfaction. This is valuable for enhancing products and services.\n",
    "\n",
    "- Social Media Analytics:\n",
    "        Social media platforms generate vast amounts of textual data. Data mining for sentiment allows businesses, marketers, and researchers to track trends, understand public sentiment, and respond effectively.\n",
    "\n",
    "#### A real-world example of practical use in Python for data mining for sentiment and opinion using a sentiment analysis model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset (replace with your own data)\n",
    "data = {'Text': ['I love this product! It works great.',\n",
    "                 'The customer service was terrible.',\n",
    "                 'This movie is amazing.',\n",
    "                 'The weather is awful today.']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform sentiment analysis using TextBlob\n",
    "df['Sentiment'] = df['Text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Visualize the sentiment scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df.index, df['Sentiment'], color=df['Sentiment'].apply(lambda x: 'green' if x > 0 else 'red'))\n",
    "plt.title('Sentiment Analysis Example')\n",
    "plt.xlabel('Text')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.xticks(df.index, df['Text'], rotation=45, ha='right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the TextBlob library for sentiment analysis on a small dataset. The sentiment scores are visualized, with positive sentiments in green and negative sentiments in red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.2. Truth discovery and misinformation identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truth discovery involves the process of determining the most accurate and reliable information from conflicting or uncertain sources. This is particularly relevant in scenarios where multiple sources provide conflicting information, and the goal is to identify the true or correct data. Misinformation identification is a related task that aims to detect false or misleading information within datasets. Data mining techniques are employed to assess the credibility of sources and uncover accurate information.\n",
    "\n",
    "#### Key points about truth discovery and misinformation identification:\n",
    "\n",
    "- Conflicting Information:\n",
    "        In datasets with conflicting information from multiple sources, determining the true or accurate data can be challenging. Truth discovery methods aim to reconcile these conflicts and identify the most reliable information.\n",
    "\n",
    "- Source Credibility:\n",
    "        Assessing the credibility of information sources is crucial. Truth discovery involves assigning weights or scores to different sources based on their historical accuracy, reliability, or expertise in a given domain.\n",
    "\n",
    "- Consistency Analysis:\n",
    "        Consistency analysis is a common approach in truth discovery. It involves evaluating the consistency of information across multiple sources. Inconsistent information may be indicative of errors or intentional misinformation.\n",
    "\n",
    "- Misinformation Detection:\n",
    "        Misinformation identification focuses on detecting false or misleading information intentionally spread to deceive. Data mining techniques, including natural language processing and machine learning, are applied to identify patterns associated with misinformation.\n",
    "\n",
    "- Applications:\n",
    "        Truth discovery and misinformation identification have applications in various domains, including journalism, social media, and data-driven decision-making, where the reliability of information is critical.\n",
    "\n",
    "#### A real-world example of practical use in Python for truth discovery and misinformation identification using a basic consistency analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Example dataset with conflicting information\n",
    "data = {'Statement': ['Climate change is a serious threat.', 'Climate change is a hoax.', 'Climate change is real.']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform a simple consistency analysis based on word frequencies\n",
    "word_frequencies = Counter(\" \".join(df['Statement']).split())\n",
    "most_common_word = word_frequencies.most_common(1)[0][0]\n",
    "\n",
    "# Identify statements consistent with the most common word\n",
    "consistent_statements = df[df['Statement'].str.contains(most_common_word)]\n",
    "\n",
    "# Display the results\n",
    "print(\"Most Common Word:\", most_common_word)\n",
    "print(\"Consistent Statements:\")\n",
    "print(consistent_statements)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we perform a basic consistency analysis by identifying the most common word in the dataset. Statements that contain this most common word are considered consistent. In a more advanced application, you would incorporate more sophisticated algorithms and additional data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.3. Information and disease propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data mining applications in information and disease propagation involve the analysis of how information and diseases spread through networks. Understanding the dynamics of information dissemination in social networks and the transmission patterns of diseases is crucial for making informed decisions in areas such as public health, crisis management, and social influence analysis.\n",
    "\n",
    "#### Key points about information and disease propagation:\n",
    "\n",
    "- Network Dynamics:\n",
    "        Information and diseases often spread through interconnected networks. Analyzing the dynamics of these networks provides insights into the factors influencing propagation patterns.\n",
    "\n",
    "- Social Influence Analysis:\n",
    "        Data mining techniques are applied to study social influence within networks. Identifying influential nodes and understanding how information spreads through social connections aids in designing effective strategies for information dissemination.\n",
    "\n",
    "- Epidemiological Modeling:\n",
    "        In the context of disease propagation, data mining is used to develop epidemiological models that simulate the spread of diseases. These models consider factors such as contact patterns, transmission rates, and population dynamics.\n",
    "\n",
    "- Early Detection and Intervention:\n",
    "        Data mining enables early detection of emerging trends in information propagation or disease outbreaks. Timely intervention based on data-driven insights is crucial for managing and mitigating the impact of these phenomena.\n",
    "\n",
    "- Public Health Decision Support:\n",
    "        Decision-makers in public health use data mining results to inform strategies for vaccination campaigns, public awareness programs, and resource allocation in response to disease outbreaks.\n",
    "\n",
    "#### A real-world example of practical use in Python for information and disease propagation using an epidemiological model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from random import choice\n",
    "\n",
    "# Generate a synthetic social network graph\n",
    "G = nx.erdos_renyi_graph(100, 0.1)\n",
    "\n",
    "# Simulate disease propagation using the SIR (Susceptible-Infectious-Recovered) model\n",
    "def simulate_disease_spread(graph, initial_infected, transmission_rate, recovery_rate):\n",
    "    state = {node: 'S' for node in graph.nodes}\n",
    "    for node in initial_infected:\n",
    "        state[node] = 'I'\n",
    "\n",
    "    infected_nodes = set(initial_infected)\n",
    "\n",
    "    while 'I' in state.values():\n",
    "        newly_infected = set()\n",
    "\n",
    "        for node in infected_nodes:\n",
    "            neighbors = set(graph.neighbors(node))\n",
    "            neighbors_susceptible = neighbors - set(infected_nodes)\n",
    "            \n",
    "            for neighbor in neighbors_susceptible:\n",
    "                if choice([True, False], p=[transmission_rate, 1-transmission_rate]):\n",
    "                    newly_infected.add(neighbor)\n",
    "        \n",
    "        for node in newly_infected:\n",
    "            state[node] = 'I'\n",
    "        \n",
    "        infected_nodes.update(newly_infected)\n",
    "\n",
    "        for node in list(infected_nodes):\n",
    "            if choice([True, False], p=[recovery_rate, 1-recovery_rate]):\n",
    "                state[node] = 'R'\n",
    "                infected_nodes.remove(node)\n",
    "\n",
    "    return state\n",
    "\n",
    "# Set initial parameters\n",
    "initial_infected_nodes = [choice(list(G.nodes))]\n",
    "transmission_rate = 0.2\n",
    "recovery_rate = 0.1\n",
    "\n",
    "# Simulate disease spread\n",
    "final_state = simulate_disease_spread(G, initial_infected_nodes, transmission_rate, recovery_rate)\n",
    "\n",
    "# Visualize the final state of the network\n",
    "node_colors = ['red' if state == 'I' else 'green' for state in final_state.values()]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "nx.draw(G, with_labels=True, node_color=node_colors, cmap=plt.cm.RdYlGn)\n",
    "plt.title('Disease Propagation Simulation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we simulate disease propagation on a synthetic social network using the SIR model. The initial infected nodes are set, and the simulation progresses with defined transmission and recovery rates. Visualize the final state of the network to observe the spread of the simulated disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.4. Productivity and team science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data mining applications in productivity and team science involve analyzing collaboration patterns and productivity factors within teams. Understanding how teams work together, identifying key contributors, and optimizing collaboration processes are essential for enhancing productivity and fostering innovation in scientific endeavors.\n",
    "\n",
    "#### Key points about productivity and team science:\n",
    "\n",
    "- Collaboration Patterns:\n",
    "        Data mining techniques are applied to study collaboration patterns within teams. This includes analyzing co-authorship networks, communication patterns, and shared resources to understand how team members collaborate.\n",
    "\n",
    "- Productivity Metrics:\n",
    "        Productivity metrics, such as publication output, project completion rates, and contribution levels, are assessed using data mining methods. These metrics help measure the impact and efficiency of team members and teams as a whole.\n",
    "\n",
    "- Identification of Key Contributors:\n",
    "        Data mining enables the identification of key contributors within teams. Analyzing individual contributions, expertise, and collaboration patterns helps recognize team members who significantly impact the team's productivity and success.\n",
    "\n",
    "- Team Composition Optimization:\n",
    "        Understanding the characteristics of successful teams is crucial for optimizing team composition. Data mining can reveal factors such as diversity, skill distribution, and communication styles that contribute to effective collaboration and productivity.\n",
    "\n",
    "- Innovation and Team Dynamics:\n",
    "        Analyzing data on team science allows researchers and decision-makers to gain insights into the dynamics of innovation. This includes identifying factors that drive successful interdisciplinary collaboration and innovation within teams.\n",
    "\n",
    "#### A real-world example of practical use in Python for analyzing collaboration patterns within teams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset with co-authorship information\n",
    "data = {'Author1': ['Alice', 'Bob', 'Alice', 'Charlie', 'David', 'Alice'],\n",
    "        'Author2': ['Bob', 'Alice', 'Charlie', 'David', 'Alice', 'Charlie']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a co-authorship network\n",
    "G = nx.from_pandas_edgelist(df, 'Author1', 'Author2')\n",
    "\n",
    "# Visualize the co-authorship network\n",
    "plt.figure(figsize=(10, 6))\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, font_size=10, font_color='black', node_size=800, node_color='skyblue', edge_color='gray', width=1)\n",
    "plt.title('Co-Authorship Network')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we create a co-authorship network using a synthetic dataset. Each edge represents a co-authorship relationship between two authors. Visualize the co-authorship network to gain insights into collaboration patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.1. Structuring unstructured data for knowledge mining: a data-driven approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structuring unstructured data is a critical step in knowledge mining, as vast amounts of valuable information exist in unstructured formats such as text, images, and audio. A data-driven approach involves leveraging data mining techniques to extract meaningful structures, patterns, and knowledge from unstructured data sources. This process enables the transformation of raw, unstructured data into a format suitable for advanced analysis and knowledge discovery.\n",
    "\n",
    "#### Key points about structuring unstructured data for knowledge mining:\n",
    "\n",
    "- Unstructured Data Challenges:\n",
    "        Unstructured data lacks a predefined data model, making it challenging to analyze and extract insights. This includes data in the form of text documents, images, videos, and other media.\n",
    "\n",
    "- Data-Driven Structuring:\n",
    "        A data-driven approach involves using data mining methodologies to automatically structure unstructured data. This can include techniques such as natural language processing, image recognition, and audio analysis to uncover patterns and relationships.\n",
    "\n",
    "- Feature Extraction:\n",
    "        Feature extraction is a key aspect of structuring unstructured data. It involves identifying relevant features or attributes from the raw data, making it amenable to analysis using traditional data mining algorithms.\n",
    "\n",
    "- Text Mining and NLP:\n",
    "        In the case of text data, natural language processing (NLP) techniques are applied for tasks such as entity recognition, sentiment analysis, and topic modeling. These techniques extract structured information from textual content.\n",
    "\n",
    "- Image and Audio Processing:\n",
    "        For images and audio data, data-driven approaches involve methods like image recognition and speech-to-text conversion. These techniques convert visual and auditory information into structured representations.\n",
    "\n",
    "- Real-World Applications:\n",
    "        Structuring unstructured data is crucial in applications such as information retrieval, content recommendation, and predictive analytics. It enables organizations to derive valuable insights from diverse data sources.\n",
    "\n",
    "#### A real-world example of practical use in Python for structuring unstructured text data using natural language processing (NLP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset with unstructured text data\n",
    "data = {'Text': ['Data mining is an exciting field with vast potential.',\n",
    "                 'Natural language processing plays a crucial role in structuring text data.',\n",
    "                 'Unstructured data, when properly analyzed, reveals valuable insights.']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Text vectorization using CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['Text'])\n",
    "\n",
    "# Apply Latent Dirichlet Allocation (LDA) for topic modeling\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "topics = lda.fit_transform(X)\n",
    "\n",
    "# Display the extracted topics\n",
    "df['Topic'] = topics.argmax(axis=1)\n",
    "df['Topic Keywords'] = df['Topic'].apply(lambda x: lda.components_[x].argsort()[:-3:-1])\n",
    "\n",
    "print(\"Structured Data with Topics:\")\n",
    "print(df[['Text', 'Topic', 'Topic Keywords']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the CountVectorizer for text vectorization and Latent Dirichlet Allocation (LDA) for topic modeling. The result is a structured representation of the unstructured text data with identified topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.2. Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used to artificially increase the size of a dataset by applying various transformations to the existing data. This approach is particularly beneficial in machine learning and data mining, as it helps improve model generalization and performance by exposing the model to diverse variations of the original data. Data augmentation is commonly employed in tasks such as image recognition, natural language processing, and other domains where large labeled datasets may be limited.\n",
    "\n",
    "#### Key points about data augmentation:\n",
    "\n",
    "- Increased Dataset Size:\n",
    "        Data augmentation involves creating new training instances by applying random transformations to the existing data. This effectively increases the size of the dataset available for model training.\n",
    "\n",
    "- Improving Model Generalization:\n",
    "        By exposing the model to diverse variations of the input data, data augmentation helps improve the model's ability to generalize to unseen examples. This is especially important when dealing with limited labeled data.\n",
    "\n",
    "- Common Transformations:\n",
    "        Common data augmentation techniques include rotation, flipping, cropping, zooming, and changes in brightness and contrast for image data. For text data, augmentation may involve synonym replacement, word insertion, or deletion.\n",
    "\n",
    "- Application in Image Recognition:\n",
    "        In image recognition tasks, data augmentation is widely used to train models robust to variations in image appearance, such as changes in viewpoint, lighting conditions, and object orientation.\n",
    "\n",
    "- Avoiding Overfitting:\n",
    "        Data augmentation is a regularization technique that helps prevent overfitting, especially when the available labeled data is limited. It introduces diversity into the training set, making the model more resilient to variations in the test set.\n",
    "\n",
    "#### Real-world example of practical use in Python for data augmentation in image classification using the Keras library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example image data\n",
    "image_path = 'path_to_your_image.jpg'\n",
    "image = plt.imread(image_path)\n",
    "\n",
    "# Display the original image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Data augmentation using Keras ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Reshape the image to (1, height, width, channels) for flow() function\n",
    "image = image.reshape((1,) + image.shape)\n",
    "\n",
    "# Generate augmented images\n",
    "augmented_images = []\n",
    "for _ in range(5):  # Generate 5 augmented images\n",
    "    augmented_batch = datagen.flow(image, batch_size=1)\n",
    "    augmented_images.append(augmented_batch[0][0])\n",
    "\n",
    "# Display augmented images\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i, augmented_image in enumerate(augmented_images):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(augmented_image)\n",
    "    plt.title(f'Augmented Image {i+1}')\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Keras ImageDataGenerator to perform data augmentation on an example image. The augmentation includes rotation, width and height shifts, shearing, zooming, and horizontal flipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.3. From correlation to causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving from correlation to causality in data mining involves going beyond identifying relationships between variables to understand the cause-and-effect relationships that govern the data. While correlation measures the statistical association between variables, establishing causality requires more nuanced analysis and often involves experimentation or advanced statistical methods. Recognizing causality is crucial for making informed decisions and predictions in various domains.\n",
    "\n",
    "#### Key points about moving from correlation to causality:\n",
    "\n",
    "- Correlation vs. Causation:\n",
    "        Correlation indicates a statistical association between two variables, but it does not imply causation. Establishing causality involves demonstrating that changes in one variable directly influence changes in another.\n",
    "\n",
    "- Experimental Design:\n",
    "        Controlled experiments are often employed to establish causality. Randomized controlled trials (RCTs) involve manipulating one variable while keeping others constant to observe the causal effect.\n",
    "\n",
    "- Counterfactual Analysis:\n",
    "        Counterfactual analysis compares observed outcomes with what would have happened in the absence of a particular intervention. This approach helps attribute changes to a specific cause.\n",
    "\n",
    "- Causal Inference Methods:\n",
    "        Advanced statistical methods, such as instrumental variable analysis and causal inference frameworks, are used to infer causality from observational data. These methods aim to account for confounding factors that may influence both the cause and the effect.\n",
    "\n",
    "- Domain Applications:\n",
    "        Moving from correlation to causality is relevant in fields such as healthcare, economics, and social sciences. Understanding the causal relationships between variables is essential for designing effective interventions and policies.\n",
    "\n",
    "#### A real-world example of practical use in Python for exploring causality using a simple simulation of an A/B test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Simulate data for an A/B test\n",
    "np.random.seed(42)\n",
    "\n",
    "# Control group (no intervention)\n",
    "control_group = np.random.normal(loc=10, scale=5, size=1000)\n",
    "\n",
    "# Experimental group (intervention)\n",
    "experimental_group = np.random.normal(loc=12, scale=5, size=1000)\n",
    "\n",
    "# Perform t-test to assess causality\n",
    "t_stat, p_value = ttest_ind(control_group, experimental_group)\n",
    "\n",
    "# Display the distributions and results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(control_group, bins=30, alpha=0.5, label='Control Group')\n",
    "plt.hist(experimental_group, bins=30, alpha=0.5, label='Experimental Group')\n",
    "plt.title('A/B Test Simulation')\n",
    "plt.xlabel('Outcome Variable')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'T-test results: t-statistic = {t_stat}, p-value = {p_value}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we simulate an A/B test where the control group represents the status quo (no intervention), and the experimental group represents the group with an intervention. The t-test is then used to assess whether there is a statistically significant difference between the two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.4. Network as a context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the network as a context in data mining involves acknowledging the interconnected nature of data, where relationships and interactions between entities play a crucial role. Networks can represent social connections, information flow, dependencies, or any form of relational structure in the data. Analyzing data within the context of networks allows for a deeper understanding of how entities influence and are influenced by their connections.\n",
    "\n",
    "#### Key points about using the network as a context:\n",
    "\n",
    "- Network Representation:\n",
    "        Data can be represented as a network, where entities (nodes) are connected by relationships or interactions (edges). This representation is applicable to various domains, including social networks, biological systems, and information flow.\n",
    "\n",
    "- Contextual Insights:\n",
    "        Analyzing data within the context of a network provides insights into the influence and impact of relationships. It allows for a more comprehensive understanding of how entities function within their connected environment.\n",
    "\n",
    "- Community Detection:\n",
    "        Community detection within networks reveals groups of entities that are densely connected internally. This helps identify substructures and functional units within the data.\n",
    "\n",
    "- Centrality Analysis:\n",
    "        Centrality measures, such as degree centrality or betweenness centrality, highlight the importance of nodes in the network. This analysis identifies key entities that play significant roles in information flow or influence.\n",
    "\n",
    "- Temporal Aspects:\n",
    "        Considering the temporal aspects of network data adds an additional layer of complexity. Analyzing how relationships evolve over time provides a dynamic perspective on the data.\n",
    "\n",
    "#### A real-world example of practical use in Python for analyzing a social network using network analysis techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample social network graph\n",
    "G = nx.Graph()\n",
    "edges = [('Alice', 'Bob'), ('Bob', 'Charlie'), ('Charlie', 'Alice'), ('Alice', 'David'), ('David', 'Eve')]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Visualize the social network graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, font_size=10, font_color='black', node_size=800, node_color='skyblue', edge_color='gray', width=1)\n",
    "plt.title('Social Network Graph')\n",
    "plt.show()\n",
    "\n",
    "# Analyze network centrality\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "print(\"Node Degree Centrality:\")\n",
    "for node, centrality in degree_centrality.items():\n",
    "    print(f\"{node}: {centrality}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we create a simple social network graph using the NetworkX library. The graph represents relationships between individuals. We then visualize the graph and calculate degree centrality to identify the importance of nodes in the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.5. Auto-ML: methods and systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoML, or Automated Machine Learning, refers to the development of methods and systems that automate the end-to-end process of applying machine learning to real-world problems. AutoML aims to make machine learning accessible to non-experts and streamline the model development process by automating tasks such as feature engineering, algorithm selection, and hyperparameter tuning. This trend in data mining methodologies has gained prominence for its potential to democratize machine learning and accelerate model deployment.\n",
    "\n",
    "#### Key points about AutoML methods and systems:\n",
    "\n",
    "- Automated Model Selection:\n",
    "        AutoML systems automatically explore and select the most suitable machine learning algorithms for a given task, taking into account the characteristics of the data.\n",
    "\n",
    "- Hyperparameter Optimization:\n",
    "        Optimization of hyperparameters, which significantly impact model performance, is automated to find the best configuration for a given algorithm.\n",
    "\n",
    "- Feature Engineering Automation:\n",
    "        Feature engineering, the process of creating relevant features for model training, is automated to reduce the manual effort required in data preprocessing.\n",
    "\n",
    "- Pipeline Construction:\n",
    "        AutoML tools construct end-to-end machine learning pipelines, encompassing data preprocessing, feature engineering, model training, and evaluation.\n",
    "\n",
    "- Scalability and Efficiency:\n",
    "        AutoML systems are designed to handle large datasets efficiently and can scale to meet the demands of complex machine learning tasks.\n",
    "\n",
    "- Democratizing Machine Learning:\n",
    "        By automating intricate aspects of machine learning, AutoML democratizes access to machine learning capabilities, enabling individuals with varying levels of expertise to leverage advanced analytics.\n",
    "\n",
    "#### a real-world example of practical use in Python for AutoML using the auto-sklearn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import autosklearn.classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the example digits dataset\n",
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, random_state=42)\n",
    "\n",
    "# Initialize and fit an AutoML classifier\n",
    "automl_classifier = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=120, per_run_time_limit=30)\n",
    "automl_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = automl_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"AutoML Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the auto-sklearn library to create an AutoML classifier for the digits dataset. The AutoML system automatically explores different algorithms, hyperparameters, and preprocessing techniques to find the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.1. Privacy-preserving data mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Privacy-preserving data mining addresses the challenge of extracting valuable insights from data while protecting the privacy of individuals whose information is part of the dataset. As data mining techniques become more advanced, there is an increased need to develop methods that prevent the disclosure of sensitive information. Privacy-preserving data mining techniques enable the analysis of data without compromising the confidentiality of personal details, fostering ethical and responsible data use.\n",
    "\n",
    "#### Key points about privacy-preserving data mining:\n",
    "\n",
    "- Sensitive Data Protection:\n",
    "        Privacy-preserving data mining aims to protect sensitive information within datasets, such as personal identifiers, health records, or financial details.\n",
    "\n",
    "- Cryptographic Techniques:\n",
    "        Cryptographic methods, such as homomorphic encryption and secure multi-party computation, are employed to perform computations on encrypted data without revealing the underlying information.\n",
    "\n",
    "- Differential Privacy:\n",
    "        Differential privacy ensures that the inclusion or exclusion of an individual's data does not significantly impact the outcome of data mining algorithms. It adds noise or randomness to the data to prevent re-identification.\n",
    "\n",
    "- Anonymization and De-identification:\n",
    "        Techniques like anonymization and de-identification involve removing or modifying personal information to reduce the risk of identifying individuals.\n",
    "\n",
    "- Ethical Considerations:\n",
    "        Privacy-preserving data mining is driven by ethical considerations, aiming to balance the benefits of data analysis with the protection of individuals' privacy rights.\n",
    "\n",
    "#### A real-world example of practical use in Python for privacy-preserving data mining using the diffprivlib library for differential privacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from diffprivlib.models import LogisticRegression as DiffPrivLogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model without privacy preservation\n",
    "normal_model = LogisticRegression()\n",
    "normal_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "normal_predictions = normal_model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy without privacy preservation\n",
    "normal_accuracy = accuracy_score(y_test, normal_predictions)\n",
    "print(f\"Normal Model Accuracy: {normal_accuracy}\")\n",
    "\n",
    "# Train a logistic regression model with differential privacy\n",
    "privacy_model = DiffPrivLogisticRegression(epsilon=1.0, data_norm=np.linalg.norm(X_train, axis=0))\n",
    "privacy_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set with differential privacy\n",
    "privacy_predictions = privacy_model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy with differential privacy\n",
    "privacy_accuracy = accuracy_score(y_test, privacy_predictions)\n",
    "print(f\"Privacy-Preserving Model Accuracy: {privacy_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the diffprivlib library to train a logistic regression model on the Iris dataset with and without privacy preservation. The epsilon parameter controls the privacy level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.2. Human-algorithm interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human-algorithm interaction explores the dynamic interplay between humans and algorithms in the context of data mining and machine learning. This trend acknowledges the pivotal role of human involvement in the data mining process, emphasizing collaboration, interpretability, and user-friendly interfaces. By fostering a symbiotic relationship between humans and algorithms, this approach aims to enhance the effectiveness, transparency, and ethical considerations in data-driven decision-making.\n",
    "\n",
    "#### Key points about human-algorithm interaction:\n",
    "\n",
    "- Collaborative Decision-Making:\n",
    "        Human-algorithm interaction promotes collaboration between data scientists, domain experts, and end-users to collectively make decisions and derive meaningful insights from data.\n",
    "\n",
    "- Interpretability and Explainability:\n",
    "        Algorithms are designed to be interpretable and explainable, enabling users to understand and trust the decisions made by the models. This is particularly important in critical domains such as healthcare and finance.\n",
    "\n",
    "- User-Friendly Interfaces:\n",
    "        The development of user-friendly interfaces facilitates seamless interaction between humans and algorithms, allowing users with varying levels of technical expertise to actively participate in the data analysis process.\n",
    "\n",
    "- Feedback Mechanisms:\n",
    "        Incorporating feedback mechanisms enables continuous improvement of algorithms based on user input, preferences, and domain-specific knowledge.\n",
    "\n",
    "- Ethical Considerations:\n",
    "        Human-algorithm interaction considers ethical implications, ensuring that decision-making processes are fair, transparent, and aligned with societal values.\n",
    "\n",
    "#### A real-world example of practical use in Python for human-algorithm interaction using an interactive visualization library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Perform dimensionality reduction using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Create an interactive scatter plot using Plotly Express\n",
    "df = pd.DataFrame({'PCA1': X_pca[:, 0], 'PCA2': X_pca[:, 1], 'Species': iris.target_names[y]})\n",
    "fig = px.scatter(df, x='PCA1', y='PCA2', color='Species', title='Interactive PCA Plot')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Plotly Express library to create an interactive scatter plot of the Iris dataset after applying PCA for dimensionality reduction. The interactive plot allows users to explore the data and gain insights visually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.3. Mining beyond maximizing accuracy: fairness, interpretability, and robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mining beyond maximizing accuracy in data mining involves considering additional factors such as fairness, interpretability, and robustness. While accuracy is essential, it may not be the sole criterion for evaluating a model's performance, especially in scenarios where ethical considerations, human understanding, and resilience to adversarial attacks are crucial. This trend highlights the importance of developing models that are fair, interpretable, and robust in real-world applications.\n",
    "\n",
    "#### Key points about mining beyond maximizing accuracy:\n",
    "\n",
    "- Fairness:\n",
    "        Fairness in data mining involves ensuring that models do not exhibit biases that could lead to discriminatory outcomes, especially for different demographic groups. Fairness metrics and algorithms are used to mitigate and measure bias.\n",
    "\n",
    "- Interpretability:\n",
    "        Interpretable models are designed to provide clear explanations of their decisions. This is crucial in applications where transparency is required, such as in healthcare or finance, to build trust and facilitate decision-making.\n",
    "\n",
    "- Robustness:\n",
    "        Robust models are resistant to adversarial attacks and variations in the input data. Ensuring robustness is important in applications where the model's performance can be impacted by intentional manipulation or unforeseen changes in the data distribution.\n",
    "\n",
    "- Ethical Considerations:\n",
    "        Mining beyond accuracy emphasizes ethical considerations in the development and deployment of data mining models. This includes addressing biases, ensuring transparency, and prioritizing fairness in decision-making.\n",
    "\n",
    "#### A real-world example of practical use in Python for ensuring fairness in a machine learning model using the Fairness Indicators library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from fairness_indicators import FairnessIndicators\n",
    "\n",
    "# Load the dataset (example: Adult Census Income dataset)\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "data = pd.read_csv(url, names=columns, delimiter=',\\s', na_values=['?'], engine='python')\n",
    "\n",
    "# Preprocess the data\n",
    "data = data.dropna()\n",
    "X = pd.get_dummies(data.drop('income', axis=1))\n",
    "y = (data['income'] == '>50K').astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a random forest classifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate fairness using Fairness Indicators\n",
    "fi = FairnessIndicators(adt=model, feature_colnames=list(X.columns), label_colname='income', favorable_classes=[1])\n",
    "fi.fit(X_train, y_train)\n",
    "fi.plot_fairness_indicators()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Fairness Indicators library to assess fairness in a machine learning model trained on the Adult Census Income dataset. The fairness indicators provide insights into potential biases, allowing for further investigation and mitigation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.4. Data mining for social good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data mining for social good involves leveraging data mining techniques to address societal challenges and contribute to positive social impact. By applying data mining to domains such as public health, education, environmental conservation, and humanitarian efforts, researchers and practitioners can derive insights that lead to informed decision-making and innovative solutions for the betterment of society.\n",
    "\n",
    "#### Key points about data mining for social good:\n",
    "\n",
    "- Societal Challenges:\n",
    "        Data mining for social good focuses on tackling pressing societal challenges, such as poverty, healthcare disparities, environmental sustainability, and access to education.\n",
    "\n",
    "- Informed Decision-Making:\n",
    "        The application of data mining enables decision-makers, policymakers, and organizations to make informed and evidence-based decisions, leading to more effective interventions and strategies.\n",
    "\n",
    "- Humanitarian Efforts:\n",
    "        Data mining is used in humanitarian efforts to optimize resource allocation, response planning, and disaster management. It plays a crucial role in improving the efficiency and effectiveness of aid delivery.\n",
    "\n",
    "- Public Health Initiatives:\n",
    "        Data mining contributes to public health initiatives by analyzing healthcare data to identify disease patterns, predict outbreaks, and optimize healthcare delivery systems.\n",
    "\n",
    "- Education and Equality:\n",
    "        In the realm of education, data mining can be employed to address issues related to access, quality, and equality, helping to tailor educational interventions to individual needs.\n",
    "\n",
    "- Ethical Considerations:\n",
    "        Data mining for social good emphasizes ethical considerations, ensuring that the use of data is respectful of privacy, transparent, and aligned with the goal of benefiting society.\n",
    "\n",
    "#### Real-world example of practical use in Python for data mining for social good using a dataset related to public health and disease prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load a public health dataset (example: Breast Cancer Wisconsin dataset)\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'\n",
    "columns = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave_points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n",
    "data = pd.read_csv(url, names=columns, header=None)\n",
    "\n",
    "# Preprocess the data\n",
    "X = data.drop(['id', 'diagnosis'], axis=1)\n",
    "y = (data['diagnosis'] == 'M').astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a random forest classifier for disease prediction\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy for disease prediction\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy for Disease Prediction: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Breast Cancer Wisconsin dataset to train a machine learning model for disease prediction. This type of application contributes to public health initiatives, aiding in the early detection and diagnosis of diseases."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
