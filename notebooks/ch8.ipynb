{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.3. Overview of basic clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster analysis is a fundamental technique in data mining that involves grouping similar data points together into clusters or groups. It helps in identifying patterns and structures within datasets, facilitating insights into the inherent organization of the data.\n",
    "\n",
    "This section explores the core concepts and methods of cluster analysis, emphasizing the techniques used to partition data into meaningful groups.\n",
    "\n",
    "Various clustering methods exist, each with its strengths and suitable scenarios. Common clustering methods include K-Means, Hierarchical Clustering, and DBSCAN (Density-Based Spatial Clustering of Applications with Noise). Each method uses different approaches to define clusters based on the characteristics of the data.\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of K-Means clustering using the famous Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardize the features\n",
    "X_standardized = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X_standardized)\n",
    "\n",
    "# Add cluster labels to the original dataset\n",
    "iris_clustered = pd.DataFrame(data=X, columns=iris.feature_names)\n",
    "iris_clustered['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(iris_clustered['sepal length (cm)'], iris_clustered['sepal width (cm)'], c=iris_clustered['Cluster'], cmap='viridis')\n",
    "plt.title('K-Means Clustering of Iris Dataset')\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the K-Means clustering algorithm to cluster the Iris dataset based on sepal length and sepal width. The data is standardized to ensure that all features have the same scale. The resulting clusters are visualized in a scatter plot, with each point colored according to its assigned cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1. k-Means: a centroid-based technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning methods involve dividing data points into clusters where each point belongs to only one cluster. One of the widely used partitioning techniques is k-Means.\n",
    "\n",
    "k-Means is a centroid-based clustering algorithm. It aims to partition the data into k clusters, where each cluster is represented by its centroid. The algorithm iteratively assigns data points to the nearest centroid and updates the centroids based on the assigned points.\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of k-Means clustering using a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a synthetic dataset with three clusters\n",
    "X, y = make_blobs(n_samples=300, centers=3, random_state=42)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Visualize the clusters and centroids\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "plt.title('k-Means Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the make_blobs function from scikit-learn to create a synthetic dataset with three clusters. The K-Means algorithm is then applied to cluster the data into three groups. The resulting clusters and centroids are visualized in a scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2. Variations of k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several variations of the k-Means algorithm exist, each designed to address specific challenges or improve performance. Examples include K-Means++, Mini-Batch K-Means, and the use of different distance metrics.\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of Mini-Batch K-Means using the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardize the features\n",
    "X_standardized = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply Mini-Batch K-Means clustering\n",
    "mbk = MiniBatchKMeans(n_clusters=3, batch_size=50, random_state=42)\n",
    "mbk.fit(X_standardized)\n",
    "\n",
    "# Reduce dimensionality for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=mbk.labels_, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.scatter(mbk.cluster_centers_[:, 0], mbk.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "plt.title('Mini-Batch K-Means Clustering of Iris Dataset')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use Mini-Batch K-Means to cluster the Iris dataset. The data is standardized, and dimensionality is reduced using PCA for visualization. Mini-Batch K-Means is a variation of the classic algorithm that processes small batches of data at each iteration, making it more scalable for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2. Agglomerative hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agglomerative Hierarchical Clustering starts with each data point as a separate cluster and iteratively merges the closest pairs of clusters until only one cluster remains. The process is governed by a linkage criterion, such as Ward's method or average linkage.\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of Agglomerative Hierarchical Clustering using the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Apply Agglomerative Hierarchical Clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=3)\n",
    "agg_labels = agg_clustering.fit_predict(X)\n",
    "\n",
    "# Create a linkage matrix for dendrogram\n",
    "linkage_matrix = linkage(X, method='ward')\n",
    "\n",
    "# Visualize the dendrogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "dendrogram(linkage_matrix, truncate_mode='level', p=3, show_leaf_counts=False, no_labels=True)\n",
    "plt.title('Dendrogram for Agglomerative Hierarchical Clustering')\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, Agglomerative Hierarchical Clustering is applied to the Iris dataset. The resulting dendrogram visually represents the hierarchy of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.3. Divisive hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divisive Hierarchical Clustering starts with all data points in a single cluster and recursively divides the cluster into sub-clusters. The process continues until each data point is in its cluster. This approach requires defining a criterion for splitting clusters.\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "Divisive Hierarchical Clustering is less common than Agglomerative Hierarchical Clustering in practice, and scikit-learn does not provide a specific implementation for divisive clustering. However, you can achieve divisive clustering using agglomerative clustering and manipulating the dendrogram. Here's a simplified example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create a linkage matrix for dendrogram\n",
    "linkage_matrix = linkage(X, method='ward')\n",
    "\n",
    "# Apply hierarchical clustering\n",
    "labels = fcluster(linkage_matrix, t=3, criterion='maxclust')\n",
    "\n",
    "# Visualize the dendrogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "dendrogram(linkage_matrix, truncate_mode='level', p=3, show_leaf_counts=False, no_labels=True)\n",
    "plt.title('Dendrogram for Divisive Hierarchical Clustering')\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.title('Divisive Hierarchical Clustering of Iris Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use hierarchical clustering with the fcluster function to perform divisive clustering on the Iris dataset. The dendrogram shows the hierarchy of clusters, and the scatter plot visualizes the resulting clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.4. BIRCH: scalable hierarchical clustering using clustering feature trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIRCH is a hierarchical clustering algorithm that utilizes a tree structure to efficiently organize and represent clusters. It employs a Clustering Feature Tree (CF Tree) to summarize the data and iteratively refines clusters in a balanced and memory-efficient manner.\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "BIRCH is available in scikit-learn. Let's use it on a synthetic dataset for illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a synthetic dataset with three clusters\n",
    "X, y = make_blobs(n_samples=300, centers=3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "X_standardized = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply BIRCH clustering\n",
    "birch = Birch(threshold=0.5, n_clusters=3)\n",
    "birch_labels = birch.fit_predict(X_standardized)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=birch_labels, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.title('BIRCH Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the BIRCH algorithm to cluster a synthetic dataset with three clusters. The threshold parameter controls the compactness of subclusters within a CF Tree. Adjust the parameters based on your specific use case and dataset. BIRCH is particularly useful for large datasets where memory efficiency is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.5. Probabilistic hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic hierarchical clustering involves using probabilistic models to describe the uncertainty or likelihood of data points belonging to different clusters in a hierarchical structure. Gaussian Mixture Models (GMMs) are an example of probabilistic clustering models.\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "Let's use Gaussian Mixture Models (GMMs) as an example of probabilistic hierarchical clustering. GMMs allow data points to belong to multiple clusters with varying probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a synthetic dataset with three clusters\n",
    "X, y = make_blobs(n_samples=300, centers=3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "X_standardized = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply Gaussian Mixture Model clustering\n",
    "gmm = GaussianMixture(n_components=3)\n",
    "gmm.fit(X_standardized)\n",
    "gmm_labels = gmm.predict(X_standardized)\n",
    "\n",
    "# Visualize the clusters with probabilities\n",
    "probs = gmm.predict_proba(X_standardized)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=gmm_labels, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.title('Probabilistic Hierarchical Clustering (GMM)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Plot ellipses representing cluster shapes based on covariances\n",
    "for i in range(3):\n",
    "    plt.contour(X[:, 0], X[:, 1], probs[:, i], colors='black', linewidths=0.5, levels=[0.1, 0.5, 0.9])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use a Gaussian Mixture Model (GMM) for probabilistic clustering on a synthetic dataset. The clusters are represented with varying probabilities, and ellipses depict the shapes of the clusters based on the covariance matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.1. DBSCAN: density-based clustering based on connected regions with high density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN is a density-based clustering algorithm that groups together data points that are close to each other and have a sufficient number of neighboring points, forming dense regions. It also identifies noise points that do not belong to any cluster.\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "Let's use DBSCAN to cluster the Iris dataset, which has distinct clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardize the features\n",
    "X_standardized = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X_standardized)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=dbscan_labels, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.title('DBSCAN Clustering of Iris Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use DBSCAN to cluster the Iris dataset. The eps parameter controls the maximum distance between two samples for one to be considered as part of the neighborhood, and min_samples sets the minimum number of samples required to form a dense region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.2. DENCLUE: clustering based on density distribution functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DENCLUE is a density-based clustering algorithm that represents clusters using density distribution functions. It identifies clusters as regions of high density in the feature space, allowing for more flexible cluster shapes compared to traditional methods.\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "DENCLUE is not as widely used as some other clustering algorithms, and it might not be available in standard machine learning libraries like scikit-learn. However, you can implement it using third-party libraries. Here's a simplified example using the DENCLUE implementation from the hdbscan library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hdbscan\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a synthetic dataset with three clusters\n",
    "X, y = make_blobs(n_samples=300, centers=3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "X_standardized = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply DENCLUE clustering\n",
    "denclue = hdbscan.HDBSCAN(min_cluster_size=5, prediction_data=True)\n",
    "denclue_labels = denclue.fit_predict(X_standardized)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=denclue_labels, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.title('DENCLUE Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the hdbscan library, which provides an implementation of DENCLUE through the HDBSCAN class. The min_cluster_size parameter controls the minimum number of samples required to form a dense region. Adjust parameters based on your specific use case and dataset. Note that for a full implementation of DENCLUE, you might need to refer to specific research papers or implement it from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.3. Grid-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid-based methods divide the data space into a grid structure, creating cells or bins. Clustering is then performed based on the distribution of data points within these cells. A popular example is STING (Statistical Information Grid).\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "For a practical example, we can use the K-Means algorithm, which, while not strictly grid-based, can be applied to grid-like structures. Here, we'll create a synthetic dataset with a grid-like structure and apply K-Means clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a synthetic dataset with a grid-like structure\n",
    "X, y = make_blobs(n_samples=300, centers=3, cluster_std=0.5, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "X_standardized = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X_standardized)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.title('K-Means Clustering on Grid-Like Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we create a synthetic dataset with a grid-like structure using the make_blobs function. K-Means clustering is then applied to the standardized data. Adjust the dataset characteristics and K-Means parameters based on your specific use case. While this is not a grid-based method in the strict sense, it showcases how clustering algorithms can be applied to datasets with grid-like patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.1. Assessing clustering tendency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing clustering tendency is a crucial step in cluster analysis, as it helps determine whether meaningful clusters exist in the dataset. This process involves evaluating whether the data naturally forms distinct groups or if it is randomly distributed. Two common methods for assessing clustering tendency are:\n",
    "\n",
    "- Hopkins Statistic:\n",
    "    The Hopkins statistic measures the tendency of a dataset to form clusters. It compares the distance distribution between randomly selected points and the actual data points. A low Hopkins score indicates a higher likelihood of clustering.\n",
    "\n",
    "- Silhouette Score:\n",
    "    The silhouette score quantifies how well-separated clusters are. It ranges from -1 to 1, where a high positive value indicates well-defined clusters, a score around 0 suggests overlapping clusters, and negative values imply incorrect clustering.\n",
    "\n",
    "These methods aid in deciding whether to proceed with cluster analysis and which clustering algorithm may be most appropriate for the dataset.\n",
    "\n",
    "#### Practical use in Python for assessing clustering tendency using the Hopkins statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate the Hopkins statistic\n",
    "def hopkins_statistic(X):\n",
    "    n, d = X.shape\n",
    "\n",
    "    # Create synthetic data for comparison\n",
    "    synthetic_data = np.random.rand(n, d)\n",
    "\n",
    "    # Calculate distance between real data points\n",
    "    real_data_distances = NearestNeighbors(n_neighbors=1).fit(X).kneighbors(X)[0].ravel()\n",
    "\n",
    "    # Calculate distance between synthetic data points\n",
    "    synthetic_data_distances = NearestNeighbors(n_neighbors=1).fit(synthetic_data).kneighbors(synthetic_data)[0].ravel()\n",
    "\n",
    "    # Calculate Hopkins Statistic\n",
    "    hopkins_stat = sum(real_data_distances) / (sum(real_data_distances) + sum(synthetic_data_distances))\n",
    "\n",
    "    return hopkins_stat\n",
    "\n",
    "# Generate synthetic data (replace this with your dataset)\n",
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Standardize the data\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Calculate Hopkins Statistic\n",
    "hopkins_score = hopkins_statistic(X)\n",
    "\n",
    "print(f\"Hopkins Statistic: {hopkins_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data using make_blobs from scikit-learn, standardize it, and then calculate the Hopkins Statistic using the hopkins_statistic function. The result gives an indication of the clustering tendency of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.2. Determining the number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the optimal number of clusters is a critical aspect of cluster analysis. A variety of methods can be employed for this purpose, and some common approaches include:\n",
    "\n",
    "- Elbow Method:\n",
    "        The Elbow Method involves plotting the explained variation as a function of the number of clusters and looking for the \"elbow\" point. The point where the rate of decrease sharply changes may indicate the optimal number of clusters.\n",
    "\n",
    "- Silhouette Analysis:\n",
    "        Silhouette analysis assesses how well-separated clusters are. It calculates a silhouette score for different numbers of clusters, and a higher score suggests a better-defined clustering.\n",
    "\n",
    "- Gap Statistics:\n",
    "        Gap statistics compare the clustering quality of the dataset with that of a random distribution. A larger gap statistic indicates a more appropriate number of clusters.\n",
    "\n",
    "Selecting the right method depends on the characteristics of the dataset and the desired outcome of the analysis.\n",
    "\n",
    "#### Real-world example of practical use in Python for determining the number of clusters using the Elbow Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate the distortion (inertia) for a range of clusters\n",
    "def calculate_distortion(X, max_clusters):\n",
    "    distortions = []\n",
    "    for i in range(1, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "        kmeans.fit(X)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "    return distortions\n",
    "\n",
    "# Generate synthetic data (replace this with your dataset)\n",
    "X, _ = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Calculate distortions for a range of clusters\n",
    "max_clusters = 10\n",
    "distortions = calculate_distortion(X, max_clusters)\n",
    "\n",
    "# Plot the Elbow Method graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, max_clusters + 1), distortions, marker='o')\n",
    "plt.title('Elbow Method for Optimal Cluster Number')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Distortion (Inertia)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data using make_blobs from scikit-learn, and then apply the Elbow Method to find the optimal number of clusters. The \"elbow\" point in the graph indicates the suggested number of clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.3. Measuring clustering quality: extrinsic methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrinsic methods for measuring clustering quality involve evaluating the performance of clustering algorithms using external information, such as ground truth labels or known class memberships. Common extrinsic metrics include:\n",
    "\n",
    "1. Adjusted Rand Index (ARI):\n",
    "        ARI measures the similarity between the true labels and the labels assigned by the clustering algorithm. It considers all pairs of samples and assesses whether they are placed in the same or different clusters, providing a score between -1 and 1.\n",
    "\n",
    "2. Normalized Mutual Information (NMI):\n",
    "        NMI quantifies the mutual information between the true labels and the predicted labels, normalized by the entropy of the labels. Like ARI, NMI ranges from 0 to 1, with higher values indicating better clustering quality.\n",
    "\n",
    "These metrics help assess how well a clustering algorithm aligns with the known structure of the data, providing insight into its effectiveness in capturing meaningful patterns.\n",
    "\n",
    "#### Example of practical use in Python for measuring clustering quality using the Adjusted Rand Index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data with ground truth labels (replace this with your dataset)\n",
    "X, true_labels = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Apply a clustering algorithm (KMeans in this case)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "predicted_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Calculate Adjusted Rand Index\n",
    "ari_score = adjusted_rand_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"Adjusted Rand Index: {ari_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with ground truth labels using make_blobs from scikit-learn, apply the KMeans clustering algorithm, and calculate the Adjusted Rand Index to measure the clustering quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.4. Intrinsic methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intrinsic methods for measuring clustering quality assess the performance of clustering algorithms based solely on the internal structure of the data, without relying on external information. Common intrinsic metrics include:\n",
    "\n",
    "- Silhouette Score:\n",
    "        The silhouette score quantifies how well-separated clusters are. It is calculated for each data point and ranges from -1 to 1. A higher silhouette score indicates better-defined clusters.\n",
    "\n",
    "- Davies-Bouldin Index:\n",
    "        The Davies-Bouldin Index measures the compactness and separation between clusters. A lower Davies-Bouldin Index suggests better clustering, with minimal intra-cluster distance and maximal inter-cluster distance.\n",
    "\n",
    "- Calinski-Harabasz Index:\n",
    "        The Calinski-Harabasz Index evaluates the ratio of between-cluster variance to within-cluster variance. A higher index indicates better-defined clusters.\n",
    "\n",
    "These metrics provide insights into the inherent quality of the clusters formed by the algorithm, without requiring external validation.\n",
    "\n",
    "#### A real-world example of practical use in Python for measuring clustering quality using the Silhouette Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data (replace this with your dataset)\n",
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Apply KMeans clustering with different numbers of clusters\n",
    "cluster_range = range(2, 11)\n",
    "silhouette_scores = []\n",
    "\n",
    "for n_clusters in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    silhouette_scores.append(silhouette_score(X, labels))\n",
    "\n",
    "# Plot the Silhouette Scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(cluster_range, silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Score for Optimal Cluster Number')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data using make_blobs from scikit-learn, apply KMeans clustering with different numbers of clusters, and plot the Silhouette Scores. The number of clusters with the highest silhouette score can be considered optimal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining_2024_spring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
