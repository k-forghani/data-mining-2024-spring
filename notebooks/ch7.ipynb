{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1. Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is a crucial step in the data preprocessing phase, involving the identification and selection of the most relevant features for a particular task. Filter methods are a category of feature selection techniques that evaluate the relevance of features based on certain statistical measures or scoring criteria. These methods assess the characteristics of individual features independently of the machine learning model.\n",
    "\n",
    "#### Key Points:\n",
    "\n",
    "1. Independence:\n",
    "        Filter methods assess each feature's relevance independently of other features.\n",
    "\n",
    "2. Scoring Criteria:\n",
    "        Features are ranked or scored based on statistical measures, such as correlation, mutual information, or statistical tests.\n",
    "\n",
    "3. Preprocessing:\n",
    "        Filter methods are applied as a preprocessing step before training a machine learning model.\n",
    "\n",
    "4. Selection Threshold:\n",
    "        A threshold is set to select the top-ranked features, and the rest are discarded.\n",
    "\n",
    "5. Advantages:\n",
    "        Computationally efficient and can handle high-dimensional data.\n",
    "        Model-agnostic, making them suitable for various algorithms.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Breast Cancer Wisconsin dataset. We'll use the chi-squared (χ²) statistical test as a filter method to select the most relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Breast Cancer Wisconsin dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SelectKBest with chi-squared test for feature selection\n",
    "k_best_selector = SelectKBest(score_func=chi2, k=10)\n",
    "X_train_selected = k_best_selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = k_best_selector.transform(X_test)\n",
    "\n",
    "# Create a Random Forest classifier with 100 trees\n",
    "random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the selected features\n",
    "random_forest_classifier.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = random_forest_classifier.predict(X_test_selected)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the chi-squared test through the SelectKBest method to select the top 10 features from the Breast Cancer Wisconsin dataset. The selected features are then used to train a Random Forest classifier, and the model's performance is evaluated on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2. Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper methods are a category of feature selection techniques that evaluate subsets of features by training and testing a machine learning model on different combinations. Unlike filter methods, wrapper methods consider the interaction between features and assess subsets based on the model's performance. These methods typically use a search algorithm to explore the feature space and select the optimal subset.\n",
    "\n",
    "#### Key Points:\n",
    "\n",
    "- Model-Dependent:\n",
    "    Wrapper methods are model-dependent, as they involve training and testing a specific machine learning model on different feature subsets.\n",
    "\n",
    "- Search Strategy:\n",
    "    The search strategy can be exhaustive (evaluating all possible subsets) or heuristic (using algorithms like forward selection or backward elimination).\n",
    "\n",
    "- Performance Evaluation:\n",
    "    Model performance serves as the criterion for selecting feature subsets. Common metrics include accuracy, F1-score, or other relevant performance measures.\n",
    "\n",
    "- Computational Intensity:\n",
    "    Wrapper methods can be computationally intensive, especially when evaluating a large number of feature subsets.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Breast Cancer Wisconsin dataset. We'll use a simple wrapper method, Recursive Feature Elimination (RFE), with a Support Vector Machine (SVM) classifier to select the optimal subset of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Breast Cancer Wisconsin dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Support Vector Machine (SVM) classifier\n",
    "svm_classifier = SVC(kernel=\"linear\", random_state=42)\n",
    "\n",
    "# Apply Recursive Feature Elimination (RFE) for feature selection\n",
    "rfe_selector = RFE(estimator=svm_classifier, n_features_to_select=10)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = rfe_selector.transform(X_test)\n",
    "\n",
    "# Train the classifier on the selected features\n",
    "svm_classifier.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = svm_classifier.predict(X_test_selected)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use Recursive Feature Elimination (RFE) with a Support Vector Machine (SVM) classifier to select the top 10 features from the Breast Cancer Wisconsin dataset. The selected features are then used to train the SVM classifier, and the model's performance is evaluated on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.3. Embedded methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded methods combine feature selection with the model training process. These methods incorporate feature selection as an integral part of the model training, aiming to identify the most relevant features during the learning process. Embedded methods are model-dependent and often leverage regularization techniques to penalize or eliminate irrelevant features.\n",
    "\n",
    "#### Key Points:\n",
    "\n",
    "- Model-Dependent:\n",
    "    Embedded methods are closely tied to specific machine learning models and utilize their built-in feature selection capabilities.\n",
    "\n",
    "- Regularization:\n",
    "    Regularization terms are introduced during model training to penalize the inclusion of unnecessary features, encouraging the model to focus on the most informative ones.\n",
    "\n",
    "- Joint Optimization:\n",
    "    The selection of features and model parameters is jointly optimized during the training process.\n",
    "\n",
    "- Computational Efficiency:\n",
    "    Embedded methods are generally more computationally efficient than wrapper methods, as they do not require external model evaluations for each feature subset.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Breast Cancer Wisconsin dataset. We'll use the LASSO (Least Absolute Shrinkage and Selection Operator) regularization technique with a linear regression model as an embedded method for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Breast Cancer Wisconsin dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LASSO regression model\n",
    "lasso_model = Lasso(alpha=0.01, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance from the LASSO model\n",
    "feature_importance = lasso_model.coef_\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importance)), feature_importance, tick_label=cancer.feature_names)\n",
    "plt.title(\"LASSO Feature Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Coefficient Magnitude\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use LASSO regularization with a linear regression model to select important features from the Breast Cancer Wisconsin dataset. The coefficients obtained from the trained LASSO model indicate the importance of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1. Concepts and mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Belief Networks (BBNs):\n",
    "\n",
    "Bayesian Belief Networks, also known as Bayesian Networks or Bayesian Graphical Models, are probabilistic graphical models that represent the probabilistic relationships among a set of variables. These networks are based on Bayesian probability theory and utilize a directed acyclic graph (DAG) to illustrate the conditional dependencies between variables. BBNs consist of two main components: nodes and edges.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "- Nodes:\n",
    "    Nodes represent variables in the system and can be either observed (evidence) or unobserved (hidden).\n",
    "\n",
    "- Edges:\n",
    "    Edges connect nodes and represent the probabilistic dependencies between variables.\n",
    "\n",
    "- Conditional Probability Tables (CPTs):\n",
    "    Each node has a conditional probability table that specifies the probability distribution of that node given its parents in the graph.\n",
    "\n",
    "- D-separation:\n",
    "    D-separation rules determine the independence relationships between variables in the graph.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Pomegranate library in Python to create a Bayesian Belief Network for a diagnostic scenario. We'll model the relationship between symptoms and possible medical conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pomegranate import (DiscreteDistribution, ConditionalProbabilityTable, State, BayesianNetwork)\n",
    "\n",
    "# Define nodes representing symptoms and conditions\n",
    "fever = DiscreteDistribution({'True': 0.1, 'False': 0.9})\n",
    "cough = DiscreteDistribution({'True': 0.3, 'False': 0.7})\n",
    "headache = DiscreteDistribution({'True': 0.2, 'False': 0.8})\n",
    "flu = ConditionalProbabilityTable(\n",
    "    [['True', 'True', 'True', 0.95],\n",
    "     ['True', 'True', 'False', 0.05],\n",
    "     ['True', 'False', 'True', 0.9],\n",
    "     ['True', 'False', 'False', 0.1],\n",
    "     ['False', 'True', 'True', 0.3],\n",
    "     ['False', 'True', 'False', 0.7],\n",
    "     ['False', 'False', 'True', 0.01],\n",
    "     ['False', 'False', 'False', 0.99]], [fever, cough, headache])\n",
    "\n",
    "# Create states for each variable\n",
    "s1 = State(fever, name=\"fever\")\n",
    "s2 = State(cough, name=\"cough\")\n",
    "s3 = State(headache, name=\"headache\")\n",
    "s4 = State(flu, name=\"flu\")\n",
    "\n",
    "# Create a Bayesian Network and add states\n",
    "network = BayesianNetwork(\"Medical Diagnosis\")\n",
    "network.add_states(s1, s2, s3, s4)\n",
    "\n",
    "# Add edges defining the dependencies\n",
    "network.add_edge(s1, s4)\n",
    "network.add_edge(s2, s4)\n",
    "network.add_edge(s3, s4)\n",
    "\n",
    "# Finalize the network\n",
    "network.bake()\n",
    "\n",
    "# Predict the probability of having the flu given symptoms\n",
    "result = network.predict_proba([['True', 'False', 'True', None]])\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Pomegranate library to create a simple Bayesian Belief Network for medical diagnosis. The network models the relationships between symptoms (fever, cough, headache) and a medical condition (flu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2. Training Bayesian belief networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Bayesian Belief Networks involves estimating the parameters of the conditional probability tables (CPTs) based on observed data. In many cases, the structure of the Bayesian Network is assumed or predefined, and the focus is on learning the probabilities associated with the edges in the graph. Learning from data helps to improve the accuracy of the network's predictions.\n",
    "\n",
    "#### Training Steps:\n",
    "\n",
    "- Data Collection:\n",
    "    Gather a dataset containing observations of the variables in the Bayesian Network.\n",
    "\n",
    "- Parameter Estimation:\n",
    "    Use statistical methods to estimate the probabilities in the CPTs based on the observed data.\n",
    "\n",
    "- Model Adjustment:\n",
    "    Refine the Bayesian Network structure or adjust parameters to improve model performance.\n",
    "\n",
    "- Validation:\n",
    "    Evaluate the trained model on new data to ensure generalization.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Pomegranate library in Python to train a Bayesian Belief Network for a diagnostic scenario. We'll use a dataset of symptoms and flu cases to estimate the parameters of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for training\n",
    "data = [[True, False, True, True],\n",
    "        [False, True, True, True],\n",
    "        [True, True, False, True],\n",
    "        [False, False, True, False],\n",
    "        [True, True, True, True],\n",
    "        [False, True, False, False],\n",
    "        [True, False, False, False],\n",
    "        [False, False, False, False]]\n",
    "\n",
    "# Define nodes representing symptoms and conditions\n",
    "fever = DiscreteDistribution.from_samples(data[:, 0])\n",
    "cough = DiscreteDistribution.from_samples(data[:, 1])\n",
    "headache = DiscreteDistribution.from_samples(data[:, 2])\n",
    "flu = ConditionalProbabilityTable.from_samples(data[:, [0, 1, 2, 3]], [fever, cough, headache])\n",
    "\n",
    "# Create states for each variable\n",
    "s1 = State(fever, name=\"fever\")\n",
    "s2 = State(cough, name=\"cough\")\n",
    "s3 = State(headache, name=\"headache\")\n",
    "s4 = State(flu, name=\"flu\")\n",
    "\n",
    "# Create a Bayesian Network and add states\n",
    "network = BayesianNetwork(\"Medical Diagnosis\")\n",
    "network.add_states(s1, s2, s3, s4)\n",
    "\n",
    "# Add edges defining the dependencies\n",
    "network.add_edge(s1, s4)\n",
    "network.add_edge(s2, s4)\n",
    "network.add_edge(s3, s4)\n",
    "\n",
    "# Finalize the network\n",
    "network.bake()\n",
    "\n",
    "# Display the original probabilities\n",
    "print(\"Original Probabilities:\")\n",
    "print(network.predict_proba([[True, False, True, None]]))\n",
    "\n",
    "# Train the network with the synthetic data\n",
    "network.fit(data)\n",
    "\n",
    "# Display the trained probabilities\n",
    "print(\"\\nTrained Probabilities:\")\n",
    "print(network.predict_proba([[True, False, True, None]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use synthetic data to train a Bayesian Belief Network for medical diagnosis using the Pomegranate library. The network is initially created with predefined probabilities, and then the fit method is used to update the probabilities based on the synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1. Linear support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Support Vector Machines (SVMs):\n",
    "\n",
    "Linear Support Vector Machines are a class of supervised machine learning models used for classification and regression tasks. SVMs operate by finding the hyperplane that best separates the data points of different classes while maximizing the margin between them. In the case of linear SVMs, the decision boundary is a linear hyperplane.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "- Hyperplane:\n",
    "        The decision boundary that separates data points of different classes. In a linear SVM, this is a straight line in two dimensions or a plane in higher dimensions.\n",
    "\n",
    "- Margin:\n",
    "        The distance between the hyperplane and the nearest data point of each class. SVM aims to maximize this margin.\n",
    "\n",
    "- Support Vectors:\n",
    "        The data points that lie closest to the hyperplane and influence its position. These points are crucial for determining the optimal decision boundary.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the famous Iris dataset to demonstrate the application of a Linear Support Vector Machine for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Consider only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Linear Support Vector Machine classifier\n",
    "svm_classifier = SVC(kernel=\"linear\", C=1.0, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k', marker='o')\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50),\n",
    "                     np.linspace(ylim[0], ylim[1], 50))\n",
    "Z = svm_classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "ax.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "ax.scatter(svm_classifier.support_vectors_[:, 0], svm_classifier.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none', edgecolors='k')\n",
    "plt.title(\"Linear SVM Decision Boundary\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, a Linear Support Vector Machine is applied to the Iris dataset, considering only the first two features for visualization purposes. The model is trained on the training data, and its decision boundary is plotted along with the support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2. Nonlinear support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonlinear Support Vector Machines:\n",
    "\n",
    "While linear Support Vector Machines (SVMs) are effective for linearly separable data, nonlinear SVMs extend the model's capability to handle complex relationships in the data by employing kernel functions. Kernel functions transform the original feature space into a higher-dimensional space, making it possible to find nonlinear decision boundaries.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "- Kernel Functions:\n",
    "    Mathematical functions that compute the dot product between data points in a higher-dimensional space without explicitly calculating the transformation.\n",
    "\n",
    "- Radial Basis Function (RBF) Kernel:\n",
    "    Commonly used kernel for nonlinear SVMs, allowing the model to capture complex patterns in the data.\n",
    "\n",
    "- Gamma Parameter:\n",
    "    A parameter in the RBF kernel that influences the shape of the decision boundary. Higher gamma values result in a more complex boundary.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Iris dataset to demonstrate the application of a Nonlinear Support Vector Machine with an RBF kernel for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Consider only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Nonlinear Support Vector Machine classifier with RBF kernel\n",
    "svm_classifier = SVC(kernel=\"rbf\", C=1.0, gamma=0.1, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k', marker='o')\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50),\n",
    "                     np.linspace(ylim[0], ylim[1], 50))\n",
    "Z = svm_classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "ax.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "ax.scatter(svm_classifier.support_vectors_[:, 0], svm_classifier.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none', edgecolors='k')\n",
    "plt.title(\"Nonlinear SVM Decision Boundary (RBF Kernel)\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, a Nonlinear Support Vector Machine with an RBF kernel is applied to the Iris dataset, considering only the first two features for visualization. The model is trained on the training data, and its decision boundary is plotted along with the support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1. Using IF-THEN rules for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IF-THEN Rules for Classification:\n",
    "\n",
    "Rule-based classification involves the creation of a set of rules that determine the class or category of an instance based on its feature values. Each rule typically takes the form \"IF condition THEN class.\" These rules are human-readable and provide transparency into the decision-making process. Rule-based systems are often employed in scenarios where interpretability and explainability are crucial.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "- IF-THEN Structure:\n",
    "    Each rule specifies a condition based on feature values, and the corresponding action (classification) to take if the condition is met.\n",
    "\n",
    "- Rule Order:\n",
    "    Rules are usually evaluated sequentially, and the first rule that matches the conditions is applied.\n",
    "\n",
    "- Interpretability:\n",
    "    Rule-based systems are transparent, making them easy to interpret and explain.\n",
    "\n",
    "- Rule Learning:\n",
    "    Rule-based models can be manually crafted or learned from data through techniques like decision tree induction.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the famous Iris dataset to demonstrate the application of rule-based classification. We'll use the \"fuzzy\" library in Python to define IF-THEN rules based on fuzzy logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzy import FuzzySystem, Rule, Antecedent, Consequent\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define fuzzy antecedents\n",
    "sepal_length = Antecedent(\"Sepal Length\", X_train[:, 0])\n",
    "sepal_width = Antecedent(\"Sepal Width\", X_train[:, 1])\n",
    "petal_length = Antecedent(\"Petal Length\", X_train[:, 2])\n",
    "petal_width = Antecedent(\"Petal Width\", X_train[:, 3])\n",
    "\n",
    "# Define fuzzy consequents\n",
    "setosa = Consequent(\"Setosa\", y_train == 0)\n",
    "versicolor = Consequent(\"Versicolor\", y_train == 1)\n",
    "virginica = Consequent(\"Virginica\", y_train == 2)\n",
    "\n",
    "# Define IF-THEN rules based on fuzzy logic\n",
    "rules = [\n",
    "    Rule(sepal_length[\"low\"] | sepal_width[\"medium\"], setosa),\n",
    "    Rule(petal_length[\"medium\"] & petal_width[\"high\"], versicolor),\n",
    "    Rule(sepal_length[\"high\"] & petal_length[\"medium\"], virginica),\n",
    "]\n",
    "\n",
    "# Create the fuzzy system\n",
    "fuzzy_system = FuzzySystem(rules)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = fuzzy_system.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the \"fuzzy\" library to define IF-THEN rules based on fuzzy logic for classifying Iris flowers. Fuzzy logic allows us to express rules in a more flexible manner than traditional crisp logic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2. Rule extraction from a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule extraction involves transforming the decision rules embedded in a decision tree model into a set of explicit IF-THEN rules. Decision trees inherently provide a set of rules used for classification, but extracting these rules can enhance interpretability and facilitate the manual creation or modification of rules.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "- Decision Tree Rules:\n",
    "    Decision trees make decisions based on a set of rules inferred from the features of the data.\n",
    "\n",
    "- Leaf Nodes:\n",
    "    Each leaf node in a decision tree corresponds to a specific class or outcome.\n",
    "\n",
    "- Rule Extraction Process:\n",
    "    The process involves traversing the decision tree and extracting the conditions present on the path from the root to each leaf.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Iris dataset to demonstrate the extraction of rules from a decision tree. We'll use the \"sklearn.tree\" module in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Extract rules from the decision tree\n",
    "tree_rules = export_text(decision_tree_classifier, feature_names=iris.feature_names)\n",
    "\n",
    "# Display the extracted rules\n",
    "print(\"Extracted Decision Tree Rules:\")\n",
    "print(tree_rules)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = decision_tree_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Iris dataset to train a decision tree classifier and extract rules from the trained model using the export_text function from the \"sklearn.tree\" module. The extracted rules provide a clear and human-readable representation of the decision tree's logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3. Rule induction using a sequential covering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule Induction Using a Sequential Covering Algorithm:\n",
    "\n",
    "Rule induction involves the automatic generation of rules from a dataset without relying on a predefined model structure. Sequential covering algorithms are a class of rule induction techniques that iteratively discover rules by selecting instances and covering them with rules. This process continues until all instances are covered or a stopping criterion is met.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "- Sequential Covering:\n",
    "        The algorithm iteratively selects instances not covered by existing rules and generates rules specifically for those instances.\n",
    "\n",
    "- Rule Quality Measures:\n",
    "        The algorithm typically uses quality measures to assess the usefulness of a rule, such as support, confidence, or information gain.\n",
    "\n",
    "- Iterative Process:\n",
    "        The process continues until a predefined stopping criterion is satisfied, such as covering all instances or reaching a certain rule complexity.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the \"RuleFit\" library in Python to perform rule induction using a sequential covering algorithm. RuleFit is a hybrid model that combines decision trees with linear models to create interpretable rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a RuleFit classifier\n",
    "rulefit_classifier = RuleFit()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rulefit_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = rulefit_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "\n",
    "# Display the extracted rules\n",
    "print(\"\\nExtracted Rules:\")\n",
    "for rule in rulefit_classifier.get_rules():\n",
    "    print(rule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the \"RuleFit\" library to perform rule induction using a sequential covering algorithm on the Iris dataset. The RuleFit class is used to train a model that combines decision trees and linear models, providing interpretable rules. The extracted rules can be displayed and analyzed for better understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.4. Associative classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associative classification, also known as rule-based classification or classification by association, combines the principles of association rule mining with traditional classification. Instead of generating rules separately, associative classification simultaneously discovers association rules and utilizes them for classification purposes. This approach often leverages techniques like Apriori algorithm for mining frequent itemsets and a classifier for generating rules based on these itemsets.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "-  Association Rule Mining:\n",
    "    Identifying frequent patterns or associations among variables in the dataset.\n",
    "\n",
    "- Rule Generation:\n",
    "    Deriving classification rules from frequent itemsets discovered during association rule mining.\n",
    "\n",
    "- Hybrid Approach:\n",
    "    Integrating the strengths of both association rule mining and classification to enhance the predictive performance of the model.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the \"pyARC\" library in Python for associative classification. The pyARC library provides functionalities for mining and using classification rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pyARC library if not installed\n",
    "# !pip install pyarc\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyarc import CBA\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a CBA (Classification Based on Associations) classifier\n",
    "cba_classifier = CBA(support=0.2, confidence=0.7)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "cba_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = cba_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "\n",
    "# Display the extracted rules\n",
    "print(\"\\nExtracted Rules:\")\n",
    "for rule in cba_classifier.clf.rules:\n",
    "    print(rule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the \"pyARC\" library to perform associative classification on the Iris dataset. The CBA class is used to create a classifier based on associations, and it is trained on the dataset. The extracted rules can be displayed and analyzed for better understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.5. Discriminative frequent pattern–based classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminative frequent pattern–based classification is an approach that focuses on finding frequent patterns that are highly correlated with specific classes in the dataset. Instead of generating rules based on associations in the entire dataset, this method aims to discover patterns that discriminate between different classes effectively.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "- Frequent Pattern Mining:\n",
    "        Identifying patterns that occur frequently in the dataset.\n",
    "\n",
    "- Discriminative Patterns:\n",
    "        Patterns that exhibit significant differences in occurrence between different classes.\n",
    "\n",
    "- Classification based on Discriminative Patterns:\n",
    "        Using the discovered discriminative patterns to build a classifier that can effectively differentiate between classes.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the \"pyFIM\" library in Python for discriminative frequent pattern–based classification. The pyFIM library provides functionalities for frequent itemset mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pyfim library if not installed\n",
    "# !pip install pyfim\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyfim import eclat\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the dataset into a transaction format (list of sets)\n",
    "transactions = [set(map(str, x)) for x in X_train]\n",
    "\n",
    "# Perform discriminative frequent pattern mining using Eclat algorithm\n",
    "patterns = eclat(transactions, target=\"c\", supp=0.2, zmin=2)\n",
    "\n",
    "# Display the discovered discriminative patterns\n",
    "print(\"Discovered Discriminative Patterns:\")\n",
    "for pattern in patterns:\n",
    "    print(pattern)\n",
    "\n",
    "# Create a simple classifier based on the discovered patterns\n",
    "def classify(transaction):\n",
    "    for pattern in patterns:\n",
    "        if pattern.issubset(transaction):\n",
    "            return pattern[-1]  # Class label associated with the pattern\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = [classify(set(map(str, x))) for x in X_test]\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the \"pyFIM\" library to perform discriminative frequent pattern–based classification on the Iris dataset. The eclat function is used to mine discriminative frequent patterns, and a simple classifier is created based on the discovered patterns. The accuracy and classification report are then displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.1. Semisupervised classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semi-supervised classification is a type of weakly supervised learning where the training dataset contains both labeled and unlabeled instances. Traditional supervised learning relies on labeled data for training, while unsupervised learning deals with unlabeled data. Semi-supervised learning aims to leverage the benefits of both by using a small amount of labeled data along with a larger amount of unlabeled data to build a more robust model.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "- Labeled and Unlabeled Data:\n",
    "        The training dataset includes instances with known labels (labeled) and instances without labels (unlabeled).\n",
    "\n",
    "- Leveraging Unlabeled Data:\n",
    "        Unlabeled data is used to improve the generalization and performance of the classifier.\n",
    "\n",
    "- Common Techniques:\n",
    "        Self-training, co-training, and multi-view learning are common semi-supervised learning techniques.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the \"scikit-learn\" library in Python for semi-supervised classification. We'll use a simple dataset and a semi-supervised algorithm known as Label Propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Introduce semi-supervision by randomly selecting a portion of labels to be -1 (unlabeled)\n",
    "import numpy as np\n",
    "rng = np.random.RandomState(42)\n",
    "y_semi_supervised = y.copy()\n",
    "y_semi_supervised[rng.rand(len(y)) < 0.5] = -1  # Label -1 indicates unlabeled\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_semi_supervised, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Label Propagation classifier\n",
    "label_propagation_classifier = LabelPropagation(kernel=\"knn\", n_neighbors=10)\n",
    "\n",
    "# Train the classifier on the training data (including unlabeled instances)\n",
    "label_propagation_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = label_propagation_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test[y_test != -1], predictions[y_test != -1])\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test[y_test != -1], predictions[y_test != -1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Iris dataset and introduce semi-supervision by randomly assigning a portion of labels to be -1 (unlabeled). The Label Propagation algorithm is then used to perform semi-supervised classification. The accuracy and classification report are displayed, showcasing the potential of leveraging both labeled and unlabeled data for improved classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.2. Active learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Active learning is a semi-supervised learning approach where the algorithm interacts with an \"oracle\" or a human annotator to intelligently query for labels on instances it finds most informative. Instead of passively receiving labeled instances, the algorithm actively selects which instances to query for labels, aiming to maximize learning efficiency with a minimal number of labeled examples.\n",
    "\n",
    "Key Concepts:\n",
    "\n",
    "    Query Strategy:\n",
    "        The algorithm employs a query strategy to select instances that are expected to provide the most information about the underlying model.\n",
    "\n",
    "    Model Uncertainty:\n",
    "        Instances with uncertain predictions or those near the decision boundary are often prioritized for labeling.\n",
    "\n",
    "    Reducing Annotation Costs:\n",
    "        Active learning aims to reduce the need for large labeled datasets by focusing on informative instances, making it particularly useful when obtaining labeled data is expensive or time-consuming.\n",
    "\n",
    "Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the \"modAL\" library in Python for active learning. We'll use a simple synthetic dataset and a basic classifier to demonstrate the active learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from modAL.models import ActiveLearner\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and pool sets\n",
    "X_train, X_pool, y_train, y_pool = train_test_split(X, y, test_size=0.95, random_state=42)\n",
    "\n",
    "# Create a random forest classifier (base learner)\n",
    "learner = ActiveLearner(estimator=RandomForestClassifier(), X_training=X_train, y_training=y_train)\n",
    "\n",
    "# Define the query strategy (uncertainty sampling)\n",
    "def query_strategy(classifier, X_pool):\n",
    "    uncertainty = classifier.predict_proba(X_pool)[:, 0]  # Example: uncertainty as probability of class 0\n",
    "    return uncertainty.argsort()[-1:]  # Query the instance with the highest uncertainty\n",
    "\n",
    "# Active learning loop\n",
    "n_queries = 50\n",
    "for _ in range(n_queries):\n",
    "    query_idx, query_instance = learner.query(X_pool, n_instances=1, query_strategy=query_strategy)\n",
    "    learner.teach(X_pool[query_idx], y_pool[query_idx])\n",
    "    X_pool, y_pool = np.delete(X_pool, query_idx, axis=0), np.delete(y_pool, query_idx)\n",
    "\n",
    "# Make predictions on the entire dataset\n",
    "predictions = learner.predict(X)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the \"modAL\" library to demonstrate active learning with a random forest classifier. The query strategy is based on uncertainty sampling, where the algorithm selects instances with the highest uncertainty for labeling. The active learning loop iteratively queries the oracle, updates the model, and repeats the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.3. Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning is a powerful technique where knowledge gained from training a model on one task is applied to improve performance on a different but related task. This is especially useful when labeled data is scarce for the target task. In the context of weak supervision, transfer learning can help leverage information from a source domain with abundant labeled data to boost the performance of a model in a target domain with limited labeled data.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example using transfer learning for text classification. We'll use the transformers library in Python, which provides pre-trained language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Example data (replace with your dataset)\n",
    "source_texts = [\"Positive review 1\", \"Positive review 2\", \"Negative review 1\", \"Negative review 2\"]\n",
    "target_texts = [\"New positive review\", \"New negative review\"]\n",
    "\n",
    "# Tokenize and prepare input tensors for source and target domains\n",
    "source_inputs = tokenizer(source_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "target_inputs = tokenizer(target_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Fine-tune the model on the source domain\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "source_labels = torch.tensor([1, 1, 0, 0])  # Binary labels for the source domain\n",
    "source_outputs = model(**source_inputs, labels=source_labels)\n",
    "loss = source_outputs.loss\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# Use the fine-tuned model for the target domain\n",
    "target_outputs = model(**target_inputs)\n",
    "target_predictions = torch.argmax(target_outputs.logits, dim=1)\n",
    "\n",
    "print(\"Predictions for target domain:\", target_predictions.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use a pre-trained BERT model on a source domain with labeled data (positive and negative reviews). We then fine-tune the model on this source domain. Finally, we apply the fine-tuned model to make predictions on a target domain with limited labeled data (new reviews). Transfer learning helps the model leverage knowledge from the source domain to improve classification performance on the target domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.4. Distant supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distant supervision is a technique that leverages auxiliary, potentially noisy, or imperfect sources of supervision to train models. This approach is particularly useful when direct labeling of instances is challenging, but there exist distant or indirect sources of information related to the task. By associating labels from these distant sources with instances in the dataset, models can learn effectively despite limited direct supervision.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example using distant supervision for sentiment analysis. We'll use a dataset of tweets that are labeled with sentiment, and we'll also leverage emoticons as a distant supervision signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Example dataset (replace with your dataset)\n",
    "data = {'text': [\"I love this product 😍\", \"Not happy with the service 😞\", \"Amazing experience! 😊\", \"Disappointed 😔\"],\n",
    "        'sentiment': ['positive', 'negative', 'positive', 'negative']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Distant supervision: Use emoticons as additional labels\n",
    "df['emoticon_label'] = df['text'].apply(lambda x: 'positive' if '😍' in x else 'negative')\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature engineering: Convert text to features using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a model using distant supervision labels\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vec, df.loc[X_train.index, 'emoticon_label'])\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = classifier.predict(X_test_vec)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use emoticons as a distant supervision signal to improve sentiment analysis. The model is trained on the labeled dataset, and additional labels from emoticons are incorporated during training. The resulting model is then evaluated on a test set, showcasing the integration of distant supervision in a practical context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.5. Zero-shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot learning is an innovative approach that enables models to generalize to classes or tasks for which they have never seen explicit examples during training. Instead of relying on labeled instances for all classes, zero-shot learning leverages auxiliary information or attributes to make predictions in unseen scenarios. This is particularly beneficial in situations where obtaining labeled data for every possible class is impractical.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of zero-shot learning using a pre-trained language model for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Example text data (replace with your dataset)\n",
    "texts = [\n",
    "    \"A delicious recipe for homemade pizza.\",\n",
    "    \"The latest advancements in artificial intelligence.\",\n",
    "    \"Exploring the wonders of outer space.\",\n",
    "]\n",
    "\n",
    "# Zero-shot classification using a pre-trained language model\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "# Example class names or labels\n",
    "class_names = [\"cooking\", \"technology\", \"science\"]\n",
    "\n",
    "# Perform zero-shot classification\n",
    "results = classifier(texts, class_names)\n",
    "\n",
    "# Display the results\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"Predicted Class:\", results[i]['labels'][0])\n",
    "    print(\"Confidence Score:\", results[i]['scores'][0])\n",
    "    print(\"----------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the transformers library to access a zero-shot classification pipeline. The model is not explicitly trained on labeled examples for the specified classes (cooking, technology, science). Instead, it leverages its understanding of language and context to make predictions on these unseen classes. This showcases the power of zero-shot learning in scenarios where traditional supervised training is impractical due to the absence of labeled data for all possible classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.1. Stream data classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stream data classification involves the real-time analysis and classification of data as it is generated. This is common in applications where data arrives continuously and decisions need to be made instantaneously. Techniques for stream data classification often require adaptive models that can evolve over time as new data arrives.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of stream data classification using the scikit-multiflow library in Python. We'll use a synthetic dataset for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultiflow.data import SEAGenerator\n",
    "from skmultiflow.lazy import KNNClassifier\n",
    "from skmultiflow.evaluation import EvaluatePrequential\n",
    "\n",
    "# Create a stream data generator\n",
    "stream = SEAGenerator(random_state=42)\n",
    "\n",
    "# Define the classifier (K-Nearest Neighbors)\n",
    "classifier = KNNClassifier(n_neighbors=3)\n",
    "\n",
    "# Evaluate the classifier on the stream data\n",
    "evaluator = EvaluatePrequential(show_plot=True, pretrain_size=1000, max_samples=5000)\n",
    "evaluator.evaluate(stream=stream, model=classifier, model_names=['KNN'])\n",
    "\n",
    "# Note: The pretrain_size and max_samples are set for illustration purposes and can be adjusted based on your specific scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the scikit-multiflow library to simulate a stream data scenario with the SEA dataset. We employ a K-Nearest Neighbors (KNN) classifier, which is suitable for online learning scenarios. The EvaluatePrequential class helps evaluate the classifier's performance over time, making it suitable for stream data classification tasks. Adjust the parameters based on your specific use case and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.2. Sequence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence classification involves the categorization of data instances that are presented in a sequential manner. This is common in various domains such as natural language processing, time-series analysis, and bioinformatics. Techniques for sequence classification often involve models that can capture dependencies and patterns over time.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of sequence classification using a recurrent neural network (RNN) for sentiment analysis on a dataset of movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Example dataset (replace with your dataset)\n",
    "data = {'text': [\"I loved the movie!\", \"It was a terrible experience.\", \"Amazing plot twists.\", \"Boring and predictable.\"],\n",
    "        'sentiment': ['positive', 'negative', 'positive', 'negative']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Build an RNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=64, input_length=X_train.shape[1]),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use an LSTM-based recurrent neural network for sequence classification. The model is trained on a dataset of movie reviews with corresponding sentiments. The sequences of words in each review are tokenized and padded for input to the model. The LSTM layer enables the model to capture sequential dependencies in the data, making it suitable for sequence classification tasks like sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.3. Graph data classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph data classification involves predicting labels or categories associated with nodes or entire graphs. This is prevalent in various domains, including social network analysis, bioinformatics, and recommendation systems. Techniques for graph data classification leverage the inherent structure and connectivity in graphs to make predictions.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of graph data classification using the stellargraph library in Python. We'll use a dataset for node classification on a citation network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stellargraph as sg\n",
    "from stellargraph.data import BiasedRandomWalk\n",
    "from stellargraph import StellarGraph\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Example dataset (replace with your dataset)\n",
    "# Assuming you have an edge list file 'edges.csv' with columns 'source' and 'target', and a node features file 'features.csv'\n",
    "edges = pd.read_csv('edges.csv')\n",
    "features = pd.read_csv('features.csv')\n",
    "\n",
    "# Create a StellarGraph from the edge list and node features\n",
    "G = StellarGraph(edges=edges, node_features=features)\n",
    "\n",
    "# Extract node labels for node classification\n",
    "node_labels = G.nodes().astype(int) % 2  # Binary node labels for illustration purposes\n",
    "\n",
    "# Split the dataset\n",
    "train_nodes, test_nodes, y_train, y_test = train_test_split(node_labels.index, node_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform biased random walks to generate node sequences for training\n",
    "rw = BiasedRandomWalk(G)\n",
    "walks = rw.run(nodes=list(train_nodes), length=80, n=10, p=0.5, q=2.0)\n",
    "\n",
    "# Use Skip-gram model to learn node embeddings from the walks\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(walks, vector_size=128, window=5, min_count=0, sg=1, workers=2, epochs=1)\n",
    "\n",
    "# Transform node embeddings into a Pandas DataFrame\n",
    "node_embeddings = pd.DataFrame({node: model.wv[node] for node in model.wv.index_to_key})\n",
    "\n",
    "# Train a classifier on the node embeddings\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(node_embeddings.loc[train_nodes], y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = clf.predict(node_embeddings.loc[test_nodes])\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the stellargraph library to create a graph from an edge list and node features. We perform biased random walks on the graph and use a Skip-gram model to learn node embeddings. These embeddings are then used to train a classifier, in this case, a Random Forest classifier, for node classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.1. Multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass classification extends the binary classification scenario to handle more than two classes. In this setup, the goal is to assign each instance to one of multiple predefined classes. Several algorithms and strategies exist to address multiclass classification challenges.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of multiclass classification using the popular Iris dataset with the Support Vector Machine (SVM) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Support Vector Machine (SVM) classifier\n",
    "svm_classifier = SVC(kernel='linear', C=1)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Iris dataset, a classic multiclass classification problem with three classes (setosa, versicolor, and virginica). We employ a Support Vector Machine (SVM) classifier to learn the patterns in the data and predict the class labels. The accuracy and classification report provide insights into the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.2. Distance metric learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance metric learning aims to optimize the metric used to measure the similarity or dissimilarity between data points. By learning a suitable distance metric, it becomes possible to improve the effectiveness of algorithms that rely on distances, such as clustering or nearest neighbors.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of distance metric learning using the metric_learn library in Python. We'll use the well-known Iris dataset and the Large Margin Nearest Neighbor (LMNN) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from metric_learn import LMNN\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Large Margin Nearest Neighbor (LMNN) for distance metric learning\n",
    "lmnn = LMNN(k=3, learn_rate=1e-6)\n",
    "lmnn.fit(X_train, y_train)\n",
    "\n",
    "# Transform the data using the learned metric\n",
    "X_train_transformed = lmnn.transform(X_train)\n",
    "X_test_transformed = lmnn.transform(X_test)\n",
    "\n",
    "# Train a k-Nearest Neighbors classifier on the transformed data\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_classifier.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = knn_classifier.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the metric_learn library to apply the Large Margin Nearest Neighbor (LMNN) algorithm for distance metric learning on the Iris dataset. The learned metric is then used to transform the data, and a k-Nearest Neighbors classifier is trained on the transformed data. The accuracy metric is used to assess the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.3. Interpretability of classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretability refers to the ease with which humans can understand and trust the decisions made by a machine learning model. In classification, interpretable models and visualization techniques help in gaining insights into feature importance and decision-making processes.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of interpreting a classification model's decisions using the shap library in Python. We'll use a popular dataset, the Titanic dataset, and a simple model for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic_data = pd.read_csv('https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv')\n",
    "\n",
    "# Preprocess the data (replace with your preprocessing steps)\n",
    "titanic_data = titanic_data.dropna(subset=['Age'])\n",
    "titanic_data = titanic_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Survived']]\n",
    "titanic_data['Sex'] = titanic_data['Sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = titanic_data.drop('Survived', axis=1)\n",
    "y = titanic_data['Survived']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Use SHAP (SHapley Additive exPlanations) for interpretability\n",
    "explainer = shap.TreeExplainer(rf_classifier)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Visualize the feature importance using a summary plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the shap library to interpret the decisions of a Random Forest classifier on the Titanic dataset. The shap library provides Shapley values, which can be used to explain the impact of each feature on the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.4. Genetic algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genetic algorithms are optimization algorithms inspired by the process of natural selection. They iteratively evolve a population of candidate solutions by applying genetic operators such as selection, crossover, and mutation. In the context of feature selection, genetic algorithms can be used to search for an optimal subset of features that maximizes or minimizes a given objective function.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of feature selection using a genetic algorithm with the genetic library in Python. We'll use the popular Iris dataset and a simple classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from genetic import evolve\n",
    "\n",
    "# Load the Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the objective function for feature selection\n",
    "def objective_function(features):\n",
    "    # Train a Random Forest classifier with the selected features\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train[:, features], y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    predictions = clf.predict(X_test[:, features])\n",
    "    \n",
    "    # Evaluate the model and return the accuracy\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy\n",
    "\n",
    "# Define the search space (features to be selected)\n",
    "search_space = list(range(X.shape[1]))\n",
    "\n",
    "# Use a genetic algorithm to find the optimal feature subset\n",
    "best_features = evolve(\n",
    "    objective_function,\n",
    "    search_space,\n",
    "    population_size=10,\n",
    "    generations=5,\n",
    "    crossover_probability=0.8,\n",
    "    mutation_probability=0.2\n",
    ")\n",
    "\n",
    "print(\"Best feature indices:\", best_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the genetic library to perform feature selection with a genetic algorithm on the Iris dataset. The objective_function represents the accuracy of a Random Forest classifier trained with a specific subset of features. The genetic algorithm evolves populations of feature subsets over generations, aiming to find the subset that maximizes the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.5. Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning involves training an agent to make sequential decisions in an environment to maximize a cumulative reward signal. In the context of feature selection, reinforcement learning can be employed to dynamically decide which features to include or exclude during the learning process.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of feature selection using a reinforcement learning approach with the Stable-Baselines3 library in Python. We'll use a simple classification task with the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.envs import DummyVecEnv\n",
    "\n",
    "# Load the Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a simple environment for feature selection\n",
    "class FeatureSelectionEnv:\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.action_space = len(X_train[0])\n",
    "        self.observation_space = len(X_train[0])\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.selected_features = np.zeros(len(self.X_train[0]))\n",
    "        self.current_step = 0\n",
    "        return self.selected_features\n",
    "\n",
    "    def step(self, action):\n",
    "        self.selected_features[action] = 1\n",
    "        self.current_step += 1\n",
    "        done = self.current_step == len(self.X_train[0])\n",
    "        accuracy = self.evaluate_model()\n",
    "        reward = accuracy if done else 0\n",
    "        return self.selected_features, reward, done, {}\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        selected_indices = np.where(self.selected_features == 1)[0]\n",
    "        if len(selected_indices) == 0:\n",
    "            return 0\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        clf.fit(self.X_train[:, selected_indices], self.y_train)\n",
    "        predictions = clf.predict(X_test[:, selected_indices])\n",
    "        return accuracy_score(y_test, predictions)\n",
    "\n",
    "# Create the environment\n",
    "env = DummyVecEnv([lambda: FeatureSelectionEnv(X_train, y_train)])\n",
    "\n",
    "# Train a Proximal Policy Optimization (PPO) agent for feature selection\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Extract the selected features from the trained agent\n",
    "selected_features = np.array(model.policy.action_proba(observation=np.ones(len(X_train[0])), actions=None)[0]) > 0.5\n",
    "\n",
    "print(\"Selected features indices:\", np.where(selected_features == 1)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Stable-Baselines3 library to train a Proximal Policy Optimization (PPO) agent for feature selection on the Iris dataset. The environment is a simple feature selection environment where the agent can choose which features to include or exclude. The agent is trained to maximize the accuracy of a Random Forest classifier on the selected features."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
