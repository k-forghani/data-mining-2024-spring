{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.2. Backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation, short for \"backward propagation of errors,\" is a key algorithm in training artificial neural networks. It is a supervised learning algorithm that adjusts the weights of the network's connections to minimize the difference between the predicted output and the actual output. The backpropagation algorithm consists of two main steps:\n",
    "\n",
    "- Forward Pass:\n",
    "        Input data is fed through the neural network, and the network calculates the predicted output. Each layer's activation is computed using the weighted sum of inputs and passed through an activation function.\n",
    "\n",
    "- Backward Pass (Backpropagation):\n",
    "        The error is calculated by comparing the predicted output with the actual output using a loss function. The algorithm then works backward through the network, adjusting the weights to minimize the error. This adjustment is done using the gradient of the loss function with respect to the weights, calculated through the chain rule of calculus.\n",
    "\n",
    "- Optimization:\n",
    "        An optimization algorithm (e.g., gradient descent) is often employed to iteratively update the weights in the direction that minimizes the error.\n",
    "\n",
    "Backpropagation allows neural networks to learn from examples and adjust their parameters to improve performance on a specific task. It is a fundamental concept in training deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic data for classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a simple neural network using scikit-learn's MLPClassifier\n",
    "# Note: This is a basic example; in practice, deep learning libraries like TensorFlow or PyTorch are commonly used for more complex models.\n",
    "model = MLPClassifier(hidden_layer_sizes=(5,), max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the model using backpropagation\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use scikit-learn to create a simple neural network classifier (MLPClassifier). The backpropagation algorithm is automatically applied during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.1. Responsive activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions play a crucial role in deep learning models by introducing non-linearity to the network. Responsive activation functions are designed to enhance the training process by addressing issues like vanishing gradients and enabling the model to learn more effectively. Some popular responsive activation functions include:\n",
    "\n",
    "- ReLU (Rectified Linear Unit):\n",
    "        ReLU is widely used due to its simplicity and effectiveness. It replaces all negative values in the input with zero, allowing the model to learn complex patterns.\n",
    "\n",
    "- Leaky ReLU:\n",
    "        Leaky ReLU is a variant of ReLU that allows a small, non-zero gradient for negative input values. This helps prevent dead neurons and facilitates training.\n",
    "\n",
    "- Parametric ReLU (PReLU):\n",
    "        PReLU introduces a learnable parameter that determines the slope of the negative part of the function. This allows the model to adapt the slope during training.\n",
    "\n",
    "- Exponential Linear Unit (ELU):\n",
    "        ELU smoothens the transition around zero by introducing a non-zero slope for negative values. It helps mitigate issues related to dead neurons and accelerates convergence.\n",
    "\n",
    "Responsive activation functions contribute to the stability and efficiency of training deep learning models, ultimately leading to improved performance.\n",
    "\n",
    "#### A real-world example of practical use in Python for a deep learning model using the Leaky ReLU activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Generate synthetic data for classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a simple deep learning model with Leaky ReLU activation\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, input_dim=20, activation='relu'),\n",
    "    layers.Dense(32, activation='leaky_relu'),  # Leaky ReLU activation\n",
    "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use TensorFlow and Keras to create a simple deep learning model with a Leaky ReLU activation function in one of its layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.2. Adaptive learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate is a hyperparameter that controls the size of the step taken during the optimization process in training a deep learning model. An adaptive learning rate adjusts itself during training based on the observed progress, aiming to overcome challenges such as slow convergence or oscillations in the loss function. Several adaptive learning rate algorithms exist, and one popular method is:\n",
    "\n",
    "- Adagrad (Adaptive Gradient Algorithm):\n",
    "        Adagrad adapts the learning rates of individual parameters based on their historical gradients. It accumulates the squared gradients over time and uses this information to adjust the learning rates for each parameter independently.\n",
    "\n",
    "- RMSprop (Root Mean Square Propagation):\n",
    "        RMSprop is a modification of Adagrad that addresses its tendency to aggressively reduce the learning rates. It uses a moving average of squared gradients, allowing the learning rates to adapt more smoothly.\n",
    "\n",
    "- Adam (Adaptive Moment Estimation):\n",
    "        Adam combines the benefits of both momentum and RMSprop. It maintains moving averages of both the gradients and their squared values, adjusting the learning rates accordingly. Adam is widely used and often provides effective optimization.\n",
    "\n",
    "Adaptive learning rate algorithms help models converge faster, especially when dealing with sparse or noisy data, and contribute to the stability of the training process.\n",
    "\n",
    "#### Real-world example of practical use in Python for a deep learning model with adaptive learning rate using the Adam optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Generate synthetic data for classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a simple deep learning model with Adam optimizer\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, input_dim=20, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and adaptive learning rate\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Adam optimizer, which automatically adapts the learning rates during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.3. Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is a regularization technique used in deep learning to prevent overfitting and improve the generalization of models. It involves randomly \"dropping out\" (setting to zero) a fraction of neurons during training. This helps prevent co-adaptation of neurons and encourages the network to learn more robust features.\n",
    "\n",
    "Key points about Dropout:\n",
    "\n",
    "- Random Neuron Deactivation:\n",
    "        During each training iteration, a random fraction of neurons is deactivated (output set to zero), both in the input and hidden layers. This prevents reliance on specific neurons and promotes a more distributed representation.\n",
    "\n",
    "- Ensemble Effect:\n",
    "        Dropout can be seen as training an ensemble of models, as different subsets of neurons are dropped out during each iteration. This ensemble effect helps improve generalization.\n",
    "\n",
    "- Regularization:\n",
    "        Dropout acts as a form of regularization, reducing the risk of overfitting by introducing noise and discouraging the network from memorizing the training data.\n",
    "\n",
    "- Applicability:\n",
    "        Dropout is commonly used in fully connected layers but can be applied to other layers as well, depending on the network architecture.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for a deep learning model with Dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Generate synthetic data for classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a simple deep learning model with Dropout\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, input_dim=20, activation='relu'),\n",
    "    layers.Dropout(0.5),  # Dropout layer with a dropout rate of 0.5 (50%)\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Dropout layer in a simple neural network for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.4. Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretraining is a technique in deep learning where a model is initially trained on a related task or dataset before being fine-tuned on the target task. This process allows the model to learn generic features from the initial dataset, which can be beneficial when the target dataset is limited or lacks sufficient labeled examples.\n",
    "\n",
    "Key points about Pretraining:\n",
    "\n",
    "- Initial Training on a Related Task:\n",
    "        The model is pretrained on a task or dataset that is related to the target task. This can be a larger dataset with similar features or a related task that shares common underlying representations.\n",
    "\n",
    "- Feature Learning:\n",
    "        Pretraining enables the model to learn generic features and representations that are potentially transferable to the target task. The initial layers of the network capture general patterns, while later layers adapt to the specific task.\n",
    "\n",
    "- Fine-Tuning:\n",
    "        After pretraining, the model is fine-tuned on the target task using the limited labeled data available. This process helps the model specialize for the specific task, leveraging the knowledge gained during pretraining.\n",
    "\n",
    "- Transfer Learning:\n",
    "        Pretraining is a form of transfer learning, where knowledge gained from one task is transferred to another. This is particularly useful in scenarios where labeled data for the target task is scarce.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for pretraining and fine-tuning using transfer learning with a pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Generate synthetic data for classification\n",
    "X, y = make_classification(n_samples=1000, n_features=224*224*3, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pre-trained VGG16 model (you can use other pre-trained models based on your task)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the pre-trained layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model with additional layers for the target task\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model on the target task\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the VGG16 model pre-trained on ImageNet and fine-tune it on a binary classification task with synthetic data. Replace the data and model with your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.5. Cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy, often used as a loss function, is a measure of the difference between two probability distributions. In the context of deep learning, it is commonly used as the loss function for classification tasks. The cross-entropy loss quantifies how well the predicted probability distribution aligns with the true distribution of the target labels.\n",
    "\n",
    "#### Key points about Cross-entropy:\n",
    "\n",
    "- Binary Cross-entropy:\n",
    "        For binary classification tasks, binary cross-entropy is used. It measures the dissimilarity between the predicted probability distribution and the true binary labels.\n",
    "\n",
    "- Categorical Cross-entropy:\n",
    "        For multi-class classification tasks, categorical cross-entropy is employed. It extends the binary cross-entropy to handle multiple classes.\n",
    "\n",
    "- Information Theory Interpretation:\n",
    "        Cross-entropy is derived from information theory and measures the average number of bits needed to represent an event from one distribution when using the optimal encoding based on another distribution.\n",
    "\n",
    "- Training Objective:\n",
    "        Minimizing cross-entropy during training aims to make the predicted probability distribution closer to the true distribution, improving the model's ability to classify accurately.\n",
    "\n",
    "#### Real-world example of practical use in Python for a deep learning model with cross-entropy as the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Generate synthetic data for binary classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a simple deep learning model with binary cross-entropy loss\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, input_dim=20, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model with binary cross-entropy loss\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use binary cross-entropy as the loss function for a simple neural network in a binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.6. Autoencoder: unsupervised deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is a type of unsupervised learning model in deep learning that is designed to learn efficient representations of data. It consists of an encoder and a decoder, which work together to reconstruct the input data. The encoder compresses the input into a latent space representation, and the decoder reconstructs the input from this representation.\n",
    "\n",
    "#### Key points about Autoencoder:\n",
    "\n",
    "- Encoder:\n",
    "        The encoder network maps the input data to a lower-dimensional representation, known as the latent space or encoding. This process captures the essential features of the input.\n",
    "\n",
    "- Decoder:\n",
    "        The decoder network reconstructs the input data from the encoding. The goal is to generate an output that is as close as possible to the original input.\n",
    "\n",
    "- Loss Function:\n",
    "        The loss function used during training measures the difference between the input and the reconstructed output. Common choices include mean squared error for real-valued data or binary cross-entropy for binary data.\n",
    "\n",
    "- Applications:\n",
    "        Autoencoders have various applications, including data denoising, dimensionality reduction, and anomaly detection. They are particularly useful when labeled data is scarce or unavailable.\n",
    "\n",
    "- Variations:\n",
    "        Variations of autoencoders include sparse autoencoders, denoising autoencoders, and variational autoencoders, each tailored to specific objectives.\n",
    "\n",
    "#### Real-world example of practical use in Python for an autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Generate synthetic data for binary classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a simple autoencoder\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 10  # Choose a lower-dimensional encoding\n",
    "autoencoder = keras.Sequential([\n",
    "    layers.Dense(encoding_dim, input_dim=input_dim, activation='relu'),\n",
    "    layers.Dense(input_dim, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X_train, X_train, epochs=10, batch_size=32, validation_data=(X_test, X_test))\n",
    "\n",
    "# Encode and decode the test set\n",
    "encoded_data = autoencoder.predict(X_test)\n",
    "\n",
    "# Evaluate the performance (e.g., using mean squared error)\n",
    "mse = np.mean(np.square(X_test - encoded_data))\n",
    "print(f\"Mean Squared Error on Test Set: {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we create a simple autoencoder using a dense neural network. The autoencoder learns a compressed representation of the input data, and the mean squared error is used to evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1. Introducing convolution operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution operation is a fundamental building block of Convolutional Neural Networks (CNNs). CNNs are particularly effective for tasks related to image analysis, but they have found applications in various domains. The convolution operation involves applying a filter (also known as a kernel) to the input data, allowing the network to capture local patterns and spatial hierarchies.\n",
    "\n",
    "#### Key points about the Convolution Operation:\n",
    "\n",
    "- Local Feature Extraction:\n",
    "        The convolution operation focuses on local regions of the input, allowing the network to capture specific features. This is in contrast to fully connected layers, which consider the entire input at once.\n",
    "\n",
    "- Shared Weights:\n",
    "        The same filter is applied across different spatial locations, promoting weight sharing. This reduces the number of parameters and allows the network to learn spatial hierarchies efficiently.\n",
    "\n",
    "- Feature Maps:\n",
    "        The output of the convolution operation is called a feature map. Each element in the feature map represents the presence of a specific feature in the input.\n",
    "\n",
    "- Pooling:\n",
    "        Pooling layers are often used after convolution to downsample the spatial dimensions and reduce computational complexity.\n",
    "\n",
    "- Applications:\n",
    "        CNNs are widely used for image classification, object detection, segmentation, and various computer vision tasks. They have also been applied to tasks in natural language processing and speech recognition.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for a convolutional neural network using TensorFlow and Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the digits dataset for image classification\n",
    "digits = load_digits()\n",
    "X, y = digits.images, digits.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the data to have a single channel (grayscale)\n",
    "X_train = X_train.reshape(-1, 8, 8, 1)\n",
    "X_test = X_test.reshape(-1, 8, 8, 1)\n",
    "\n",
    "# Create a simple CNN model with convolutional layers\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(8, 8, 1)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')  # Output layer for 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use a simple CNN for image classification on the digits dataset. The CNN includes convolutional layers, pooling layers, and dense layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.2. Multidimensional convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multidimensional convolution extends the concept of convolution to multiple dimensions, enabling Convolutional Neural Networks (CNNs) to process data with spatial structures in more than one dimension. While 2D convolution is commonly used for image data, multidimensional convolution is applicable to volumetric data such as 3D images or sequences with temporal dependencies.\n",
    "\n",
    "#### Key points about Multidimensional Convolution:\n",
    "\n",
    "- 3D Convolution:\n",
    "        In addition to height and width, 3D convolution considers depth (or time in the case of sequences). It involves applying a 3D filter to the input data, capturing spatiotemporal patterns.\n",
    "\n",
    "- Applications:\n",
    "        Multidimensional convolution is particularly useful for tasks involving volumetric data, such as medical imaging (3D CT or MRI scans), video analysis, and spatiotemporal data in general.\n",
    "\n",
    "- Filter Depth:\n",
    "        Filters used in multidimensional convolution have a depth dimension that matches the input data. Each element in the depth dimension of the filter corresponds to a slice of the input data.\n",
    "\n",
    "- Strides and Padding:\n",
    "        Similar to 2D convolution, multidimensional convolution can use strides and padding to control the spatial dimensions of the output feature map.\n",
    "\n",
    "- Example:\n",
    "        While 2D convolution is common for image data, 3D convolution is applied when considering volumetric data, and the concept extends to higher dimensions for more complex data structures.\n",
    "\n",
    "#### A real-world example of practical use in Python for multidimensional convolution using a 3D CNN with volumetric data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the 3D shape dataset (replace with your 3D data)\n",
    "# For this example, we use the COIL-20 dataset (volumetric images of objects)\n",
    "coil_20 = fetch_openml(name=\"COIL-20\", version=2)\n",
    "X, y = coil_20.data, coil_20.target.astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the data for 3D convolution (assuming volumetric data)\n",
    "X_train = X_train.reshape(-1, 32, 32, 32, 1)\n",
    "X_test = X_test.reshape(-1, 32, 32, 32, 1)\n",
    "\n",
    "# Create a 3D CNN model\n",
    "model = keras.Sequential([\n",
    "    layers.Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=(32, 32, 32, 1)),\n",
    "    layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(len(np.unique(y)), activation='softmax')  # Output layer for classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the 3D CNN\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use a 3D CNN for a volumetric dataset (COIL-20). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.3. Convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional layer is a core component of Convolutional Neural Networks (CNNs) responsible for learning local patterns and hierarchical representations within input data. It performs convolution operations by applying filters to the input, enabling the network to capture spatial hierarchies and features.\n",
    "\n",
    "#### Key points about the Convolutional Layer:\n",
    "\n",
    "- Filters (Kernels):\n",
    "        The convolutional layer uses filters (also called kernels) to scan the input data. These filters are small, learnable matrices that slide over the input, capturing local patterns.\n",
    "\n",
    "- Local Receptive Fields:\n",
    "        Each filter focuses on a local receptive field of the input. By sharing weights across the receptive field, the network learns to detect similar patterns at different spatial locations.\n",
    "\n",
    "- Strides and Padding:\n",
    "        Strides control the step size of the filter as it moves across the input, affecting the spatial dimensions of the output feature map. Padding can be used to preserve the spatial dimensions.\n",
    "\n",
    "- Activation Function:\n",
    "        Typically, a non-linear activation function (e.g., ReLU) follows the convolution operation, introducing non-linearity to the model and enabling it to learn complex representations.\n",
    "\n",
    "- Depth:\n",
    "        The depth of the convolutional layer corresponds to the number of filters applied. Each filter produces a feature map, and the depth of the layer is equal to the number of feature maps.\n",
    "\n",
    "- Pooling:\n",
    "        Pooling layers often follow convolutional layers to downsample the spatial dimensions of the feature maps and reduce computation.\n",
    "\n",
    "#### A real-world example of practical use in Python for a convolutional layer within a CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the digits dataset for image classification\n",
    "digits = load_digits()\n",
    "X, y = digits.images, digits.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the data to have a single channel (grayscale)\n",
    "X_train = X_train.reshape(-1, 8, 8, 1)\n",
    "X_test = X_test.reshape(-1, 8, 8, 1)\n",
    "\n",
    "# Create a CNN model with a convolutional layer\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(8, 8, 1)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')  # Output layer for 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we create a simple CNN for image classification on the digits dataset. The convolutional layer is applied to capture local patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4.1. Basic RNN models and applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNNs) are a type of neural network designed to handle sequential data by introducing a feedback loop that allows information to persist. Basic RNN models have a simple architecture that enables them to capture dependencies and patterns in sequences. They find applications in various domains, including natural language processing, time series analysis, and speech recognition.\n",
    "\n",
    "#### Key points about Basic RNN Models and Applications:\n",
    "\n",
    "- Sequential Processing:\n",
    "        RNNs process sequences by maintaining hidden states that capture information from previous time steps. This enables them to model dependencies in sequential data.\n",
    "\n",
    "- Vanishing Gradient Problem:\n",
    "        Basic RNNs suffer from the vanishing gradient problem, making it challenging for them to capture long-term dependencies. This limitation led to the development of more advanced RNN architectures.\n",
    "\n",
    "- Applications:\n",
    "        Basic RNNs are used in applications such as language modeling, machine translation, sentiment analysis, and stock price prediction. They are suitable for tasks where the context of previous elements in a sequence is crucial.\n",
    "\n",
    "- Unrolling in Time:\n",
    "        RNNs can be conceptualized as unrolled over time, with each time step representing a different input in the sequence. This unrolling illustrates how information flows through the network.\n",
    "\n",
    "- Challenges:\n",
    "        While basic RNNs are intuitive, they face challenges in capturing long-range dependencies, and more advanced architectures like Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) have been developed to address these issues.\n",
    "\n",
    "#### A real-world example of practical use in Python for a basic RNN using TensorFlow and Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load a dataset for sequential analysis (replace with your sequential data)\n",
    "imdb = fetch_openml(name=\"IMDB Reviews\", version=2)\n",
    "X, y = imdb.data, imdb.target.astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the text data (tokenization and padding)\n",
    "max_sequence_length = 100\n",
    "X_train = pad_sequences(X_train, maxlen=max_sequence_length, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Create a basic RNN model\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(input_dim=10000, output_dim=64, input_length=max_sequence_length),\n",
    "    layers.SimpleRNN(32),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the RNN\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use a basic RNN for sentiment analysis on the IMDB movie reviews dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4.2. Gated RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gated Recurrent Neural Networks (RNNs) represent an advancement over basic RNNs by introducing gating mechanisms that address the vanishing gradient problem. Gated RNNs, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), are designed to capture long-term dependencies in sequential data more effectively.\n",
    "\n",
    "#### Key points about Gated RNNs:\n",
    "\n",
    "- Vanishing Gradient Problem:\n",
    "        Gated RNNs mitigate the vanishing gradient problem encountered in basic RNNs, which hinders the learning of long-range dependencies in sequences.\n",
    "\n",
    "- LSTM and GRU:\n",
    "        LSTM and GRU are two popular architectures of gated RNNs. They incorporate gating mechanisms to control the flow of information, allowing the network to selectively retain or discard information from previous time steps.\n",
    "\n",
    "- Memory Cells:\n",
    "        Both LSTM and GRU introduce memory cells that can store information for long periods. These memory cells enable the network to capture relevant information over extended sequences.\n",
    "\n",
    "- Gating Mechanisms:\n",
    "        Gating mechanisms include the input gate, forget gate, and output gate. These gates regulate the flow of information, making it possible for the network to selectively update its memory.\n",
    "\n",
    "- Applications:\n",
    "        Gated RNNs find applications in tasks requiring the understanding of context and long-term dependencies, such as natural language processing, speech recognition, and time series prediction.\n",
    "\n",
    "#### Real-world example of practical use in Python for a Gated RNN using the LSTM architecture with TensorFlow and Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load a dataset for sequential analysis (replace with your sequential data)\n",
    "imdb = fetch_openml(name=\"IMDB Reviews\", version=2)\n",
    "X, y = imdb.data, imdb.target.astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the text data (tokenization and padding)\n",
    "max_sequence_length = 100\n",
    "X_train = pad_sequences(X_train, maxlen=max_sequence_length, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Create a Gated RNN model with LSTM\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(input_dim=10000, output_dim=64, input_length=max_sequence_length),\n",
    "    layers.LSTM(32),  # LSTM layer with 32 units\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the Gated RNN (LSTM)\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use a Gated RNN with the LSTM architecture for sentiment analysis on the IMDB movie reviews dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5.2. Graph convolutional networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Convolutional Networks (GCNs) are a type of Graph Neural Network designed to operate on graph-structured data. They extend the convolutional neural network concept to graph domains, enabling the learning of node representations that capture both local and global structural information.\n",
    "\n",
    "#### Key points about Graph Convolutional Networks:\n",
    "\n",
    "1. Graph Structure:\n",
    "        GCNs are designed to work with data organized in the form of a graph, where nodes represent entities, and edges represent relationships between entities.\n",
    "\n",
    "2. Node Representations:\n",
    "        GCNs learn embeddings for nodes in the graph, capturing information about the node itself and its neighborhood.\n",
    "\n",
    "3. Graph Convolution Operation:\n",
    "        The key operation in GCNs is the graph convolution, which involves aggregating information from neighboring nodes. This allows nodes to incorporate information from their local context.\n",
    "\n",
    "4. Depth-wise Propagation:\n",
    "        GCNs can be stacked in multiple layers to allow information propagation through the graph. Each layer refines node representations by considering increasingly broader contexts.\n",
    "\n",
    "5. Applications:\n",
    "        GCNs find applications in tasks such as node classification, link prediction, and graph-level tasks. They are useful in scenarios where understanding the relationships and dependencies in a graph is essential.\n",
    "\n",
    "#### A real-world example of practical use in Python for a Graph Convolutional Network using the DGL library (Deep Graph Library):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.data\n",
    "\n",
    "# Load a dataset for graph analysis (replace with your graph data)\n",
    "dataset = dgl.data.CoraGraphDataset()\n",
    "g = dataset[0]\n",
    "\n",
    "# Define a simple Graph Convolutional Network (GCN) model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = dgl.nn.GraphConv(in_feats, hidden_size)\n",
    "        self.conv2 = dgl.nn.GraphConv(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = F.relu(self.conv1(g, features))\n",
    "        x = self.conv2(g, x)\n",
    "        return x\n",
    "\n",
    "# Prepare the data and model\n",
    "features = g.ndata['feat']\n",
    "labels = g.ndata['label']\n",
    "train_mask = g.ndata['train_mask']\n",
    "test_mask = g.ndata['test_mask']\n",
    "\n",
    "# Create and initialize the GCN model\n",
    "model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(30):\n",
    "    logits = model(g, features)\n",
    "    loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{30} | Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(g, features)\n",
    "    pred = logits.argmax(1)\n",
    "    accuracy = (pred[test_mask] == labels[test_mask]).float().mean().item()\n",
    "\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use a simple GCN model to perform node classification on the Cora citation network dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining_2024_spring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
