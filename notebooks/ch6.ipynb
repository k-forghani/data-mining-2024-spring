{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1. Decision tree induction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree induction is a popular machine learning technique used for both classification and regression tasks. It's a tree-like model where each node represents a decision based on a feature, and each branch represents the outcome of that decision, leading to the next node. The leaves of the tree represent the final output or the decision.\n",
    "Decision Tree Induction:\n",
    "\n",
    "#### Decision tree induction involves the following steps:\n",
    "\n",
    "    - Selection of the Root Node: The algorithm selects the feature that best splits the data into subsets, considering criteria such as Gini impurity or information gain.\n",
    "\n",
    "    - Splitting the Nodes: The data is split into subsets based on the chosen feature.\n",
    "\n",
    "    - Recursive Process: Steps 1 and 2 are recursively applied to each subset until a stopping criterion is met, such as a specific depth or a minimum number of samples in a node.\n",
    "\n",
    "    - Assigning Labels: The final step involves assigning labels or values to the leaves of the tree, based on the majority class or average value in the leaf node.\n",
    "\n",
    "Decision trees are interpretable, easy to understand, and can handle both numerical and categorical data.\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a real-world example using the famous Iris dataset, where the goal is to classify iris flowers into three species (setosa, versicolor, or virginica) based on features like sepal length, sepal width, petal length, and petal width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Display the accuracy of the model\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display the decision tree rules\n",
    "tree_rules = export_text(clf, feature_names=iris.feature_names)\n",
    "print(\"Decision Tree Rules:\\n\", tree_rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the scikit-learn library to create a DecisionTreeClassifier, train it on the Iris dataset, make predictions, and evaluate the accuracy. The export_text function is used to display the decision tree rules in a human-readable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2. Attribute selection measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of data mining and machine learning, attribute selection measures, also known as feature selection, refer to the process of choosing a subset of relevant features from a larger set of features. The goal is to improve the model's performance, reduce overfitting, and enhance interpretability. There are various attribute selection measures, each with its own criteria for evaluating the importance of features. Some common \n",
    "#### measures include:\n",
    "\n",
    "1. Information Gain: Measures how well a feature separates the data into classes.\n",
    "\n",
    "2. Gain Ratio: Similar to Information Gain but adjusts for the number of branches a node has.\n",
    "\n",
    "3. Gini Index: Measures the impurity of a set of examples, with lower values indicating better splits.\n",
    "\n",
    "4. Chi-square: Tests the independence between the feature and the class, helping to select features that are statistically significant.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's use the famous Iris dataset again and apply Information Gain as the attribute selection measure to choose the most relevant features for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use Information Gain for feature selection\n",
    "selector = SelectKBest(mutual_info_classif, k=2)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the classifier to the selected training data\n",
    "clf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the selected test data\n",
    "predictions = clf.predict(X_test_selected)\n",
    "\n",
    "# Display the accuracy of the model\n",
    "accuracy = clf.score(X_test_selected, y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display the decision tree rules for the selected features\n",
    "selected_feature_names = [iris.feature_names[i] for i in selector.get_support(indices=True)]\n",
    "tree_rules = export_text(clf, feature_names=selected_feature_names)\n",
    "print(\"Decision Tree Rules for Selected Features:\\n\", tree_rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use scikit-learn's SelectKBest with mutual information as the scoring function to select the top k features with the highest information gain. We then train a decision tree classifier on the selected features and evaluate its accuracy. This example demonstrates the practical application of attribute selection measures to enhance the performance of a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3. Tree pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree pruning is a technique used in decision tree algorithms to prevent overfitting. Overfitting occurs when a tree is too complex and captures noise in the training data, leading to poor generalization on new, unseen data. Pruning involves removing parts of the tree that do not provide significant predictive power. There are two main types of pruning:\n",
    "\n",
    "1. Pre-pruning (Early Stopping): This involves stopping the tree-building process early, before it becomes too complex. Common pre-pruning strategies include limiting the maximum depth of the tree, setting a minimum number of samples required to split a node, or requiring a minimum number of samples in a leaf.\n",
    "\n",
    "2. Post-pruning (Pruning After Tree Construction): This involves building the full tree and then removing nodes that do not contribute significantly to predictive accuracy. Post-pruning methods include cost-complexity pruning, where a hyperparameter (alpha) controls the trade-off between tree complexity and fit to the training data.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's use the Iris dataset again and apply post-pruning using cost-complexity pruning to a decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Define the hyperparameter grid for cost-complexity pruning\n",
    "param_grid = {'ccp_alpha': [0.001, 0.002, 0.003, 0.004, 0.005]}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameter\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best decision tree classifier\n",
    "best_clf = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = best_clf.predict(X_test)\n",
    "\n",
    "# Display the accuracy of the pruned tree\n",
    "accuracy = best_clf.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display the decision tree rules for the pruned tree\n",
    "tree_rules = export_text(best_clf, feature_names=iris.feature_names)\n",
    "print(\"Decision Tree Rules for Pruned Tree:\\n\", tree_rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use GridSearchCV to perform a search over a hyperparameter grid for the best alpha value (ccp_alpha) for cost-complexity pruning. The selected hyperparameter helps control the trade-off between tree complexity and fit to the training data, resulting in a pruned decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1. Bayes’ theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' Theorem is a fundamental concept in probability theory, named after Reverend Thomas Bayes. It provides a way to update our beliefs about a hypothesis based on new evidence. The theorem is expressed mathematically as:\n",
    "\n",
    "P(A∣B)=P(B∣A)⋅P(A)P(B)\n",
    "\n",
    "where:\n",
    "\n",
    "    - P(A∣B) is the probability of event A occurring given that event B has occurred.\n",
    "    - P(B∣A) is the probability of event B occurring given that event A has occurred.\n",
    "    - P(A) and P(B) are the probabilities of events A and B occurring, respectively.\n",
    "\n",
    "Bayes' Theorem is widely used in statistics and machine learning for tasks such as classification, spam filtering, and medical diagnosis.\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example of spam email classification using the Naive Bayes algorithm. We'll use the famous \"Spambase\" dataset, which contains features based on the frequency of certain words and characters in emails, along with labels indicating whether an email is spam or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the Spambase dataset (you may need to adjust the path)\n",
    "data = pd.read_csv('path_to_spambase_dataset/spambase.csv')\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Naive Bayes classifier (MultinomialNB for discrete features)\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Naive Bayes algorithm (specifically, Multinomial Naive Bayes) to classify emails as spam or non-spam based on the frequency of certain words and characters. The CountVectorizer is used to convert the text data into numerical features suitable for the Naive Bayes algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2. Naïve Bayesian classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve Bayesian Classification is a probabilistic classification technique based on Bayes' Theorem with the \"naïve\" assumption of feature independence. Despite its simplicity and the independence assumption, Naïve Bayes classifiers often perform well in practice, especially for text classification and spam filtering.\n",
    "\n",
    "The \"naïve\" assumption implies that the presence (or absence) of a particular feature in a class is independent of the presence (or absence) of other features. This assumption simplifies the computation and makes it computationally efficient, even for datasets with a large number of features.\n",
    "\n",
    "The basic formula for Naïve Bayesian Classification is:\n",
    "\n",
    "P(y∣X)=P(X∣y)⋅P(y)P(X)\n",
    "\n",
    "where:\n",
    "\n",
    "    - P(y∣X) is the probability of class y given the features X.\n",
    "    - P(X∣y) is the probability of features X given class y.\n",
    "    - P(y) is the prior probability of class y.\n",
    "    - P(X) is the prior probability of features X.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example of text classification using Naïve Bayes. We'll use the famous 20 Newsgroups dataset, which consists of newsgroup documents categorized into 20 different topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text data to numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Create a Naïve Bayes classifier (MultinomialNB for discrete features)\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Train the classifier on the TF-IDF transformed training data\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the TF-IDF transformed test data\n",
    "predictions = clf.predict(X_test_tfidf)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions, target_names=newsgroups.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Multinomial Naive Bayes classifier on the TF-IDF (Term Frequency-Inverse Document Frequency) transformed text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.1. k-nearest-neighbor classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-Nearest-Neighbor (k-NN) algorithm is a simple and intuitive classification technique based on the idea that similar instances are likely to belong to the same class. In other words, it classifies a data point based on the majority class of its k nearest neighbors in the feature space. The distance metric (e.g., Euclidean distance) is commonly used to measure the similarity between instances.\n",
    "\n",
    "#### The key steps in the k-NN algorithm are:\n",
    "\n",
    "1. Choose a value for k: Decide on the number of neighbors (k) to consider when making a classification.\n",
    "\n",
    "2. Compute distances: Calculate the distance between the target instance and all instances in the training set.\n",
    "\n",
    "3. Identify k-nearest neighbors: Select the k instances with the smallest distances.\n",
    "\n",
    "4. Majority voting: Assign the class label based on the majority class among the k neighbors.\n",
    "\n",
    "k-NN is a lazy learner, meaning it doesn't build a model during the training phase. Instead, it stores the entire training dataset and performs computations at the time of prediction.\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's use the famous Iris dataset and apply the k-NN algorithm for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a k-NN classifier with k=3 (you can adjust the value of k)\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions, target_names=iris.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use scikit-learn to create a k-NN classifier with k=3. The classifier is trained on the Iris dataset, and predictions are made on the test data. The accuracy and classification report are then displayed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2. Case-based reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case-Based Reasoning is a problem-solving approach that relies on retrieving and adapting solutions from past experiences or cases. It operates on the principle that similar problems have similar solutions. CBR consists of four main steps:\n",
    "\n",
    "1. Retrieve: Identify similar cases from the case base (database of past experiences) based on the current problem.\n",
    "\n",
    "2. Reuse: Apply the solution from the retrieved case to the current problem. If an exact match is not found, adapt the solution to fit the current context.\n",
    "\n",
    "3. Revise: Evaluate the solution's success and, if necessary, revise the solution based on feedback or new information.\n",
    "\n",
    "4. Retain: Store the new case in the case base for future use.\n",
    "\n",
    "CBR is particularly useful in situations where traditional rule-based or model-based approaches may be challenging due to uncertainty, complexity, or changing environments.\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example of case-based reasoning for a recommendation system. We'll use the MovieLens dataset and recommend movies based on user preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the MovieLens dataset (you may need to adjust the path)\n",
    "movies = pd.read_csv('path_to_movielens_dataset/movies.csv')\n",
    "ratings = pd.read_csv('path_to_movielens_dataset/ratings.csv')\n",
    "\n",
    "# Merge movies and ratings data\n",
    "movie_ratings = pd.merge(ratings, movies, on='movieId')\n",
    "\n",
    "# Create a user-item matrix for collaborative filtering\n",
    "user_item_matrix = movie_ratings.pivot_table(index='userId', columns='title', values='rating', fill_value=0)\n",
    "\n",
    "# Use TF-IDF to convert movie titles into numerical features\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(movies['title'])\n",
    "\n",
    "# Calculate cosine similarity between movie titles\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Function to get movie recommendations using CBR\n",
    "def get_movie_recommendations(movie_title):\n",
    "    movie_index = movies.index[movies['title'] == movie_title].tolist()[0]\n",
    "    cosine_scores = list(enumerate(cosine_sim[movie_index]))\n",
    "    cosine_scores = sorted(cosine_scores, key=lambda x: x[1], reverse=True)\n",
    "    top_similar_movies = cosine_scores[1:6]  # Exclude the input movie itself\n",
    "\n",
    "    recommended_movies = []\n",
    "    for index, score in top_similar_movies:\n",
    "        recommended_movies.append(movies['title'].iloc[index])\n",
    "\n",
    "    return recommended_movies\n",
    "\n",
    "# Example usage\n",
    "input_movie = \"The Dark Knight\"\n",
    "recommendations = get_movie_recommendations(input_movie)\n",
    "\n",
    "# Display the recommendations\n",
    "print(f\"Movies similar to '{input_movie}':\")\n",
    "for movie in recommendations:\n",
    "    print(\"-\", movie)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use case-based reasoning to recommend movies based on the similarity of their titles. The TF-IDF vectorizer is used to convert movie titles into numerical features, and cosine similarity is calculated to measure the similarity between movies. The get_movie_recommendations function retrieves similar movies based on the input movie title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.1. Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a statistical method used for modeling the relationship between a dependent variable and one or more independent variables. The relationship is assumed to be linear, meaning that changes in the dependent variable are proportional to changes in the independent variable(s). The basic equation for a simple linear regression is:\n",
    "\n",
    "y=mx+b\n",
    "\n",
    "where:\n",
    "\n",
    "    - y is the dependent variable.\n",
    "    - x is the independent variable.\n",
    "    - m is the slope (the rate at which y changes with respect to x).\n",
    "    - b is the y-intercept (the value of y when x is 0).\n",
    "\n",
    "In multiple linear regression, the equation extends to include multiple independent variables:\n",
    "\n",
    "y=b0+b1x1+b2x2+…+bnxn\n",
    "\n",
    "where:\n",
    "\n",
    "    - b0​ is the y-intercept.\n",
    "    - b1,b2,…,bn​ are the coefficients for the independent variables x1,x2,…,xn.\n",
    "\n",
    "Linear regression is commonly used for predicting numeric values, such as predicting house prices based on square footage or predicting sales based on advertising spend.\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example of linear regression for predicting house prices using the Boston Housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = pd.DataFrame(boston.target, columns=['target'])\n",
    "\n",
    "# Select a single feature (e.g., 'RM' - average number of rooms per dwelling)\n",
    "X_feature = X[['RM']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_feature, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Display the model coefficients and performance metrics\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Plot the regression line\n",
    "plt.scatter(X_test, y_test, color='black')\n",
    "plt.plot(X_test, y_pred, color='blue', linewidth=3)\n",
    "plt.xlabel('Average Number of Rooms (RM)')\n",
    "plt.ylabel('House Price')\n",
    "plt.title('Linear Regression: House Price Prediction')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the average number of rooms ('RM') as the independent variable to predict house prices. The linear regression model is trained on the Boston Housing dataset, and predictions are made on the test data. The performance of the model is evaluated using mean squared error and R-squared. The regression line is plotted to visualize the relationship between the average number of rooms and house prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.2. Perceptron: turning linear regression to classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron is a basic building block of artificial neural networks and serves as a simple binary classifier. It's an algorithm for supervised learning that takes a set of input features and produces an output (binary classification) based on a linear combination of those inputs. The perceptron is trained to learn the weights associated with each input feature, and it applies a step function to make a binary decision.\n",
    "\n",
    "In the context of turning linear regression into classification using a perceptron:\n",
    "\n",
    "1. Linear Regression:\n",
    "    In linear regression, the model predicts a continuous output. The linear regression equation is used to calculate a numerical value based on input features.\n",
    "\n",
    "2. Thresholding with Perceptron:\n",
    "    To convert this into a classification problem, a thresholding step is introduced. If the calculated numerical value is above a certain threshold, the perceptron outputs one class (e.g., 1), and if it's below the threshold, it outputs the other class (e.g., 0).\n",
    "\n",
    "3. Activation Function:\n",
    "    The step function used for thresholding is the activation function of the perceptron. Commonly used activation functions include the Heaviside step function or the sigmoid function.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example of using a perceptron for binary classification using the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Consider only the first two features for simplicity and binary classification\n",
    "X = X[:, :2]\n",
    "\n",
    "# Map iris classes to binary classes (setosa vs. non-setosa)\n",
    "y_binary = (y == 0).astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a perceptron classifier\n",
    "perceptron = Perceptron()\n",
    "\n",
    "# Train the perceptron on the training data\n",
    "perceptron.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = perceptron.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_binary, cmap=plt.cm.Paired, edgecolors='k')\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.title('Perceptron: Iris Binary Classification')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the first two features of the Iris dataset for simplicity and perform binary classification to distinguish setosa from non-setosa flowers. The perceptron is trained on the training data and used to make predictions on the test data. The accuracy and classification report are then displayed, and the decision boundary is visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.3. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite its name, logistic regression is a classification algorithm, not a regression one. It's particularly useful for binary classification problems, where the goal is to predict whether an instance belongs to one of two classes. Logistic regression models the probability that an instance belongs to a particular class using the logistic function (also known as the sigmoid function). The logistic function outputs values between 0 and 1, which can be interpreted as probabilities.\n",
    "\n",
    "The logistic regression model can be mathematically expressed as:\n",
    "\n",
    "P(y=1)=11+e−(b0+b1x1+b2x2+…+bnxn)\n",
    "\n",
    "where:\n",
    "\n",
    "    - P(y=1) is the probability of the instance belonging to class 1.\n",
    "    - e is the base of the natural logarithm.\n",
    "    - b0,b1,…,bn are the coefficients.\n",
    "    - x1,x2,…,xn​ are the input features.\n",
    "\n",
    "Logistic regression is widely used in various domains, such as healthcare for disease prediction, marketing for customer churn prediction, and finance for credit scoring.\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example of using logistic regression for binary classification using the famous Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Titanic dataset (you may need to adjust the path)\n",
    "titanic = pd.read_csv('path_to_titanic_dataset/titanic.csv')\n",
    "\n",
    "# Drop rows with missing values and select relevant features\n",
    "titanic = titanic.dropna(subset=['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Survived'])\n",
    "X = titanic[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
    "y = titanic['Survived']\n",
    "\n",
    "# Convert categorical features to numerical using one-hot encoding\n",
    "X = pd.get_dummies(X, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = logreg.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = pd.crosstab(y_test, predictions, rownames=['Actual'], colnames=['Predicted'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, logistic regression is applied to predict whether passengers on the Titanic survived or not. The model is trained on features such as passenger class, gender, age, number of siblings/spouses aboard, number of parents/children aboard, fare, and embarkation port. The accuracy, classification report, and confusion matrix are displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.1. Metrics for evaluating classifier performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the performance of a classifier is crucial to understanding how well it generalizes to new, unseen data. Various metrics provide insights into different aspects of a classifier's performance. Here are some common metrics:\n",
    "\n",
    "1. Accuracy:\n",
    "    Accuracy is the ratio of correctly predicted instances to the total instances. It provides an overall measure of classification correctness.\n",
    "    Accuracy=True Positives+True NegativesTotal InstancesAccuracy=Total InstancesTrue Positives+True Negatives​\n",
    "\n",
    "2. Precision:\n",
    "    Precision is the ratio of correctly predicted positive observations to the total predicted positives. It measures the accuracy of the positive predictions.\n",
    "    Precision=True PositivesTrue Positives+False PositivesPrecision=True Positives+False PositivesTrue Positives​\n",
    "\n",
    "3. Recall (Sensitivity or True Positive Rate):\n",
    "    Recall is the ratio of correctly predicted positive observations to the total actual positives. It measures the ability of the classifier to capture all positive instances.\n",
    "    Recall=True PositivesTrue Positives+False NegativesRecall=True Positives+False NegativesTrue Positives​\n",
    "\n",
    "4. F1 Score:\n",
    "    The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall.\n",
    "    F1 Score=2×Precision×RecallPrecision+RecallF1 Score=Precision+Recall2×Precision×Recall​\n",
    "\n",
    "5. Confusion Matrix:\n",
    "    A confusion matrix is a table that summarizes the classifier's performance, showing the counts of true positive, true negative, false positive, and false negative predictions.\n",
    "\n",
    "6. Receiver Operating Characteristic (ROC) Curve:\n",
    "    The ROC curve plots the true positive rate against the false positive rate at various threshold settings. It helps visualize the trade-off between sensitivity and specificity.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Titanic dataset and evaluate the performance of a logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Titanic dataset (you may need to adjust the path)\n",
    "titanic = pd.read_csv('path_to_titanic_dataset/titanic.csv')\n",
    "\n",
    "# Drop rows with missing values and select relevant features\n",
    "titanic = titanic.dropna(subset=['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Survived'])\n",
    "X = titanic[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
    "y = titanic['Survived']\n",
    "\n",
    "# Convert categorical features to numerical using one-hot encoding\n",
    "X = pd.get_dummies(X, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = logreg.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Display the metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Plot the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, logistic regression is used to predict survival on the Titanic. The classifier's performance is evaluated using accuracy, precision, recall, F1 score, and a confusion matrix. Additionally, an ROC curve is plotted to visualize the trade-off between sensitivity and specificity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.2. Holdout method and random subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Holdout Method and Random Subsampling are techniques used in machine learning for evaluating the performance of a model. They involve splitting the dataset into training and testing sets to assess how well the model generalizes to new, unseen data.\n",
    "\n",
    "#### Holdout Method:\n",
    "    In the Holdout Method, the dataset is divided into two parts: a training set and a testing set. The training set is used to train the model, while the testing set is reserved for evaluating its performance. Common split ratios include 70-30, 80-20, or 90-10, depending on the size of the dataset.\n",
    "\n",
    "#### Random Subsampling (or k-Fold Cross-Validation):\n",
    "    Random Subsampling involves dividing the dataset into k subsets (folds). The model is trained k times, each time using k-1 folds for training and the remaining fold for testing. This process is repeated k times, with each fold serving as the testing set exactly once. The performance metrics are then averaged across all iterations.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Iris dataset and the Holdout Method for evaluating a k-NN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets using the Holdout Method\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a k-NN classifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the Iris dataset is split into training and testing sets using the Holdout Method. A k-NN classifier is then trained on the training set and evaluated on the testing set. The accuracy and classification report are displayed to assess the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.3. Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a resampling technique used to assess the performance and generalization of a machine learning model. It helps to overcome the limitations of a single train-test split by providing a more robust estimate of the model's performance. The most common form of cross-validation is k-Fold Cross-Validation, where the dataset is divided into k subsets (folds). The model is trained k times, each time using k-1 folds for training and the remaining fold for testing. This process is repeated k times, with each fold serving as the testing set exactly once. The performance metrics are then averaged across all iterations.\n",
    "\n",
    "The steps of k-Fold Cross-Validation are as follows:\n",
    "\n",
    "1. Split Data:\n",
    "    Divide the dataset into k equally sized folds.\n",
    "\n",
    "2. Train-Test Iterations:\n",
    "    For each iteration, use k-1 folds for training and the remaining fold for testing.\n",
    "\n",
    "3. Performance Metrics:\n",
    "    Evaluate the model's performance on each iteration and record the performance metrics.\n",
    "\n",
    "4. Average Metrics:\n",
    "    Calculate the average of the recorded performance metrics across all iterations.\n",
    "\n",
    "#### Benefits of Cross-Validation:\n",
    "\n",
    "    - Provides a more reliable estimate of a model's performance.\n",
    "\n",
    "    - Reduces the impact of variability in a single train-test split.\n",
    "\n",
    "    - Utilizes the entire dataset for training and testing.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Iris dataset and k-Fold Cross-Validation with a support vector machine (SVM) classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create a support vector machine (SVM) classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "\n",
    "# Perform k-Fold Cross-Validation (k=5)\n",
    "cv_scores = cross_val_score(svm_classifier, X, y, cv=5)\n",
    "\n",
    "# Display the cross-validated accuracy scores\n",
    "print(\"Cross-Validated Accuracy Scores:\", cv_scores)\n",
    "print(f\"Average Accuracy: {cv_scores.mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the Iris dataset is used to perform k-Fold Cross-Validation with a linear SVM classifier. The cross_val_score function from scikit-learn is employed to obtain accuracy scores for each fold. The average accuracy across all folds is then calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.4. Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap is a resampling technique used to estimate the variability and uncertainty associated with a sample statistic, such as the mean or standard deviation. It involves repeatedly sampling, with replacement, from the observed data to create multiple bootstrap samples. Each bootstrap sample is then used to compute the sample statistic of interest. By analyzing the distribution of these computed statistics across multiple bootstrap samples, one can obtain confidence intervals and make more robust statistical inferences.\n",
    "\n",
    "#### The steps of the Bootstrap method are as follows:\n",
    "\n",
    "1. Original Sample:\n",
    "        Start with the original dataset of size NN.\n",
    "\n",
    "2. Bootstrap Samples:\n",
    "        Generate BB bootstrap samples by randomly selecting NN data points from the original dataset with replacement.\n",
    "\n",
    "3. Statistic Computation:\n",
    "        For each bootstrap sample, compute the sample statistic of interest (e.g., mean, standard deviation).\n",
    "\n",
    "4. Analysis:\n",
    "        Analyze the distribution of computed statistics to estimate variability, confidence intervals, or perform hypothesis testing.\n",
    "\n",
    "#### Benefits of Bootstrap:\n",
    "\n",
    "    - Provides an empirical estimate of the sampling distribution of a statistic.\n",
    "\n",
    "    - Useful when assumptions of parametric statistical methods are violated.\n",
    "\n",
    "    - Robust in the presence of outliers.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Iris dataset to estimate the confidence interval of the mean sepal length using the Bootstrap method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "sepal_length = iris.data[:, 0]\n",
    "\n",
    "# Set the number of bootstrap samples\n",
    "num_bootstrap_samples = 1000\n",
    "\n",
    "# Initialize an array to store bootstrap sample means\n",
    "bootstrap_means = np.zeros(num_bootstrap_samples)\n",
    "\n",
    "# Generate bootstrap samples and compute means\n",
    "for i in range(num_bootstrap_samples):\n",
    "    bootstrap_sample = resample(sepal_length, replace=True)\n",
    "    bootstrap_means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "# Display the results\n",
    "print(\"Bootstrap Sample Mean: {:.2f}\".format(np.mean(bootstrap_means)))\n",
    "print(\"95% Confidence Interval: [{:.2f}, {:.2f}]\".format(confidence_interval[0], confidence_interval[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the Bootstrap method is used to estimate the 95% confidence interval of the mean sepal length in the Iris dataset. The resample function from scikit-learn is utilized to generate bootstrap samples. The mean of each bootstrap sample is computed, and the distribution of bootstrap sample means is used to estimate the confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.5. Model selection using statistical tests of significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection is a critical step in the data mining process, involving the identification and evaluation of different models to choose the one that best fits the data. Statistical tests of significance can be employed for model selection by comparing the performance of different models and determining if the observed differences are statistically significant.\n",
    "\n",
    "#### The process typically involves the following steps:\n",
    "\n",
    "1. Select Candidate Models:\n",
    "    Choose a set of candidate models that are relevant to the problem at hand.\n",
    "\n",
    "2. Train Models:\n",
    "    Train each model on the training dataset.\n",
    "\n",
    "3. Evaluate Models:\n",
    "    Evaluate the performance of each model on a validation dataset or through cross-validation.\n",
    "\n",
    "4. Statistical Testing:\n",
    "    Apply statistical tests to compare the performance metrics of the models.\n",
    "\n",
    "5. Select Best Model:\n",
    "    Choose the model with the best performance, considering both practical significance and statistical significance.\n",
    "\n",
    "Commonly used statistical tests for model selection include t-tests, ANOVA, or their non-parametric counterparts. These tests help determine if observed differences in performance metrics are likely due to genuine differences in model effectiveness rather than random chance.\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Iris dataset to compare the performance of two classification models (e.g., Decision Tree and Random Forest) using a t-test for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create classifiers (Decision Tree and Random Forest)\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform cross-validation for each model\n",
    "dt_scores = cross_val_score(dt_classifier, X, y, cv=5, scoring='accuracy')\n",
    "rf_scores = cross_val_score(rf_classifier, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Perform a paired t-test for accuracy\n",
    "t_stat, p_value = ttest_rel(dt_scores, rf_scores)\n",
    "\n",
    "# Display the results\n",
    "print(\"Decision Tree Mean Accuracy: {:.2f}\".format(dt_scores.mean()))\n",
    "print(\"Random Forest Mean Accuracy: {:.2f}\".format(rf_scores.mean()))\n",
    "print(\"Paired t-test p-value: {:.4f}\".format(p_value))\n",
    "\n",
    "# Check for statistical significance (e.g., p-value < 0.05)\n",
    "if p_value < 0.05:\n",
    "    print(\"The difference in accuracy is statistically significant. Choose the model accordingly.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant difference in accuracy between the models.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, cross-validation is performed for both Decision Tree and Random Forest classifiers on the Iris dataset. A paired t-test is then conducted to determine if there is a statistically significant difference in accuracy between the two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.6. Comparing classifiers based on cost–benefit and ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing classifiers involves assessing their performance using various metrics and visualization techniques. Two common approaches are based on cost–benefit analysis and Receiver Operating Characteristic (ROC) curves.\n",
    "\n",
    "1. Cost–Benefit Analysis:\n",
    "    Cost–benefit analysis involves considering the practical consequences of classification decisions. It assigns costs and benefits to different outcomes (true positive, false positive, true negative, false negative) and calculates a total cost or benefit. This approach is especially useful when the consequences of false positives and false negatives are different.\n",
    "\n",
    "2. ROC Curves:\n",
    "    ROC curves graphically depict the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) for different classification thresholds. The area under the ROC curve (AUC-ROC) summarizes the overall performance of the classifier across various threshold settings.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Breast Cancer Wisconsin dataset to compare two classifiers (e.g., Logistic Regression and Random Forest) based on cost–benefit analysis and ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Breast Cancer Wisconsin dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create classifiers (Logistic Regression and Random Forest)\n",
    "logreg_classifier = LogisticRegression()\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the classifiers\n",
    "logreg_classifier.fit(X_train, y_train)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "logreg_predictions = logreg_classifier.predict(X_test)\n",
    "rf_predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "# Cost–Benefit Analysis (Example: Assuming cost of false negative is 5 times the cost of false positive)\n",
    "cost_factor = 5\n",
    "cost_benefit_logreg = confusion_matrix(y_test, logreg_predictions) * np.array([[0, 1], [cost_factor, 0]]).T\n",
    "cost_benefit_rf = confusion_matrix(y_test, rf_predictions) * np.array([[0, 1], [cost_factor, 0]]).T\n",
    "\n",
    "# Display the cost–benefit matrices\n",
    "print(\"Cost–Benefit Matrix (Logistic Regression):\\n\", cost_benefit_logreg)\n",
    "print(\"Total Cost (Logistic Regression):\", np.sum(cost_benefit_logreg))\n",
    "\n",
    "print(\"\\nCost–Benefit Matrix (Random Forest):\\n\", cost_benefit_rf)\n",
    "print(\"Total Cost (Random Forest):\", np.sum(cost_benefit_rf))\n",
    "\n",
    "# ROC Curves\n",
    "fpr_logreg, tpr_logreg, _ = roc_curve(y_test, logreg_classifier.predict_proba(X_test)[:, 1])\n",
    "roc_auc_logreg = auc(fpr_logreg, tpr_logreg)\n",
    "\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_classifier.predict_proba(X_test)[:, 1])\n",
    "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "# Plot ROC Curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_logreg, tpr_logreg, color='darkorange', lw=2, label=f'Logistic Regression (AUC = {roc_auc_logreg:.2f})')\n",
    "plt.plot(fpr_rf, tpr_rf, color='green', lw=2, label=f'Random Forest (AUC = {roc_auc_rf:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, Logistic Regression and Random Forest classifiers are trained on the Breast Cancer Wisconsin dataset. Cost–benefit analysis is performed based on assumed costs of false positives and false negatives. ROC curves are plotted to compare the classifiers' performance in terms of sensitivity and specificity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.1. Introducing ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods are machine learning techniques that combine predictions from multiple individual models to create a more robust and accurate model. The idea is to leverage the diversity among different models to improve overall predictive performance. Ensemble methods can be applied to both classification and regression tasks.\n",
    "\n",
    "#### There are two main types of ensemble methods:\n",
    "\n",
    "1. Bagging (Bootstrap Aggregating):\n",
    "    Bagging involves training multiple instances of the same model on different subsets of the training data, often created through bootstrapping (sampling with replacement). The final prediction is obtained by averaging (for regression) or voting (for classification) over the predictions of individual models. Random Forest is a popular bagging algorithm.\n",
    "\n",
    "2. Boosting:\n",
    "    Boosting builds a sequence of weak learners (models that perform slightly better than random chance) sequentially, with each model focusing on correcting the errors of its predecessor. Boosting assigns weights to training instances, giving more importance to misclassified instances. AdaBoost and Gradient Boosting are common boosting algorithms.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the famous Iris dataset to demonstrate the application of an ensemble method, specifically the Random Forest algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, a Random Forest classifier is trained on the Iris dataset. The classifier is an ensemble of decision trees, where each tree is trained on a different subset of the data. The final prediction is obtained by aggregating the individual tree predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.2. Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging, short for Bootstrap Aggregating, is an ensemble learning technique that involves training multiple instances of the same model on different subsets of the training data. The primary idea behind bagging is to introduce diversity among individual models by creating these subsets through bootstrapping, a sampling technique where instances are randomly selected with replacement.\n",
    "\n",
    "#### The key steps in bagging are as follows:\n",
    "\n",
    "1. Bootstrap Sampling:\n",
    "    Randomly select subsets of the training data with replacement (bootstrapping) to create multiple training datasets.\n",
    "\n",
    "2. Model Training:\n",
    "    Train a base model (e.g., decision tree, neural network) independently on each bootstrap sample.\n",
    "\n",
    "3. Aggregation:\n",
    "    Combine the predictions of individual models through averaging (for regression) or voting (for classification) to obtain the final ensemble prediction.\n",
    "\n",
    "4. Diversity:\n",
    "    The diversity among models comes from the different subsets of data used for training, which helps improve the model's generalization performance.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Breast Cancer Wisconsin dataset to demonstrate the application of bagging with the Random Forest algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Breast Cancer Wisconsin dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier with 100 trees\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, a Random Forest classifier, a bagging algorithm, is trained on the Breast Cancer Wisconsin dataset. The classifier consists of an ensemble of decision trees, each trained on a different subset of the data created through bootstrapping. The final prediction is obtained by aggregating the individual tree predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.3. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is an ensemble learning technique that builds a sequence of weak learners (models that perform slightly better than random chance) sequentially. Each model in the sequence focuses on correcting the errors of its predecessor. The key idea behind boosting is to assign weights to training instances, giving more importance to instances that were misclassified by the previous models.\n",
    "\n",
    "#### The primary steps in boosting are as follows:\n",
    "\n",
    "1. Weight Assignment:\n",
    "    Assign equal weights to all training instances initially.\n",
    "\n",
    "2. Model Training:\n",
    "    Train a weak learner (e.g., decision tree, shallow neural network) on the training data with the assigned weights.\n",
    "\n",
    "3. Prediction and Error Calculation:\n",
    "    Make predictions on the training data and calculate the errors.\n",
    "\n",
    "4. Instance Weight Update:\n",
    "    Increase the weights of misclassified instances, making them more influential in the next model.\n",
    "\n",
    "5. Repeat:\n",
    "    Repeat steps 2-4 for a predefined number of iterations or until a performance threshold is reached.\n",
    "\n",
    "6. Final Prediction:\n",
    "    Combine the predictions of all models with each model's weight, often using a weighted sum, to obtain the final ensemble prediction.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the famous Iris dataset to demonstrate the application of a boosting algorithm, specifically AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an AdaBoost classifier with 50 weak learners (Decision Trees)\n",
    "adaboost_classifier = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = adaboost_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, an AdaBoost classifier is trained on the Iris dataset. AdaBoost sequentially trains multiple weak learners (Decision Trees by default) and assigns higher weights to instances that are misclassified by previous models. The final prediction is a weighted sum of the individual model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.4. Random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble learning technique that combines the power of bagging with decision trees to create a robust and accurate predictive model. It builds multiple decision trees during training and merges their predictions to obtain a more stable and reliable result. The \"random\" in Random Forest comes from the introduction of randomness at two levels: in the selection of data samples used for training each tree (bootstrap sampling) and in the selection of features considered at each split of the trees.\n",
    "\n",
    "#### The key characteristics of Random Forest are:\n",
    "\n",
    "1. Bootstrap Sampling:\n",
    "    For each tree, a random sample of the training data is selected with replacement. This process is known as bootstrap sampling.\n",
    "\n",
    "2. Random Feature Selection:\n",
    "    At each node of the decision tree, a random subset of features is considered for splitting. This introduces diversity among the trees and helps prevent overfitting.\n",
    "\n",
    "3. Aggregation:\n",
    "    The predictions of individual trees are combined through averaging (for regression) or voting (for classification) to obtain the final ensemble prediction.\n",
    "\n",
    "4. Robustness:\n",
    "    Random Forest is less prone to overfitting and is generally more robust compared to individual decision trees.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Breast Cancer Wisconsin dataset to demonstrate the application of Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Breast Cancer Wisconsin dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier with 100 trees\n",
    "random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "random_forest_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = random_forest_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, a Random Forest classifier is trained on the Breast Cancer Wisconsin dataset. The classifier consists of an ensemble of decision trees, each trained on a different subset of the data created through bootstrap sampling. The final prediction is obtained by aggregating the individual tree predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.5. Improving classification accuracy of class-imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class-imbalanced datasets are characterized by a significant difference in the number of instances between different classes. In such scenarios, standard machine learning models might be biased towards the majority class, leading to suboptimal performance on the minority class. Improving classification accuracy on imbalanced data involves addressing the imbalance to ensure that the model generalizes well across all classes.\n",
    "Techniques to Improve Classification Accuracy on Imbalanced Data:\n",
    "\n",
    "1. Resampling:\n",
    "    Over-sampling: Increase the number of instances in the minority class by replicating or generating synthetic examples.\n",
    "    Under-sampling: Decrease the number of instances in the majority class by randomly removing examples.\n",
    "\n",
    "2. Weighted Loss Functions:\n",
    "    Assign different weights to classes in the loss function during model training. This gives more importance to the minority class.\n",
    "\n",
    "3. Ensemble Methods:\n",
    "    Utilize ensemble methods like Random Forest or AdaBoost, which can handle class imbalances better than individual models.\n",
    "\n",
    "4. Anomaly Detection:\n",
    "    Treat the minority class as an anomaly and use anomaly detection techniques to identify instances of the minority class.\n",
    "\n",
    "5. Cost-sensitive Learning:\n",
    "    Introduce misclassification costs to make the model penalize errors in the minority class more than in the majority class.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the famous Iris dataset where we artificially introduce class imbalance. We'll use the Synthetic Minority Over-sampling Technique (SMOTE) for over-sampling the minority class and a Random Forest classifier for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Introduce class imbalance (assuming class 0 is the minority class)\n",
    "y[y == 0] = 1\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE for over-sampling the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create a Random Forest classifier with 100 trees\n",
    "random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the resampled training data\n",
    "random_forest_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = random_forest_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we introduce class imbalance in the Iris dataset and apply SMOTE for over-sampling the minority class. Then, we train a Random Forest classifier on the resampled data and evaluate its performance on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "breast-cancer-detection-association-rule-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
