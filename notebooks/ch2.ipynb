{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Author: SMB H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Nominal attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nominal Attributes:\n",
    "\n",
    "Nominal attributes are categorical variables that represent distinct categories or labels with no inherent order or ranking among them. These attributes classify data into various groups based on qualitative differences. Examples include colors, gender, types of animals, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Python Code for Handling Nominal Attributes:\n",
    "\n",
    "Let's create a simple Python code snippet using the pandas library to work with a dataset containing nominal attributes. We'll load a sample dataset, explore nominal attributes, and perform one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset with nominal attributes\n",
    "data = {\n",
    "    'Animal': ['Dog', 'Cat', 'Fish', 'Bird', 'Snake'],\n",
    "    'Color': ['Brown', 'Black', 'Gold', 'Blue', 'Green'],\n",
    "    'Habitat': ['Forest', 'Home', 'Aquarium', 'Sky', 'Jungle']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Explore the nominal attributes\n",
    "nominal_attributes = ['Animal', 'Color', 'Habitat']\n",
    "\n",
    "# Display the unique values in each nominal attribute\n",
    "for attribute in nominal_attributes:\n",
    "    print(f\"Unique values in {attribute}: {df[attribute].unique()}\")\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df_encoded = pd.get_dummies(df, columns=nominal_attributes)\n",
    "\n",
    "# Display the dataset after one-hot encoding\n",
    "print(\"\\nDataset after One-Hot Encoding:\")\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "    We create a pandas DataFrame with three nominal attributes: 'Animal', 'Color', and 'Habitat'.\n",
    "    We explore the unique values in each nominal attribute.\n",
    "    We use pd.get_dummies() to perform one-hot encoding, converting each nominal attribute into binary columns.\n",
    "    The resulting DataFrame (df_encoded) is displayed after one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Binary attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Attributes:\n",
    "\n",
    "Binary attributes are categorical variables that can take on one of two possible values, typically representing the presence or absence of a certain characteristic. These attributes are fundamental in many datasets, and examples include Yes/No, True/False, 1/0, or any other two distinct categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Python Code for Handling Binary Attributes:\n",
    "\n",
    "Let's create a Python code snippet using the pandas library to work with a dataset containing binary attributes. We'll load a sample dataset, explore binary attributes, and perform basic operations on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset with binary attributes\n",
    "data = {\n",
    "    'StudentID': [1, 2, 3, 4, 5],\n",
    "    'PassedExam': [1, 0, 1, 1, 0],\n",
    "    'EnrolledInCourse': [1, 1, 0, 1, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Explore the binary attributes\n",
    "binary_attributes = ['PassedExam', 'EnrolledInCourse']\n",
    "\n",
    "# Display the count of each unique value in binary attributes\n",
    "for attribute in binary_attributes:\n",
    "    print(f\"Counts for {attribute}:\\n{df[attribute].value_counts()}\\n\")\n",
    "\n",
    "# Perform basic operations on binary attributes\n",
    "df['TotalAttributes'] = df['PassedExam'] + df['EnrolledInCourse']\n",
    "\n",
    "# Display the dataset after the operation\n",
    "print(\"Dataset after performing an operation on binary attributes:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "    We create a pandas DataFrame with two binary attributes: 'PassedExam' and 'EnrolledInCourse'.\n",
    "    We explore the counts of each unique value in binary attributes using value_counts().\n",
    "    We perform a basic operation (addition) on binary attributes to create a new attribute, 'TotalAttributes'.\n",
    "    The resulting DataFrame (df) is displayed after these operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Ordinal attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinal Attributes:\n",
    "\n",
    "Ordinal attributes are categorical variables with a meaningful order or ranking among the categories. Unlike nominal attributes, ordinal attributes have a clear, meaningful sequence, but the intervals between them are not necessarily uniform or well-defined. Examples of ordinal attributes include education levels (e.g., elementary, high school, college), customer satisfaction ratings, or socioeconomic classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Python Code for Handling Ordinal Attributes:\n",
    "\n",
    "Let's create a Python code snippet using the pandas library to work with a dataset containing ordinal attributes. We'll load a sample dataset, explore ordinal attributes, and demonstrate how to encode them to preserve the ordinal relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset with ordinal attributes\n",
    "data = {\n",
    "    'StudentID': [1, 2, 3, 4, 5],\n",
    "    'EducationLevel': ['High School', 'College', 'Elementary', 'College', 'High School'],\n",
    "    'SatisfactionRating': [3, 5, 2, 4, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Explore the ordinal attributes\n",
    "ordinal_attributes = ['EducationLevel', 'SatisfactionRating']\n",
    "\n",
    "# Display the unique values in each ordinal attribute\n",
    "for attribute in ordinal_attributes:\n",
    "    print(f\"Unique values in {attribute}: {df[attribute].unique()}\")\n",
    "\n",
    "# Encode ordinal attributes with meaningful numerical values\n",
    "education_level_mapping = {'Elementary': 1, 'High School': 2, 'College': 3}\n",
    "df['EducationLevelEncoded'] = df['EducationLevel'].map(education_level_mapping)\n",
    "\n",
    "# Display the dataset after encoding ordinal attributes\n",
    "print(\"\\nDataset after encoding ordinal attributes:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "    We create a pandas DataFrame with two ordinal attributes: 'EducationLevel' and 'SatisfactionRating'.\n",
    "    We explore the unique values in each ordinal attribute.\n",
    "    We encode ordinal attributes with meaningful numerical values using the map() function to create a new attribute, 'EducationLevelEncoded'.\n",
    "    The resulting DataFrame (df) is displayed after these operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4. Numeric attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric Attributes:\n",
    "\n",
    "Numeric attributes represent quantities and can take on numerical values. There are two main types of numeric attributes: discrete and continuous. Discrete numeric attributes can only take on distinct, separate values (e.g., the number of bedrooms in a house), while continuous numeric attributes can take on any value within a range (e.g., height, weight)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Python Code for Handling Numeric Attributes:\n",
    "\n",
    "Let's create a Python code snippet using the pandas library to work with a dataset containing numeric attributes. We'll load a sample dataset, explore numeric attributes, and perform basic operations on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset with numeric attributes\n",
    "data = {\n",
    "    'StudentID': [1, 2, 3, 4, 5],\n",
    "    'Age': [21, 19, 22, 20, 23],\n",
    "    'Height (cm)': [175, 160, 180, 165, 185],\n",
    "    'Score': [85, 92, 78, 89, 95]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Explore the numeric attributes\n",
    "numeric_attributes = ['Age', 'Height (cm)', 'Score']\n",
    "\n",
    "# Display summary statistics for numeric attributes\n",
    "print(\"Summary Statistics for Numeric Attributes:\")\n",
    "print(df[numeric_attributes].describe())\n",
    "\n",
    "# Perform basic operations on numeric attributes\n",
    "df['NormalizedScore'] = (df['Score'] - df['Score'].mean()) / df['Score'].std()\n",
    "\n",
    "# Display the dataset after the operation\n",
    "print(\"\\nDataset after performing an operation on numeric attributes:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "    We create a pandas DataFrame with three numeric attributes: 'Age', 'Height (cm)', and 'Score'.\n",
    "    We explore summary statistics for numeric attributes using describe().\n",
    "    We perform a basic operation (normalization) on the 'Score' attribute to create a new attribute, 'NormalizedScore'.\n",
    "    The resulting DataFrame (df) is displayed after these operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5. Discrete vs. continuous attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discrete Attributes:\n",
    "\n",
    "Definition: Discrete attributes can only take on distinct, separate values.\n",
    "Examples: The number of bedrooms in a house, the count of items in a shopping cart, the number of students in a class.\n",
    "Nature: These attributes are often counted in whole numbers and have clear boundaries between values.\n",
    "\n",
    "#### Continuous Attributes:\n",
    "\n",
    "Definition: Continuous attributes can take on any value within a range.\n",
    "Examples: Height, weight, temperature, and any measurement that can have decimal values.\n",
    "Nature: These attributes have a continuous and infinite set of possible values, making them suitable for measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Python Code for Discrete vs. Continuous Attributes:\n",
    "\n",
    "Let's create a Python code snippet using the pandas library to work with a dataset containing both discrete and continuous attributes. We'll load a sample dataset, explore the nature of each type, and perform basic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset with discrete and continuous attributes\n",
    "data = {\n",
    "    'StudentID': [1, 2, 3, 4, 5],\n",
    "    'NumCourses': [4, 5, 3, 6, 4],  # Discrete attribute (number of courses)\n",
    "    'GPA': [3.5, 4.0, 3.2, 3.8, 3.9],  # Continuous attribute (GPA)\n",
    "    'Income': [25000, 30000, 20000, 35000, 32000]  # Continuous attribute (income in dollars)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Explore the nature of attributes\n",
    "print(\"Nature of Attributes:\")\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'int':\n",
    "        print(f\"{column} is a discrete attribute.\")\n",
    "    elif df[column].dtype == 'float':\n",
    "        print(f\"{column} is a continuous attribute.\")\n",
    "\n",
    "# Perform basic operations on continuous attributes\n",
    "df['ScaledIncome'] = df['Income'] / 1000  # Scale income for readability\n",
    "\n",
    "# Display the dataset after the operation\n",
    "print(\"\\nDataset after performing an operation on continuous attributes:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "    We create a pandas DataFrame with three attributes: 'NumCourses' (discrete), 'GPA' (continuous), and 'Income' (continuous).\n",
    "    We explore the nature of each attribute based on its data type.\n",
    "    We perform a basic operation (scaling) on a continuous attribute ('Income') to create a new attribute, 'ScaledIncome'.\n",
    "    The resulting DataFrame (df) is displayed after these operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Measuring the central tendency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring central tendency is a way to summarize a set of data by identifying the central or average value. There are three common measures of central tendency: mean, median, and mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean (Average): \n",
    "It is calculated by summing up all the values in a dataset and dividing the sum by the number of values.\n",
    "\n",
    "#### Median: \n",
    "The median is the middle value of a dataset when it is sorted in ascending or descending order. If the dataset has an even number of values, the median is the average of the two middle values.\n",
    "\n",
    "#### Mode: \n",
    "The mode is the value that appears most frequently in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Python Code for Measuring the central tendency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "data = np.array([15, 18, 2, 36, 12, 25, 18, 40, 28, 22])\n",
    "\n",
    "# Mean\n",
    "mean_value = np.mean(data)\n",
    "print(f\"Mean: {mean_value}\")\n",
    "\n",
    "# Median\n",
    "median_value = np.median(data)\n",
    "print(f\"Median: {median_value}\")\n",
    "\n",
    "# Mode\n",
    "mode_value = np.argmax(np.bincount(data))\n",
    "print(f\"Mode: {mode_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Make sure to install NumPy if you haven't already by running pip install numpy in your Python environment.\n",
    "\n",
    "    This code uses NumPy functions to calculate the mean, median, and mode of the given dataset. You can replace the data array with your own dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Measuring the dispersion of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring the dispersion of data is important in understanding how spread out or clustered the values in a dataset are. There are several measures of dispersion, including range, variance, and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Range: \n",
    "The range is the difference between the maximum and minimum values in a dataset. It provides a simple measure of how spread out the values are.\n",
    "\n",
    "#### Variance: \n",
    "Variance measures the average squared difference of each value from the mean of the dataset. A higher variance indicates greater dispersion.\n",
    "\n",
    "#### Standard Deviation: \n",
    "The standard deviation is the square root of the variance. It provides a more interpretable measure of the spread of data, as it is in the same unit as the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Python Code for Measuring the dispersion of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "data = np.array([15, 18, 2, 36, 12, 25, 18, 40, 28, 22])\n",
    "\n",
    "# Range\n",
    "data_range = np.ptp(data)\n",
    "print(f\"Range: {data_range}\")\n",
    "\n",
    "# Variance\n",
    "data_variance = np.var(data)\n",
    "print(f\"Variance: {data_variance}\")\n",
    "\n",
    "# Standard Deviation\n",
    "data_stddev = np.std(data)\n",
    "print(f\"Standard Deviation: {data_stddev}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this code, np.ptp calculates the range, np.var calculates the variance, and np.std calculates the standard deviation. Replace the data array with your own dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Covariance and correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance and correlation analysis are statistical measures that describe the degree to which two variables change together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance:\n",
    "\n",
    "Covariance measures the extent to which the values of two variables change in relation to each other.\n",
    "A positive covariance indicates that as one variable increases, the other variable tends to increase as well, and vice versa for negative covariance.\n",
    "However, the scale of covariance is not standardized, making it challenging to interpret the strength of the relationship.\n",
    "\n",
    "#### Correlation:\n",
    "\n",
    "Correlation is a standardized measure of the strength and direction of the linear relationship between two variables.\n",
    "The correlation coefficient ranges from -1 to 1.\n",
    "A correlation coefficient of 1 indicates a perfect positive correlation, -1 indicates a perfect negative correlation, and 0 indicates no linear correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Python Code for Covariance and correlation analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "data = {\n",
    "    'Variable1': [15, 18, 2, 36, 12, 25, 18, 40, 28, 22],\n",
    "    'Variable2': [10, 12, 5, 30, 8, 20, 15, 35, 25, 18]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Covariance matrix\n",
    "cov_matrix = np.cov(df, rowvar=False)\n",
    "print(f\"Covariance Matrix:\\n{cov_matrix}\")\n",
    "\n",
    "# Correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "print(f\"Correlation Matrix:\\n{correlation_matrix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This code calculates the covariance matrix using np.cov and the correlation matrix using the corr method of a Pandas DataFrame. Replace the Variable1 and Variable2 columns with your own variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4. Graphic displays of basic statistics of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphic displays of basic statistics are essential for visualizing the characteristics of a dataset. These displays can help in gaining insights into the distribution, central tendency, and dispersion of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms:\n",
    "\n",
    "Explanation: Histograms provide a visual representation of the distribution of a dataset by dividing it into bins and displaying the frequency or probability of values falling into each bin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box Plots:\n",
    "\n",
    "Explanation: Box plots (box-and-whisker plots) provide a graphical summary of the distribution of a dataset, including the median, quartiles, and potential outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter Plots:\n",
    "\n",
    "Explanation: Scatter plots display individual data points in a two-dimensional space, making it easy to identify patterns, relationships, and outliers between two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Python Code for Graphic displays of basic statistics of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "data = np.random.randn(1000)  # Replace with your own dataset\n",
    "\n",
    "# Create a histogram\n",
    "plt.hist(data, bins=20, color='blue', alpha=0.7)\n",
    "plt.title('Histogram of the Dataset')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "data = np.random.randn(1000)  # Replace with your own dataset\n",
    "\n",
    "# Create a box plot\n",
    "plt.boxplot(data, vert=False)\n",
    "plt.title('Box Plot of the Dataset')\n",
    "plt.xlabel('Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example datasets\n",
    "x = np.random.randn(100)\n",
    "y = 2 * x + np.random.randn(100)  # Replace with your own datasets\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(x, y, color='red', alpha=0.7)\n",
    "plt.title('Scatter Plot of Two Variables')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Histograms, box plots, and scatter plots can enhance the understanding of data distributions, relationships, and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Data matrix vs. dissimilarity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data mining, understanding the concepts of data matrix and dissimilarity matrix is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Matrix:\n",
    "\n",
    "Explanation: A data matrix is a structured representation of a dataset, where rows correspond to individual observations or instances, and columns represent different attributes or features. Each cell in the matrix contains the value of a specific attribute for a particular instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dissimilarity Matrix:\n",
    "\n",
    "Explanation: A dissimilarity matrix represents the dissimilarity or similarity between pairs of instances in a dataset. It quantifies how different or similar two instances are based on some measure of dissimilarity, such as distance or dissimilarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Matrix\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example data matrix\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4],\n",
    "    'Feature1': [10, 15, 20, 25],\n",
    "    'Feature2': [0.5, 1.2, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Data Matrix:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissimilarity Matrix\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import pandas as pd\n",
    "\n",
    "# Example data matrix\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4],\n",
    "    'Feature1': [10, 15, 20, 25],\n",
    "    'Feature2': [0.5, 1.2, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate Euclidean distance matrix\n",
    "dissimilarity_matrix = squareform(pdist(df[['Feature1', 'Feature2']], metric='euclidean'))\n",
    "print(\"Dissimilarity Matrix:\")\n",
    "print(pd.DataFrame(dissimilarity_matrix, index=df['ID'], columns=df['ID']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the pdist function from SciPy calculates the pairwise Euclidean distances between instances based on selected features. The squareform function converts the condensed distance matrix to a square form.\n",
    "\n",
    "    Matrices provide a structured representation of raw data, while dissimilarity matrices capture the pairwise dissimilarities between instances, which is useful in various data mining applications like clustering, classification, and similarity search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Proximity measures for nominal attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proximity measures for nominal attributes are used to quantify the similarity or dissimilarity between instances when dealing with categorical or nominal attributes. Unlike numerical attributes, nominal attributes don't have a natural ordering, making traditional distance metrics unsuitable. Proximity measures address this issue by focusing on the agreement or disagreement between categorical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Proximity Measure: Jaccard Similarity\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "Jaccard Similarity measures the similarity between two sets by dividing the size of their intersection by the size of their union. In the context of nominal attributes, each attribute value is treated as a set of binary values (1 if the attribute is present, 0 otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "# Example dataset with nominal attributes\n",
    "data = {\n",
    "    'Instance1': ['Red', 'Square', 'Large'],\n",
    "    'Instance2': ['Blue', 'Circle', 'Small']\n",
    "}\n",
    "\n",
    "# Convert nominal attributes to binary values\n",
    "binary_data = pd.get_dummies(pd.DataFrame(data))\n",
    "binary_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Jaccard Similarity\n",
    "jaccard_similarity = jaccard_score(binary_data.iloc[0], binary_data.iloc[1])\n",
    "jaccard_similarity_1 = jaccard_score(binary_data.iloc[0], binary_data.iloc[0])\n",
    "print(f\"Jaccard Similarity: {jaccard_similarity}\")\n",
    "print(f\"Jaccard Similarity: {jaccard_similarity_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the nominal attributes ('Red', 'Square', 'Large') and ('Blue', 'Circle', 'Small') are converted into binary vectors using one-hot encoding. The Jaccard Similarity is then calculated using the jaccard_score function from scikit-learn.\n",
    "\n",
    "    Proximity measures for nominal attributes are essential for clustering, classification, and other data mining tasks involving categorical data. The practical example illustrates the application of Jaccard Similarity to quantify the similarity between instances with nominal attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3. Proximity measures for binary attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proximity measures for binary attributes are employed when dealing with datasets that consist of binary (0/1) attributes. These measures assess the similarity or dissimilarity between instances based on the presence or absence of specific binary features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Proximity Measure: Hamming Distance\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "Hamming Distance measures the dissimilarity between two binary strings by counting the number of positions at which the corresponding bits are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import hamming\n",
    "\n",
    "# Example dataset with binary attributes\n",
    "data = {\n",
    "    'Instance1': [1, 0, 1, 0],\n",
    "    'Instance2': [0, 1, 1, 0]\n",
    "}\n",
    "\n",
    "# Convert binary attributes to NumPy arrays\n",
    "binary_data = np.array([data['Instance1'], data['Instance2']])\n",
    "\n",
    "# Calculate Hamming Distance\n",
    "hamming_distance = hamming(binary_data[0], binary_data[1])\n",
    "print(f\"Hamming Distance: {hamming_distance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, instances are represented by binary vectors [1, 0, 1, 0] and [0, 1, 1, 0]. The Hamming Distance between these vectors is computed using the hamming function from SciPy.\n",
    "\n",
    "    Proximity measures for binary attributes are crucial for tasks such as clustering, pattern recognition, and classification when dealing with datasets consisting of binary features. The Hamming Distance example demonstrates how to quantify dissimilarity between instances based on binary attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4. Dissimilarity of numeric data: Minkowski distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Minkowski distance is a dissimilarity measure used to assess the similarity or dissimilarity between two points in a multidimensional space. It is a generalization of other distance measures, including Euclidean distance and Manhattan distance, and is defined by the following formula:\n",
    "\n",
    "D(x,y)=(∑i=1n∣xi−yi∣p)1p\n",
    "\n",
    "where x and y are vectors representing the numeric data points, n is the number of dimensions, and p is a parameter that determines the order of the Minkowski distance. When p=1, the Minkowski distance is equivalent to the Manhattan distance, and when p=2, it is equivalent to the Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import minkowski\n",
    "\n",
    "# Example dataset with numeric attributes\n",
    "data1 = [2, 3, 5, 7]\n",
    "data2 = [1, 4, 6, 8]\n",
    "\n",
    "# Calculate Minkowski distance with p=2 (Euclidean distance)\n",
    "minkowski_distance = minkowski(data1, data2, p=2)\n",
    "print(f\"Minkowski Distance (p=2): {minkowski_distance}\")\n",
    "\n",
    "# Calculate Minkowski distance with p=1 (Manhattan distance)\n",
    "manhattan_distance = minkowski(data1, data2, p=1)\n",
    "print(f\"Minkowski Distance (p=1): {manhattan_distance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the Minkowski distance is calculated between two numeric vectors [2, 3, 5, 7] and [1, 4, 6, 8] using both the Euclidean distance (p=2) and the Manhattan distance (p=1). The minkowski function from SciPy is used for the calculation.\n",
    "\n",
    "    The Minkowski distance is a flexible measure that adapts to different scenarios based on the chosen value of pp. It is applicable in various data mining tasks such as clustering, classification, and outlier detection, especially when dealing with datasets containing numeric attributes. The"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5. Proximity measures for ordinal attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proximity measures for ordinal attributes are used when dealing with datasets that contain attributes with an inherent order or ranking. Unlike nominal attributes, ordinal attributes have a meaningful order, but the intervals between values may not be uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Proximity Measure: Spearman Rank Correlation Coefficient\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "The Spearman Rank Correlation Coefficient measures the strength and direction of the monotonic relationship between two ordinal variables. It is based on the ranks of the values rather than their actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Example dataset with ordinal attributes\n",
    "data = {\n",
    "    'Instance1': [2, 3, 1, 4],\n",
    "    'Instance2': [1, 4, 2, 3]\n",
    "}\n",
    "\n",
    "# Calculate Spearman Rank Correlation Coefficient\n",
    "spearman_corr, _ = spearmanr(data['Instance1'], data['Instance2'])\n",
    "print(f\"Spearman Rank Correlation Coefficient: {spearman_corr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, instances are represented by ordinal attributes [2, 3, 1, 4] and [1, 4, 2, 3]. The Spearman Rank Correlation Coefficient is calculated using the spearmanr function from SciPy.\n",
    "\n",
    "    Proximity measures for ordinal attributes are important when dealing with data that has a meaningful order but lacks a clear numerical scale. The Spearman Rank Correlation Coefficient is particularly useful for assessing the monotonic relationship between ordinal variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6. Dissimilarity for attributes of mixed types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling dissimilarity for attributes of mixed types is a common challenge in data mining, as datasets often include a combination of numeric, categorical, and ordinal attributes. It's crucial to employ appropriate dissimilarity measures that can accommodate the diverse nature of these attribute types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Dissimilarity Measure: Gower's Distance\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "Gower's Distance is a dissimilarity measure designed for datasets with mixed types of attributes. It calculates the dissimilarity between two instances by considering the attribute types and applying appropriate measures, such as Euclidean distance for numeric attributes, Jaccard similarity for binary attributes, and simple matching coefficient for nominal attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example dataset with mixed attribute types\n",
    "data = {\n",
    "    'Numeric': [2.5, 1.8, 3.2, 4.0],\n",
    "    'Binary': [1, 0, 1, 1],\n",
    "    'Nominal': ['A', 'B', 'A', 'C'],\n",
    "    'Ordinal': [2, 1, 3, 2]\n",
    "}\n",
    "\n",
    "# Standardize numeric attributes\n",
    "numeric_data = np.array([data['Numeric']]).T\n",
    "numeric_data = StandardScaler().fit_transform(numeric_data)\n",
    "\n",
    "# Convert binary attributes to binary values\n",
    "binary_data = pd.get_dummies(pd.DataFrame(data['Binary'], columns=['Binary']))\n",
    "\n",
    "# Convert nominal attributes to binary values\n",
    "nominal_data = pd.get_dummies(pd.DataFrame(data['Nominal'], columns=['Nominal']))\n",
    "\n",
    "# Combine all attribute types\n",
    "mixed_data = np.concatenate((numeric_data, binary_data, nominal_data, np.array([data['Ordinal']]).T), axis=1)\n",
    "\n",
    "# Calculate Gower's Distance\n",
    "gower_distance = squareform(pdist(mixed_data, metric='euclidean'))\n",
    "print(\"Gower's Distance Matrix:\")\n",
    "print(gower_distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, Gower's Distance is calculated for a dataset with numeric, binary, nominal, and ordinal attributes. Numeric attributes are standardized, binary and nominal attributes are converted to binary values, and all attribute types are combined into a single matrix. The pdist function from SciPy is then used to calculate pairwise Euclidean distances, and the squareform function converts the condensed distance matrix to a square form.\n",
    "\n",
    "    The Gower's Distance is a versatile measure for dissimilarity in datasets with mixed attribute types, providing a comprehensive solution for handling various data characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.7. Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity is a metric used to measure the similarity between two non-zero vectors of an inner product space. In the context of data mining, it is often employed to assess the similarity between documents in natural language processing or to compare feature vectors in recommendation systems. The cosine similarity ranges from -1 to 1, where 1 indicates perfect similarity, 0 indicates no similarity, and -1 indicates perfect dissimilarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example dataset with text documents\n",
    "documents = [\n",
    "    \"Data mining is an exciting field of study.\",\n",
    "    \"Machine learning techniques are used for data analysis.\",\n",
    "    \"Python programming is widely used in data science applications.\"\n",
    "]\n",
    "\n",
    "# Convert documents to a bag-of-words representation\n",
    "vectorizer = CountVectorizer()\n",
    "document_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarities = cosine_similarity(document_matrix, document_matrix)\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(cosine_similarities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the CountVectorizer from scikit-learn is used to convert a collection of text documents into a matrix of token counts (bag-of-words representation). The cosine_similarity function then computes the cosine similarity between the document vectors.\n",
    "\n",
    "    Cosine similarity is particularly useful in scenarios where the magnitude of the vectors is not crucial, and the focus is on the direction of the vectors. It is widely applied in text analysis, document retrieval, and collaborative filtering systems. The provided example demonstrates how to calculate cosine similarity for a set of text documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.8. Measuring similar distributions: the Kullback-Leibler divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kullback-Leibler (KL) Divergence is a measure of how one probability distribution diverges from a second, expected probability distribution. In the context of data mining, it is often used to quantify the difference or similarity between two probability distributions. KL Divergence is not a true distance metric as it is not symmetric and does not satisfy the triangle inequality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Example probability distributions\n",
    "distribution1 = np.array([0.2, 0.3, 0.5])\n",
    "distribution2 = np.array([0.1, 0.6, 0.3])\n",
    "\n",
    "# Calculate Kullback-Leibler Divergence\n",
    "kl_divergence = entropy(distribution1, distribution2)\n",
    "print(f\"Kullback-Leibler Divergence: {kl_divergence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, two probability distributions are represented by arrays distribution1 and distribution2. The entropy function from SciPy calculates the KL Divergence between these distributions.\n",
    "\n",
    "    KL Divergence is often used in information theory and statistics to measure the difference between two probability distributions. It finds applications in various areas, including natural language processing, machine learning, and pattern recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.9. Capturing hidden semantics in similarity measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturing hidden semantics in similarity measures involves finding meaningful patterns, relationships, or similarities in data that may not be apparent at first glance. This process often requires more advanced techniques that go beyond simple distance or similarity metrics. Methods like word embeddings, semantic similarity, or advanced neural network models are employed to capture and leverage hidden semantics in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example dataset with text documents\n",
    "documents = [\n",
    "    \"Data mining is an exciting field of study.\",\n",
    "    \"Machine learning techniques are used for data analysis.\",\n",
    "    \"Python programming is widely used in data science applications.\"\n",
    "]\n",
    "\n",
    "# Tokenize the documents into words\n",
    "tokenized_documents = [doc.split() for doc in documents]\n",
    "\n",
    "# Train a Word2Vec model on the tokenized documents\n",
    "model = Word2Vec(tokenized_documents, vector_size=10, window=3, min_count=1, workers=4)\n",
    "\n",
    "# Calculate cosine similarity using word embeddings\n",
    "embedding_similarity = cosine_similarity([model.wv['data']], [model.wv['mining']])\n",
    "print(f\"Cosine Similarity using Word Embeddings: {embedding_similarity[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the Gensim library is used to train a Word2Vec model on a small dataset of text documents. Word embeddings capture semantic relationships between words, and cosine similarity is then used to measure the similarity between the word vectors of two words ('data' and 'mining').\n",
    "\n",
    "    Capturing hidden semantics often involves leveraging advanced techniques such as word embeddings, neural networks, or deep learning models. These methods are particularly useful in applications like natural language processing, recommendation systems, and image analysis where capturing the underlying meaning or semantics is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1. Data quality measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data quality measures are used to assess the reliability and accuracy of a dataset. Ensuring high-quality data is essential for successful data mining and analysis. Various metrics and techniques are employed to evaluate different aspects of data quality, including completeness, accuracy, consistency, and timeliness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Data Quality Measure: Missing Values\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "Missing values are a common issue in datasets and can impact the quality of analysis. One measure of data quality is assessing the percentage of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dataset with missing values\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4, 5],\n",
    "    'Feature1': [10, 15, None, 25, 20],\n",
    "    'Feature2': [5, None, 8, 12, 10],\n",
    "    'Feature3': [2, 4, 6, None, 8]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate percentage of missing values in each column\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "print(\"Percentage of Missing Values:\")\n",
    "print(missing_percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the dataset contains missing values represented as None. The code calculates the percentage of missing values in each column using the isnull() function and the sum() function in pandas.\n",
    "\n",
    "    Assessing data quality is crucial for making informed decisions during the data mining process. The example focuses on missing values, but other data quality measures could include checking for outliers, ensuring consistency in categorical values, and validating data against predefined business rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning is a crucial step in the data preprocessing phase, aiming to identify and rectify errors, inconsistencies, and inaccuracies in the dataset. It involves tasks such as handling missing values, correcting data types, removing duplicates, and addressing outliers. Ensuring clean and reliable data is essential for obtaining meaningful and accurate results in data mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Data Cleaning Task: Handling Missing Values\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "One common data cleaning task is handling missing values. Depending on the nature of the missing data, strategies such as imputation or removal may be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dataset with missing values\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4, 5],\n",
    "    'Feature1': [10, 15, None, 25, 20],\n",
    "    'Feature2': [5, None, 8, 12, 10],\n",
    "    'Feature3': [2, 4, 6, None, 8]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Handling missing values by imputation (using mean)\n",
    "df_filled = df.fillna(df.mean())\n",
    "\n",
    "# Display the cleaned dataset\n",
    "print(\"Cleaned Dataset:\")\n",
    "print(df_filled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the dataset contains missing values represented as None. The code uses the fillna function in pandas to replace missing values with the mean value of each column.\n",
    "\n",
    "    Data cleaning is a critical step to ensure the quality and reliability of the dataset. While the example focuses on handling missing values, other data cleaning tasks might include removing duplicate records, correcting data types, and dealing with outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3. Data integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data integration is the process of combining data from different sources into a unified and coherent view. It involves handling inconsistencies, resolving conflicts, and creating a merged dataset that can be used for analysis and mining. Data integration is crucial when working with diverse datasets that may come from various databases, files, or platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Data Integration Task: Merging Datasets\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "A common data integration task is merging datasets with a common key or identifier. This helps create a consolidated dataset that includes information from multiple sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example datasets with a common key\n",
    "data1 = {'ID': [1, 2, 3], 'Name': ['Alice', 'Bob', 'Charlie']}\n",
    "data2 = {'ID': [2, 3, 4], 'Age': [25, 30, 22]}\n",
    "\n",
    "df1 = pd.DataFrame(data1)\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Merge datasets on the common key 'ID'\n",
    "merged_df = pd.merge(df1, df2, on='ID', how='outer')\n",
    "\n",
    "# Display the merged dataset\n",
    "print(\"Merged Dataset:\")\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, two datasets (df1 and df2) share a common key ('ID'). The merge function in pandas is used to merge these datasets based on the 'ID' column.\n",
    "\n",
    "    Data integration is essential for creating a comprehensive view of the information and relationships across different datasets. While the example focuses on merging datasets, other data integration tasks might include handling schema mismatches, resolving naming conflicts, and dealing with data from various sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is a data preprocessing technique used to scale and transform numerical features in a dataset to a common scale. This process is essential when the features have different units or scales, as it ensures that each feature contributes proportionally to the analysis. Normalization typically involves transforming the data to a standard scale, such as between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Normalization in Python: Min-Max Scaling\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "Min-Max Scaling is a common normalization method that transforms the values of a feature to a specific range, usually between 0 and 1. The formula for Min-Max Scaling is: Xnormalized=X−Xmin/Xmax−Xmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example dataset with numerical features\n",
    "data = {\n",
    "    'Feature1': [10, 20, 15, 25, 30],\n",
    "    'Feature2': [500, 1000, 750, 1250, 1500]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Min-Max Scaling to normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(df)\n",
    "\n",
    "# Display the normalized dataset\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=df.columns)\n",
    "print(\"Normalized Dataset:\")\n",
    "print(normalized_df)\n",
    "\n",
    "# Plot original and normalized datasets\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Original dataset\n",
    "ax1.scatter(df['Feature1'], df['Feature2'], c='blue', label='Original Data')\n",
    "ax1.set_title('Original Dataset')\n",
    "ax1.set_xlabel('Feature1')\n",
    "ax1.set_ylabel('Feature2')\n",
    "ax1.legend()\n",
    "\n",
    "# Normalized dataset\n",
    "ax2.scatter(normalized_df['Feature1'], normalized_df['Feature2'], c='red', label='Normalized Data')\n",
    "ax2.set_title('Normalized Dataset')\n",
    "ax2.set_xlabel('Normalized Feature1')\n",
    "ax2.set_ylabel('Normalized Feature2')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the MinMaxScaler from scikit-learn is used to normalize the numerical features in the dataset.\n",
    "\n",
    "    Normalization is crucial for certain machine learning algorithms, especially those sensitive to the scale of the input features (e.g., k-nearest neighbors or support vector machines). The Min-Max Scaling example demonstrates how to normalize numerical features in a datase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2. Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretization is a data preprocessing technique that involves converting continuous numerical attributes into discrete categories or bins. This can be useful in situations where the continuous nature of the data is not necessary or when working with algorithms that perform better with categorical data. Discretization simplifies the data and can make it more suitable for certain types of analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Discretization in Python: Equal Width Binning\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "Equal Width Binning is a simple discretization method that divides the range of continuous values into equal-width intervals or bins. Each interval represents a discrete category, and values within that range are assigned to the corresponding bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# Example dataset with a continuous numerical feature\n",
    "data = {\n",
    "    'Age': [22, 35, 42, 28, 55, 48, 33, 40, 60, 70]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Equal Width Binning to discretize the 'Age' feature\n",
    "discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "discretized_data = discretizer.fit_transform(df)\n",
    "\n",
    "# Display the discretized dataset\n",
    "discretized_df = pd.DataFrame(discretized_data, columns=df.columns)\n",
    "print(\"Discretized Dataset:\")\n",
    "print(discretized_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the KBinsDiscretizer from scikit-learn is used to apply equal-width binning to the 'Age' feature, dividing it into three bins.\n",
    "\n",
    "    Discretization is valuable when working with algorithms that require categorical input or when simplifying the analysis of continuous data. The Equal Width Binning example demonstrates how to discretize a numerical feature into categorical bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3. Data compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data compression is a process of reducing the size or volume of data to save storage space and improve computational efficiency. In the context of data mining, compression techniques can be applied to handle large datasets more efficiently, speed up processing, and reduce the resource requirements for storage and transmission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Data Compression in Python: Run-Length Encoding\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "Run-Length Encoding (RLE) is a simple data compression technique that represents consecutive repeated values as a single value and a count. It is particularly effective for datasets with long runs of identical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length_encoding(data):\n",
    "    encoded_data = []\n",
    "    count = 1\n",
    "\n",
    "    for i in range(1, len(data)):\n",
    "        if data[i] == data[i - 1]:\n",
    "            count += 1\n",
    "        else:\n",
    "            encoded_data.append((data[i - 1], count))\n",
    "            count = 1\n",
    "\n",
    "    # Add the last run\n",
    "    encoded_data.append((data[-1], count))\n",
    "    return encoded_data\n",
    "\n",
    "# Example dataset with consecutive repeated values\n",
    "data = [1, 1, 1, 2, 3, 3, 3, 3, 4, 4]\n",
    "\n",
    "# Apply Run-Length Encoding to compress the data\n",
    "compressed_data = run_length_encoding(data)\n",
    "\n",
    "# Display the compressed data\n",
    "print(\"Compressed Data:\")\n",
    "print(compressed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the run_length_encoding function is used to compress a dataset with consecutive repeated values using Run-Length Encoding.\n",
    "\n",
    "    Data compression is beneficial for reducing storage requirements and speeding up data processing. While Run-Length Encoding is a basic example, there are more advanced compression techniques like Huffman coding or Lempel-Ziv compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.4. Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling is a data preprocessing technique that involves selecting a subset of data from a larger dataset. This subset, known as a sample, is representative of the overall population and is often used for various purposes, such as exploratory analysis, model training, or performance testing. Sampling can be particularly valuable when working with large datasets, as it allows for more efficient processing and analysis of a manageable subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Sampling in Python: Random Sampling\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "Random Sampling is a common sampling technique where data points are selected randomly from the dataset, providing an unbiased representation of the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "data = {\n",
    "    'ID': range(1, 101),\n",
    "    'Value': np.random.randint(1, 100, size=100)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform random sampling to select a subset of data\n",
    "sample_size = 20\n",
    "random_sample = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Display the randomly sampled data\n",
    "print(\"Randomly Sampled Data:\")\n",
    "print(random_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the sample function from pandas is used to perform random sampling on a dataset. The random_state parameter ensures reproducibility.\n",
    "\n",
    "    Sampling is essential when dealing with large datasets to make computations more manageable and to speed up the analysis process. Random Sampling is just one of many sampling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1. Principal components analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Components Analysis (PCA) is a dimensionality reduction technique used to transform a dataset into a lower-dimensional space while retaining as much of the original variance as possible. PCA identifies the principal components, which are orthogonal vectors that capture the directions of maximum variance in the data. By projecting the data onto these components, PCA allows for a more concise representation of the dataset, making it easier to visualize and analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Principal Components Analysis in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example dataset\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "target = pd.Series(data=iris.target, name='target')\n",
    "\n",
    "# Apply PCA for dimensionality reduction to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(data)\n",
    "\n",
    "# Create a DataFrame with the principal components and target\n",
    "pc_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "pc_df['target'] = target\n",
    "\n",
    "# Plot the data in the reduced dimensional space\n",
    "plt.figure(figsize=(8, 6))\n",
    "targets = [0, 1, 2]\n",
    "colors = ['r', 'g', 'b']\n",
    "\n",
    "for target, color in zip(targets, colors):\n",
    "    indices_to_keep = pc_df['target'] == target\n",
    "    plt.scatter(pc_df.loc[indices_to_keep, 'PC1'],\n",
    "                pc_df.loc[indices_to_keep, 'PC2'],\n",
    "                c=color,\n",
    "                label=target,\n",
    "                alpha=0.7)\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Principal Components Analysis')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, PCA is applied to the Iris dataset, which contains measurements of sepal length, sepal width, petal length, and petal width for three species of iris flowers. The code reduces the dimensionality to two principal components and visualizes the data in the reduced space.\n",
    "\n",
    "    PCA is widely used for feature reduction, noise reduction, and visualization. The example demonstrates how PCA can be applied to simplify and visualize high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2. Attribute subset selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute subset selection, also known as feature selection, involves identifying and selecting a subset of relevant features from the original set of attributes in a dataset. The goal is to improve model performance, reduce dimensionality, and enhance interpretability by retaining only the most informative attributes. This process is crucial for addressing the \"curse of dimensionality\" and can lead to more efficient and accurate data mining models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Attribute Subset Selection in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example dataset\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "target = pd.Series(data=iris.target, name='target')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply RandomForestClassifier for feature selection\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Select features based on importance\n",
    "sfm = SelectFromModel(clf, threshold=0.2)\n",
    "sfm.fit(X_train, y_train)\n",
    "\n",
    "# Transform datasets with selected features\n",
    "X_train_selected = sfm.transform(X_train)\n",
    "X_test_selected = sfm.transform(X_test)\n",
    "\n",
    "# Train and evaluate a classifier with the selected features\n",
    "clf_selected = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_selected.fit(X_train_selected, y_train)\n",
    "y_pred_selected = clf_selected.predict(X_test_selected)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_selected)\n",
    "print(f'Accuracy with selected features: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, a RandomForestClassifier is used to assess feature importance, and SelectFromModel is applied to select features based on their importance. The classifier is then trained and evaluated using only the selected features.\n",
    "\n",
    "    Attribute subset selection is crucial for improving model efficiency, reducing overfitting, and enhancing interpretability. The example demonstrates how to apply attribute subset selection using feature importance from a random forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.3. Nonlinear dimensionality reduction methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonlinear dimensionality reduction methods aim to capture complex relationships and structures in high-dimensional data that may not be well-represented by linear techniques like PCA. These methods transform data into a lower-dimensional space while preserving the inherent nonlinear relationships within the data. Nonlinear dimensionality reduction is particularly useful when dealing with datasets containing intricate patterns, clusters, or manifolds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Nonlinear Dimensionality Reduction in Python: t-Distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example dataset\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "target = pd.Series(data=iris.target, name='target')\n",
    "\n",
    "# Apply t-SNE for nonlinear dimensionality reduction to 2 components\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embedded_data = tsne.fit_transform(data)\n",
    "\n",
    "# Create a DataFrame with the embedded data and target\n",
    "embedded_df = pd.DataFrame(data=embedded_data, columns=['Component 1', 'Component 2'])\n",
    "embedded_df['target'] = target\n",
    "\n",
    "# Plot the data in the reduced dimensional space\n",
    "plt.figure(figsize=(8, 6))\n",
    "targets = [0, 1, 2]\n",
    "colors = ['r', 'g', 'b']\n",
    "\n",
    "for target, color in zip(targets, colors):\n",
    "    indices_to_keep = embedded_df['target'] == target\n",
    "    plt.scatter(embedded_df.loc[indices_to_keep, 'Component 1'],\n",
    "                embedded_df.loc[indices_to_keep, 'Component 2'],\n",
    "                c=color,\n",
    "                label=target,\n",
    "                alpha=0.7)\n",
    "\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.title('t-SNE: Nonlinear Dimensionality Reduction')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, t-SNE is applied to the Iris dataset, and the data is visualized in a two-dimensional space. t-SNE is known for preserving local similarities, making it effective for capturing intricate structures and patterns in the data.\n",
    "\n",
    "    Nonlinear dimensionality reduction methods, like t-SNE, are valuable for visualizing and understanding complex relationships in high-dimensional data. The example introduces how t-SNE can be used for nonlinear dimensionality reduction and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining_2024_spring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
