{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.2. Types of outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers in a dataset can take various forms, and understanding the types of outliers is crucial for effective outlier detection. Broadly, outliers can be categorized into the following types:\n",
    "\n",
    "- Global Outliers:\n",
    "        Global outliers are data points that deviate significantly from the overall pattern of the entire dataset. They are characterized by extreme values that are far from the central tendency of the data.\n",
    "\n",
    "- Contextual Outliers:\n",
    "        Contextual outliers are data points that are unusual within a specific context or subset of the data. Identifying contextual outliers requires considering the local characteristics of the data.\n",
    "\n",
    "- Collective Outliers:\n",
    "        Collective outliers, also known as cluster outliers or subpopulations, refer to groups of data points that together form an anomalous pattern. Detecting collective outliers involves identifying unusual structures within the dataset.\n",
    "\n",
    "- Attribute Outliers:\n",
    "        Attribute outliers exhibit extreme values in one or more attributes or features. These outliers can be identified by examining individual features independently.\n",
    "\n",
    "Understanding the types of outliers is essential for selecting appropriate outlier detection techniques and interpreting the results in the context of specific data characteristics.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for outlier detection using the Isolation Forest algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Generate synthetic data with global, contextual, and collective outliers\n",
    "np.random.seed(42)\n",
    "data_normal = np.random.normal(loc=0, scale=1, size=(100, 2))\n",
    "data_global_outliers = np.random.normal(loc=0, scale=10, size=(5, 2))\n",
    "data_contextual_outliers = np.random.normal(loc=20, scale=1, size=(5, 2))\n",
    "data_collector_outliers_1 = np.random.normal(loc=0, scale=1, size=(20, 2))\n",
    "data_collector_outliers_2 = np.random.normal(loc=30, scale=1, size=(20, 2))\n",
    "\n",
    "# Combine the datasets\n",
    "data = np.vstack([data_normal, data_global_outliers, data_contextual_outliers, data_collector_outliers_1, data_collector_outliers_2])\n",
    "\n",
    "# Apply Isolation Forest for outlier detection\n",
    "isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "labels = isolation_forest.fit_predict(data)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('Outlier Detection using Isolation Forest')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with different types of outliers (global, contextual, and collective) and apply the Isolation Forest algorithm for outlier detection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.1. Parametric methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametric methods for outlier detection assume that the underlying distribution of the data can be characterized by a specific parametric model (e.g., Gaussian distribution). These methods rely on statistical measures and hypothesis testing to identify data points that deviate significantly from the expected distribution.\n",
    "\n",
    "#### Key points about parametric methods:\n",
    "\n",
    "1. Assumption of Distribution:\n",
    "        Parametric methods assume that the data follow a known distribution. Commonly used distributions include the normal (Gaussian) distribution.\n",
    "\n",
    "2. Z-Score:\n",
    "        Z-Score is a common parametric method that measures how many standard deviations a data point is from the mean. Data points with high absolute Z-Scores are considered outliers.\n",
    "\n",
    "3. Hypothesis Testing:\n",
    "        Parametric methods often involve hypothesis testing to assess whether a data point or a set of data points significantly deviate from the expected distribution. Common tests include the t-test and chi-square test.\n",
    "\n",
    "4. Model-Based Approaches:\n",
    "        Some parametric methods build explicit models of the data distribution and identify outliers based on the likelihood of observed data under the model.\n",
    "\n",
    "5. Assumption Sensitivity:\n",
    "        The effectiveness of parametric methods depends on the accuracy of the distributional assumptions. If the data deviate significantly from the assumed distribution, parametric methods may provide inaccurate results.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for outlier detection using Z-Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Generate synthetic data with outliers\n",
    "np.random.seed(42)\n",
    "data_normal = np.random.normal(loc=0, scale=1, size=100)\n",
    "data_outliers = np.random.normal(loc=10, scale=1, size=5)\n",
    "\n",
    "# Combine normal data with outliers\n",
    "data = np.concatenate([data_normal, data_outliers])\n",
    "\n",
    "# Calculate Z-Score for each data point\n",
    "z_scores = zscore(data)\n",
    "\n",
    "# Define a threshold for identifying outliers\n",
    "threshold = 3.5\n",
    "\n",
    "# Identify outliers based on the threshold\n",
    "outliers = np.abs(z_scores) > threshold\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(len(data)), data, c=outliers, cmap='viridis', alpha=0.7)\n",
    "plt.axhline(y=threshold, color='r', linestyle='--', label='Outlier Threshold')\n",
    "plt.title('Outlier Detection using Z-Score')\n",
    "plt.xlabel('Data Point Index')\n",
    "plt.ylabel('Data Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with outliers and use the Z-Score to identify outliers based on a specified threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.2. Nonparametric methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonparametric methods for outlier detection do not make strong assumptions about the underlying distribution of the data. These methods rely on ranking or other distribution-free techniques to identify data points that deviate from the expected pattern.\n",
    "\n",
    "#### Key points about nonparametric methods:\n",
    "\n",
    "1. Rank-Based Techniques:\n",
    "        Nonparametric methods often use rank-based statistics, such as the median and interquartile range (IQR), which are less sensitive to extreme values than mean and standard deviation.\n",
    "\n",
    "2. Percentile-based Approaches:\n",
    "        Outliers can be identified based on percentiles or quantiles of the data distribution. Data points falling outside a certain percentile range are considered outliers.\n",
    "\n",
    "3. Kernel Density Estimation (KDE):\n",
    "        KDE is a nonparametric method that estimates the probability density function of the data. Outliers can be identified based on low-density regions in the estimated distribution.\n",
    "\n",
    "4. Local Outlier Factor (LOF):\n",
    "        LOF is a nonparametric method that measures the local density deviation of a data point with respect to its neighbors. Points with significantly lower local density are considered outliers.\n",
    "\n",
    "5. Distribution-Free Tests:\n",
    "        Nonparametric methods often use statistical tests that do not rely on specific distributional assumptions. For example, the Kolmogorov-Smirnov test assesses whether a sample comes from a specific distribution.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for outlier detection using the Local Outlier Factor (LOF) algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Generate synthetic data with outliers\n",
    "np.random.seed(42)\n",
    "data_normal = np.random.normal(loc=0, scale=1, size=100)\n",
    "data_outliers = np.random.normal(loc=10, scale=1, size=5)\n",
    "\n",
    "# Combine normal data with outliers\n",
    "data = np.concatenate([data_normal, data_outliers]).reshape(-1, 1)\n",
    "\n",
    "# Apply Local Outlier Factor (LOF) for outlier detection\n",
    "lof = LocalOutlierFactor(contamination=0.1)\n",
    "labels = lof.fit_predict(data)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(len(data)), data, c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('Outlier Detection using Local Outlier Factor (LOF)')\n",
    "plt.xlabel('Data Point Index')\n",
    "plt.ylabel('Data Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with outliers and apply the Local Outlier Factor (LOF) algorithm for outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.1. Distance-based outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance-based outlier detection methods identify outliers by measuring the dissimilarity or distance between data points. These methods assume that outliers are often isolated or far from the majority of the data points, resulting in larger distances.\n",
    "\n",
    "#### Key points about distance-based outlier detection:\n",
    "\n",
    "1. Euclidean Distance:\n",
    "        The Euclidean distance between data points is a common measure used in distance-based outlier detection. It calculates the straight-line distance between two points in the feature space.\n",
    "\n",
    "2. Mahalanobis Distance:\n",
    "        Mahalanobis distance takes into account the correlation between features and provides a more accurate measure of distance in the presence of correlated features.\n",
    "\n",
    "3. Nearest Neighbor Approaches:\n",
    "        Distance-based outlier detection often involves comparing the distances of data points to their k-nearest neighbors. Outliers are identified as points with unusually large distances to their neighbors.\n",
    "\n",
    "4. Outlier Scores:\n",
    "        Outlier scores are computed based on the distances, and a threshold is set to distinguish outliers from normal data points.\n",
    "\n",
    "5. Robust to Distribution:\n",
    "        Distance-based methods are often robust to the underlying distribution of the data, making them suitable for various types of datasets.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for outlier detection using the Isolation Forest algorithm, which is a distance-based method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Generate synthetic data with outliers\n",
    "np.random.seed(42)\n",
    "data_normal = np.random.normal(loc=0, scale=1, size=(100, 2))\n",
    "data_outliers = np.random.normal(loc=10, scale=1, size=(5, 2))\n",
    "\n",
    "# Combine normal data with outliers\n",
    "data = np.vstack([data_normal, data_outliers])\n",
    "\n",
    "# Apply Isolation Forest for distance-based outlier detection\n",
    "isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "labels = isolation_forest.fit_predict(data)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('Outlier Detection using Isolation Forest (Distance-Based)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with outliers and apply the Isolation Forest algorithm, which is a distance-based method, for outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.2. Density-based outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density-based outlier detection methods identify outliers based on the local density of data points. These methods assume that outliers are characterized by significantly lower local density compared to their neighbors. Density-based approaches are particularly effective in identifying outliers in regions of varying data density.\n",
    "\n",
    "#### Key points about density-based outlier detection:\n",
    "\n",
    "1. Local Density Estimation:\n",
    "        Density-based methods estimate the local density around each data point, typically by considering the number of neighboring points within a specified distance.\n",
    "\n",
    "2. Clustering-Based Approaches:\n",
    "        Density-based approaches often involve clustering, where dense regions are considered clusters, and outliers are points that do not belong to any cluster or belong to a low-density cluster.\n",
    "\n",
    "3. Outlier Scores:\n",
    "        Outlier scores are computed based on the local density, and points with lower density are considered potential outliers.\n",
    "\n",
    "4. Example Method: DBSCAN (Density-Based Spatial Clustering of Applications with Noise):\n",
    "        DBSCAN is a popular density-based method that identifies clusters based on data density and marks points outside these clusters as outliers.\n",
    "\n",
    "5. Adaptive to Local Patterns:\n",
    "        Density-based methods are adaptive to local patterns and can identify outliers in regions of varying data density, making them suitable for complex datasets.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for outlier detection using DBSCAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Generate synthetic data with outliers\n",
    "data, _ = make_moons(n_samples=200, noise=0.05, random_state=42)\n",
    "\n",
    "# Apply DBSCAN for density-based outlier detection\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
    "labels = dbscan.fit_predict(data)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('Outlier Detection using DBSCAN (Density-Based)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with outliers using the make_moons function from scikit-learn and apply DBSCAN for density-based outlier detection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.1. Matrix factorization–based methods for numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix factorization–based methods for outlier detection involve decomposing the original data matrix into lower-dimensional matrices, aiming to capture the underlying structure of the data. Outliers are then identified based on the differences between the observed and reconstructed data.\n",
    "\n",
    "#### Key points about matrix factorization–based methods:\n",
    "\n",
    "1. Matrix Decomposition:\n",
    "        Matrix factorization techniques decompose the original data matrix into two or more lower-dimensional matrices. Common methods include Singular Value Decomposition (SVD) and Non-Negative Matrix Factorization (NMF).\n",
    "\n",
    "2. Reconstruction Error:\n",
    "        The difference between the original data matrix and its reconstruction is calculated as the reconstruction error. Outliers are identified based on unusually high reconstruction errors.\n",
    "\n",
    "3. Low-Rank Approximation:\n",
    "        Matrix factorization aims to approximate the original data matrix with a low-rank approximation, capturing the essential features while reducing the impact of noise and outliers.\n",
    "\n",
    "4. Robust to Anomalies:\n",
    "        Matrix factorization methods are often robust to outliers, as they focus on capturing the dominant patterns in the data.\n",
    "\n",
    "5. Applications:\n",
    "        These methods are commonly used for outlier detection in numerical data, such as sensor readings, image data, and other multidimensional datasets.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for outlier detection using matrix factorization with Singular Value Decomposition (SVD):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic data with outliers\n",
    "np.random.seed(42)\n",
    "data_normal = np.random.normal(loc=0, scale=1, size=(100, 2))\n",
    "data_outliers = np.random.normal(loc=10, scale=1, size=(5, 2))\n",
    "\n",
    "# Combine normal data with outliers\n",
    "data = np.vstack([data_normal, data_outliers])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "# Apply Singular Value Decomposition (SVD) for matrix factorization\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "svd.fit(data_standardized)\n",
    "reconstructed_data = svd.inverse_transform(svd.transform(data_standardized))\n",
    "\n",
    "# Calculate the reconstruction error\n",
    "reconstruction_error = np.mean(np.square(data_standardized - reconstructed_data), axis=1)\n",
    "\n",
    "# Set a threshold for identifying outliers\n",
    "threshold = 2.5\n",
    "\n",
    "# Identify outliers based on the threshold\n",
    "outliers = reconstruction_error > threshold\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(len(data)), reconstruction_error, c=outliers, cmap='viridis', alpha=0.7)\n",
    "plt.axhline(y=threshold, color='r', linestyle='--', label='Outlier Threshold')\n",
    "plt.title('Outlier Detection using Matrix Factorization (SVD)')\n",
    "plt.xlabel('Data Point Index')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with outliers and apply matrix factorization using Singular Value Decomposition (SVD) for outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.2. Pattern-based compression methods for categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pattern-based compression methods for categorical data involve representing the original categorical data using compressed patterns or codes. Outliers are then identified based on the dissimilarity between the observed and reconstructed categorical patterns.\n",
    "\n",
    "#### Key points about pattern-based compression methods:\n",
    "\n",
    "1. Categorical Data Representation:\n",
    "        Categorical data is represented using compressed patterns or codes, typically obtained through techniques like frequent pattern mining or encoding schemes.\n",
    "\n",
    "2. Reconstruction Error:\n",
    "        The difference between the original categorical data and its reconstructed form is calculated as the reconstruction error. Outliers are identified based on unusually high reconstruction errors.\n",
    "\n",
    "3. Pattern Mining Techniques:\n",
    "        Frequent pattern mining algorithms, such as Apriori or FP-growth, can be employed to discover significant patterns in categorical data.\n",
    "\n",
    "4. One-Hot Encoding:\n",
    "        One-Hot Encoding is a common encoding scheme where each category is represented as a binary vector, and the compression is achieved by capturing the presence or absence of categories.\n",
    "\n",
    "5. Applications:\n",
    "        These methods are particularly useful for outlier detection in datasets where categorical variables dominate, such as customer transaction data or item purchase histories.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for outlier detection using pattern-based compression methods with One-Hot Encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Generate synthetic categorical data with outliers\n",
    "data = pd.DataFrame({\n",
    "    'Category_A': ['A', 'B', 'A', 'C', 'B', 'A', 'A'],\n",
    "    'Category_B': ['X', 'Y', 'X', 'Z', 'Y', 'Z', 'X'],\n",
    "    'Category_C': ['High', 'Low', 'Medium', 'High', 'Medium', 'Low', 'Low']\n",
    "})\n",
    "\n",
    "# Apply One-Hot Encoding to the categorical data\n",
    "encoder = OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(data).toarray()\n",
    "\n",
    "# Apply Singular Value Decomposition (SVD) for matrix factorization\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "svd.fit(encoded_data)\n",
    "reconstructed_data = svd.inverse_transform(svd.transform(encoded_data))\n",
    "\n",
    "# Calculate the reconstruction error\n",
    "reconstruction_error = ((encoded_data - reconstructed_data) ** 2).sum(axis=1)\n",
    "\n",
    "# Set a threshold for identifying outliers\n",
    "threshold = 0.1\n",
    "\n",
    "# Identify outliers based on the threshold\n",
    "outliers = reconstruction_error > threshold\n",
    "\n",
    "# Visualize the results\n",
    "data['Reconstruction Error'] = reconstruction_error\n",
    "data['Outlier'] = outliers.astype(int)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic categorical data with outliers and apply pattern-based compression using One-Hot Encoding followed by matrix factorization with Singular Value Decomposition (SVD) for outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.1. Clustering-based approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering-based approaches for outlier detection involve grouping data points into clusters based on similarity and then identifying outliers as points that do not conform well to any cluster. Outliers are often considered as data points that do not fit within the established clusters.\n",
    "\n",
    "#### Key points about clustering-based approaches:\n",
    "\n",
    "1. Cluster Formation:\n",
    "        Data points are grouped into clusters based on similarity or proximity. Common clustering algorithms include K-Means, hierarchical clustering, and DBSCAN.\n",
    "\n",
    "2. Outlier Identification:\n",
    "        Outliers are points that do not belong to any well-defined cluster or are assigned to small or noise clusters. These points are distant from the centroids or centers of established clusters.\n",
    "\n",
    "3. Distance Measures:\n",
    "        Distance measures, such as Euclidean distance or Mahalanobis distance, are often used to assess the similarity between data points and cluster centers.\n",
    "\n",
    "4. Cluster Size and Density:\n",
    "        Outliers are often identified in clusters with small sizes or low density, as these clusters may represent anomalies in the data.\n",
    "\n",
    "5. Applications:\n",
    "        Clustering-based outlier detection is applicable to a wide range of datasets, including those with both numerical and categorical features.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for outlier detection using the DBSCAN clustering algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic data with outliers\n",
    "data, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)\n",
    "outliers = np.array([[0, 10], [5, 5]])\n",
    "\n",
    "# Combine normal data with outliers\n",
    "data = np.vstack([data, outliers])\n",
    "\n",
    "# Apply DBSCAN for clustering-based outlier detection\n",
    "dbscan = DBSCAN(eps=1.5, min_samples=5)\n",
    "labels = dbscan.fit_predict(data)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('Outlier Detection using DBSCAN (Clustering-Based)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with outliers and apply the DBSCAN clustering algorithm for clustering-based outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.2. Classification-based approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification-based approaches for outlier detection involve training a machine learning model to distinguish between normal and anomalous instances. The model is trained on a labeled dataset, where normal instances are the majority class, and outliers are the minority class. Once trained, the model can predict whether a new instance is normal or an outlier.\n",
    "\n",
    "#### Key points about classification-based approaches:\n",
    "\n",
    "1. Supervised Learning:\n",
    "        Classification-based approaches are supervised learning methods that require labeled data for training. The dataset should include instances labeled as either normal or outliers.\n",
    "\n",
    "2. Model Selection:\n",
    "        Common classification algorithms, such as Support Vector Machines (SVM), Random Forests, or Neural Networks, can be employed for building outlier detection models.\n",
    "\n",
    "3. Imbalanced Data Handling:\n",
    "        Outlier detection datasets are often imbalanced, with the majority of instances being normal. Handling class imbalance is crucial for model performance, and techniques like oversampling, undersampling, or using specialized algorithms are employed.\n",
    "\n",
    "4. Decision Threshold:\n",
    "        The model's decision is based on a threshold probability or score. Instances with a probability below the threshold are classified as outliers.\n",
    "\n",
    "5. Applications:\n",
    "        Classification-based outlier detection is versatile and applicable to various domains, including fraud detection, network security, and healthcare.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for outlier detection using a Support Vector Machine (SVM) classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic data with outliers\n",
    "data, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)\n",
    "outliers = np.array([[0, 10], [5, 5]])\n",
    "\n",
    "# Combine normal data with outliers\n",
    "data = np.vstack([data, outliers])\n",
    "\n",
    "# Apply One-Class SVM for classification-based outlier detection\n",
    "svm = OneClassSVM(nu=0.05)\n",
    "labels = svm.fit_predict(data)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('Outlier Detection using One-Class SVM (Classification-Based)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with outliers and apply a One-Class SVM classifier for classification-based outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.1. Transforming contextual outlier detection to conventional outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contextual outlier detection involves considering the context or environment in which data points exist. Transforming contextual outlier detection to conventional outlier detection often involves extracting relevant features or representations from the contextual information, allowing the use of standard outlier detection techniques.\n",
    "\n",
    "#### Key points about transforming contextual outlier detection:\n",
    "\n",
    "1. Contextual Information:\n",
    "        Contextual outlier detection considers additional information or features that describe the context or environment of data points. This could include temporal information, spatial information, or any other relevant context.\n",
    "\n",
    "2. Feature Extraction:\n",
    "        To transform contextual outlier detection to conventional outlier detection, feature extraction techniques are often applied to distill important aspects of the contextual information into numerical features.\n",
    "\n",
    "3. Conventional Techniques:\n",
    "        Once relevant features are extracted, conventional outlier detection techniques, such as clustering or classification-based methods, can be applied to identify outliers.\n",
    "\n",
    "4. Applications:\n",
    "        This approach is useful when dealing with data where contextual information is available but needs to be translated into a format suitable for standard outlier detection methods.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for transforming contextual outlier detection using feature extraction and applying a clustering-based approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generate synthetic data with contextual information\n",
    "np.random.seed(42)\n",
    "data_contextual = np.random.normal(loc=0, scale=1, size=(100, 2))\n",
    "data_outliers = np.random.normal(loc=10, scale=1, size=(5, 2))\n",
    "\n",
    "# Combine normal data with outliers\n",
    "data = np.vstack([data_contextual, data_outliers])\n",
    "\n",
    "# Add temporal information as a context\n",
    "temporal_context = np.arange(0, 100).reshape(-1, 1)\n",
    "data_with_context = np.hstack([data, temporal_context])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data_with_context)\n",
    "\n",
    "# Apply Principal Component Analysis (PCA) for feature extraction\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data_standardized)\n",
    "\n",
    "# Apply DBSCAN for conventional outlier detection on the extracted features\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "labels = dbscan.fit_predict(data_pca)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data_pca[:, 0], data_pca[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('Transformed Contextual Outlier Detection using PCA and DBSCAN')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with contextual information (temporal context) and transform it into conventional outlier detection by using Principal Component Analysis (PCA) for feature extraction, followed by applying the DBSCAN clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.2. Modeling normal behavior with respect to contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling normal behavior with respect to contexts involves building models that capture the typical patterns or behaviors within specific contexts. This approach aims to distinguish between normal and anomalous instances based on the learned context-specific normal behavior models.\n",
    "\n",
    "#### Key points about modeling normal behavior with respect to contexts:\n",
    "\n",
    "- Contextual Information:\n",
    "        Consideration of contextual information is essential. This could include environmental factors, time-based patterns, or any other context-specific attributes.\n",
    "\n",
    "- Normal Behavior Modeling:\n",
    "        Normal behavior within each context is modeled to understand the expected patterns or distributions. This modeling is often done using statistical methods, machine learning models, or domain-specific knowledge.\n",
    "\n",
    "- Anomaly Detection:\n",
    "        Anomalies are identified by comparing observed instances with the modeled normal behavior. Instances that deviate significantly from the expected patterns are considered outliers.\n",
    "\n",
    "- Dynamic Contexts:\n",
    "        In dynamic contexts, models may need to adapt over time as normal behavior evolves. Continuous monitoring and retraining of models might be necessary.\n",
    "\n",
    "- Applications:\n",
    "        This approach is suitable for scenarios where normal behavior varies across different contexts, and a context-aware understanding is required for effective outlier detection.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for modeling normal behavior with respect to contexts using a Gaussian Mixture Model (GMM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic data with contextual information\n",
    "np.random.seed(42)\n",
    "data_contextual = np.random.normal(loc=0, scale=1, size=(100, 2))\n",
    "data_outliers = np.random.normal(loc=10, scale=1, size=(5, 2))\n",
    "\n",
    "# Combine normal data with outliers\n",
    "data = np.vstack([data_contextual, data_outliers])\n",
    "\n",
    "# Add temporal information as a context\n",
    "temporal_context = np.arange(0, 100).reshape(-1, 1)\n",
    "data_with_context = np.hstack([data, temporal_context])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data_with_context)\n",
    "\n",
    "# Apply Gaussian Mixture Model (GMM) for modeling normal behavior\n",
    "gmm = GaussianMixture(n_components=2, covariance_type='full', random_state=42)\n",
    "gmm.fit(data_standardized)\n",
    "\n",
    "# Predict the probability of each data point belonging to the modeled normal behavior\n",
    "probabilities = gmm.score_samples(data_standardized)\n",
    "\n",
    "# Set a threshold for identifying outliers\n",
    "threshold = -8\n",
    "\n",
    "# Identify outliers based on the threshold\n",
    "outliers = probabilities < threshold\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data_with_context[:, 0], data_with_context[:, 1], c=outliers, cmap='viridis', alpha=0.7)\n",
    "plt.title('Modeling Normal Behavior with GMM and Contextual Information')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with contextual information (temporal context) and model normal behavior using a Gaussian Mixture Model (GMM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.3. Mining collective outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mining collective outliers involves identifying groups or subsets of instances that collectively exhibit anomalous behavior. Instead of focusing on individual data points, this approach looks for patterns of anomalies that emerge when considering the interactions or relationships between multiple instances.\n",
    "\n",
    "#### Key points about mining collective outliers:\n",
    "\n",
    "- Collective Anomalies:\n",
    "        Collective outliers are anomalies that arise when considering the relationships, interactions, or dependencies among multiple instances rather than individual instances.\n",
    "\n",
    "- Relationship Modeling:\n",
    "        Mining collective outliers often involves modeling relationships or dependencies between data points, such as association rules, graph-based approaches, or other forms of relational modeling.\n",
    "\n",
    "- Emergent Patterns:\n",
    "        Collective outliers may exhibit emergent patterns that are not apparent when analyzing individual instances in isolation. The collective behavior becomes abnormal due to the combined effects of the group.\n",
    "\n",
    "- Applications:\n",
    "        This approach is beneficial for scenarios where anomalies are better understood in the context of relationships or interactions, such as fraud detection in networks, group behavior analysis, or collaborative filtering.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for mining collective outliers using a graph-based approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic data with clusters and outliers\n",
    "data, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)\n",
    "outliers = np.array([[0, 10], [5, 5]])\n",
    "\n",
    "# Combine normal data with outliers\n",
    "data = np.vstack([data, outliers])\n",
    "\n",
    "# Create a graph representation of the data\n",
    "G = nx.Graph()\n",
    "for i in range(data.shape[0]):\n",
    "    G.add_node(i)\n",
    "\n",
    "# Define a threshold for edge creation\n",
    "threshold = 2.0\n",
    "\n",
    "# Add edges between nodes based on proximity\n",
    "for i in range(data.shape[0]):\n",
    "    for j in range(i + 1, data.shape[0]):\n",
    "        distance = np.linalg.norm(data[i] - data[j])\n",
    "        if distance < threshold:\n",
    "            G.add_edge(i, j)\n",
    "\n",
    "# Detect connected components as potential collective outliers\n",
    "connected_components = list(nx.connected_components(G))\n",
    "\n",
    "# Identify nodes in collective outliers\n",
    "collective_outliers = [node for component in connected_components if len(component) > 1 for node in component]\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], c='blue', alpha=0.7, label='Normal Data')\n",
    "plt.scatter(data[collective_outliers, 0], data[collective_outliers, 1], c='red', marker='x', s=100, label='Collective Outliers')\n",
    "plt.title('Mining Collective Outliers using Graph-Based Approach')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with clusters and outliers and use a graph-based approach to identify collective outliers based on connected components in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.1. Extending conventional outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending conventional outlier detection methods to high-dimensional data involves addressing challenges that arise when dealing with datasets with a large number of features. Traditional approaches may struggle due to the curse of dimensionality, and modifications or specialized techniques are needed to effectively identify outliers in high-dimensional spaces.\n",
    "\n",
    "#### Key points about extending conventional outlier detection for high-dimensional data:\n",
    "\n",
    "- Curse of Dimensionality:\n",
    "        In high-dimensional spaces, data points become sparser, and the distances between points tend to increase, making conventional distance-based methods less effective.\n",
    "\n",
    "- Dimensionality Reduction:\n",
    "        Techniques such as Principal Component Analysis (PCA) or t-Distributed Stochastic Neighbor Embedding (t-SNE) can be employed to reduce the dimensionality of the data while preserving important information.\n",
    "\n",
    "- Sparse Models:\n",
    "        High-dimensional data often exhibits sparsity, where only a small subset of features may contribute to outliers. Sparse models, like LASSO (Least Absolute Shrinkage and Selection Operator), can help identify relevant features.\n",
    "\n",
    "- Ensemble Methods:\n",
    "        Combining multiple outlier detection models or using ensemble methods tailored for high-dimensional data can improve overall performance.\n",
    "\n",
    "- Applications:\n",
    "        This approach is crucial for domains with datasets containing a large number of features, such as genomics, image analysis, or financial data with numerous variables.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for extending conventional outlier detection to high-dimensional data using ensemble methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generate synthetic high-dimensional data with outliers\n",
    "data, _ = make_multilabel_classification(n_samples=500, n_features=20, n_classes=2, n_clusters_per_class=1, n_informative=10, random_state=42)\n",
    "outliers = np.random.normal(loc=0, scale=1, size=(10, 20))\n",
    "\n",
    "# Combine normal data with outliers\n",
    "data = np.vstack([data, outliers])\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data)\n",
    "\n",
    "# Apply Isolation Forest for ensemble-based outlier detection\n",
    "isolation_forest = IsolationForest(contamination=0.02, random_state=42)\n",
    "labels = isolation_forest.fit_predict(data)\n",
    "\n",
    "# Visualize the results in the reduced space\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data_pca[:, 0], data_pca[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('Outlier Detection in High-Dimensional Data using PCA and Isolation Forest')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic high-dimensional data with outliers, apply PCA for dimensionality reduction, and then use the Isolation Forest algorithm for ensemble-based outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.2. Finding outliers in subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding outliers in subspaces involves identifying anomalies in specific subsets of features rather than considering the entire high-dimensional space. This approach recognizes that outliers may manifest only in certain combinations of features, and traditional outlier detection techniques may not capture such localized anomalies.\n",
    "\n",
    "#### Key points about finding outliers in subspaces:\n",
    "\n",
    "1. Localized Anomalies:\n",
    "        Outliers may exist in specific subspaces or combinations of features, and these anomalies may not be evident when analyzing the entire set of features together.\n",
    "\n",
    "2. Subspace Exploration:\n",
    "        Techniques involve exploring different feature subsets or subspaces to identify which combinations of features contribute to the manifestation of outliers.\n",
    "\n",
    "3. Combinatorial Approaches:\n",
    "        Combinatorial methods, such as feature selection algorithms or brute-force exploration of feature combinations, can help identify relevant subspaces for outlier detection.\n",
    "\n",
    "4. Applications:\n",
    "        Useful for datasets where outliers are prominent in specific contexts or subsets of features, such as sensor networks, medical diagnostics, or any domain where different combinations of features might indicate anomalous behavior.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for finding outliers in subspaces using a combinatorial approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from itertools import combinations\n",
    "\n",
    "# Generate synthetic high-dimensional data with outliers\n",
    "data, _ = make_multilabel_classification(n_samples=500, n_features=20, n_classes=2, n_clusters_per_class=1, n_informative=10, random_state=42)\n",
    "outliers = np.random.normal(loc=0, scale=1, size=(10, 20))\n",
    "\n",
    "# Combine normal data with outliers\n",
    "data = np.vstack([data, outliers])\n",
    "\n",
    "# Define the number of features to select in each subspace\n",
    "num_features_per_subspace = 5\n",
    "\n",
    "# Generate all possible combinations of feature indices\n",
    "feature_combinations = list(combinations(range(data.shape[1]), num_features_per_subspace))\n",
    "\n",
    "# Apply Isolation Forest for outlier detection in each subspace\n",
    "outliers_subspaces = np.zeros(data.shape[0])\n",
    "\n",
    "for combination in feature_combinations:\n",
    "    # Extract data for the current subspace\n",
    "    data_subspace = data[:, combination]\n",
    "\n",
    "    # Apply Isolation Forest for outlier detection\n",
    "    isolation_forest = IsolationForest(contamination=0.02, random_state=42)\n",
    "    labels = isolation_forest.fit_predict(data_subspace)\n",
    "\n",
    "    # Identify outliers in the subspace\n",
    "    outliers_subspaces[labels == -1] += 1\n",
    "\n",
    "# Threshold for considering a point as an outlier in at least one subspace\n",
    "threshold = 3\n",
    "outliers_final = outliers_subspaces >= threshold\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=outliers_final, cmap='viridis', alpha=0.7)\n",
    "plt.title('Outlier Detection in Subspaces using Isolation Forest')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic high-dimensional data with outliers and use Isolation Forest for outlier detection in different subspaces defined by combinations of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.3. Outlier detection ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier detection ensemble methods involve combining the outputs of multiple outlier detection models to enhance the overall accuracy and robustness of outlier detection in high-dimensional data. This approach aims to mitigate the limitations of individual models and provide a more reliable identification of anomalies.\n",
    "\n",
    "#### Key points about outlier detection ensembles:\n",
    "\n",
    "- Diverse Models:\n",
    "        Ensemble methods leverage the strengths of diverse outlier detection algorithms. Each model may excel in different aspects or be more effective in specific subspaces.\n",
    "\n",
    "- Aggregation Strategies:\n",
    "        The outputs of individual models are aggregated to make a collective decision about whether a data point is an outlier. Common aggregation strategies include voting, averaging, or using meta-models.\n",
    "\n",
    "- Robustness:\n",
    "        Ensemble methods enhance the robustness of outlier detection by reducing the impact of individual model weaknesses or biases. This is particularly crucial in high-dimensional data where the effectiveness of a single model may be limited.\n",
    "\n",
    "- Applications:\n",
    "        Useful in scenarios where high-dimensional data poses challenges for individual models. Applications include fraud detection, network security, and any domain where outliers may exhibit diverse patterns.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for outlier detection ensembles using a combination of Isolation Forest and Local Outlier Factor (LOF):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Generate synthetic high-dimensional data with outliers\n",
    "data, _ = make_multilabel_classification(n_samples=500, n_features=20, n_classes=2, n_clusters_per_class=1, n_informative=10, random_state=42)\n",
    "outliers = np.random.normal(loc=0, scale=1, size=(10, 20))\n",
    "\n",
    "# Combine normal data with outliers\n",
    "data = np.vstack([data, outliers])\n",
    "\n",
    "# Define Isolation Forest and Local Outlier Factor models\n",
    "isolation_forest = IsolationForest(contamination=0.02, random_state=42)\n",
    "lof = LocalOutlierFactor(contamination=0.02)\n",
    "\n",
    "# Create an ensemble of outlier detection models\n",
    "ensemble = VotingClassifier(estimators=[('Isolation Forest', isolation_forest), ('Local Outlier Factor', lof)], voting='hard')\n",
    "\n",
    "# Fit the ensemble model\n",
    "ensemble.fit(data)\n",
    "\n",
    "# Predict outliers using the ensemble model\n",
    "labels = ensemble.predict(data)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('Outlier Detection Ensemble using Isolation Forest and Local Outlier Factor')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic high-dimensional data with outliers and create an ensemble of outlier detection models using Isolation Forest and Local Outlier Factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.4. Taming high dimensionality by deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taming high dimensionality by deep learning involves leveraging neural network architectures to automatically learn representations and patterns within high-dimensional data. Deep learning models, particularly autoencoders, are employed for dimensionality reduction, feature learning, and capturing complex structures in the data, aiding in the identification of outliers.\n",
    "\n",
    "#### Key points about taming high dimensionality by deep learning:\n",
    "\n",
    "- Autoencoders:\n",
    "        Autoencoders are neural network architectures designed for unsupervised learning tasks, including dimensionality reduction. They consist of an encoder and a decoder, where the encoder maps input data to a lower-dimensional representation, and the decoder reconstructs the input from this representation.\n",
    "\n",
    "- Feature Learning:\n",
    "        Deep learning models, by learning hierarchical representations, can capture intricate features and relationships within high-dimensional data. This enables more effective outlier detection by focusing on relevant patterns.\n",
    "\n",
    "- Nonlinear Transformations:\n",
    "        Deep learning models are capable of capturing nonlinear relationships in the data, which is especially valuable in high-dimensional spaces where linear methods may fall short.\n",
    "\n",
    "- Applications:\n",
    "        Taming high dimensionality with deep learning is beneficial in various domains, including image processing, genomics, and any field dealing with complex, high-dimensional data where traditional methods may struggle.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for taming high dimensionality by deep learning using an autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Generate synthetic high-dimensional data with outliers\n",
    "data, _ = make_multilabel_classification(n_samples=500, n_features=20, n_classes=2, n_clusters_per_class=1, n_informative=10, random_state=42)\n",
    "outliers = np.random.normal(loc=0, scale=1, size=(10, 20))\n",
    "\n",
    "# Combine normal data with outliers\n",
    "data = np.vstack([data, outliers])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "# Define and train an autoencoder model\n",
    "input_dim = data_standardized.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(15, activation='relu', input_dim=input_dim),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(15, activation='relu'),\n",
    "    Dense(input_dim, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(data_standardized, data_standardized, epochs=50, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "\n",
    "# Use the trained autoencoder for outlier detection\n",
    "decoded_data = model.predict(data_standardized)\n",
    "mse = np.mean(np.square(data_standardized - decoded_data), axis=1)\n",
    "\n",
    "# Set a threshold for identifying outliers\n",
    "threshold = 2.5\n",
    "outliers = mse > threshold\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data_standardized[:, 0], data_standardized[:, 1], c=outliers, cmap='viridis', alpha=0.7)\n",
    "plt.title('Outlier Detection using Autoencoder in High-Dimensional Data')\n",
    "plt.xlabel('Feature 1 (Standardized)')\n",
    "plt.ylabel('Feature 2 (Standardized)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic high-dimensional data with outliers and use an autoencoder for dimensionality reduction and outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7.5. Modeling high-dimensional outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling high-dimensional outliers involves using statistical and machine learning models that are specifically designed to capture the complex patterns present in high-dimensional data. These models aim to distinguish between normal and anomalous instances in a way that is effective for datasets with a large number of features.\n",
    "\n",
    "#### Key points about modeling high-dimensional outliers:\n",
    "\n",
    "- Distributional Assumptions:\n",
    "        Some models assume a specific distribution of normal data and identify outliers as instances that deviate significantly from this assumed distribution. However, in high-dimensional spaces, the choice of an appropriate distribution becomes challenging.\n",
    "\n",
    "- Distance Metrics:\n",
    "        Distance-based methods, such as Mahalanobis distance, are adapted to account for correlations among features in high-dimensional datasets. These methods measure the distance of a data point from the center of the distribution, considering the covariance structure.\n",
    "\n",
    "- Robust Estimators:\n",
    "        Robust statistical estimators, like the Minimum Covariance Determinant (MCD), are designed to be less sensitive to outliers. They can be especially useful in high-dimensional scenarios where the presence of outliers can distort traditional estimators.\n",
    "\n",
    "- Machine Learning Models:\n",
    "        Ensemble methods, support vector machines, and other machine learning models can be adapted or specialized for high-dimensional outlier detection, taking into account the challenges posed by the curse of dimensionality.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for modeling high-dimensional outliers using the Minimum Covariance Determinant (MCD) estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic high-dimensional data with outliers\n",
    "data, _ = make_multilabel_classification(n_samples=500, n_features=20, n_classes=2, n_clusters_per_class=1, n_informative=10, random_state=42)\n",
    "outliers = np.random.normal(loc=0, scale=1, size=(10, 20))\n",
    "\n",
    "# Combine normal data with outliers\n",
    "data = np.vstack([data, outliers])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "# Apply the Minimum Covariance Determinant (MCD) estimator\n",
    "mcd_estimator = EllipticEnvelope(contamination=0.02)\n",
    "labels = mcd_estimator.fit_predict(data_standardized)\n",
    "\n",
    "# Identify outliers\n",
    "outliers = labels == -1\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data_standardized[:, 0], data_standardized[:, 1], c=outliers, cmap='viridis', alpha=0.7)\n",
    "plt.title('Outlier Detection using Minimum Covariance Determinant (MCD) Estimator')\n",
    "plt.xlabel('Feature 1 (Standardized)')\n",
    "plt.ylabel('Feature 2 (Standardized)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic high-dimensional data with outliers and use the Minimum Covariance Determinant (MCD) estimator for outlier detection. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "breast-cancer-detection-association-rule-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
